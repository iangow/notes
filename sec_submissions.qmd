---
title: "Working with JSON data: The case of SEC submissions data"
author: Ian D. Gow
date: 2024-05-20
date-format: "D MMMM YYYY"
format:
  html:
    colorlinks: true
  pdf: 
    colorlinks: true
    geometry:
      - left=2.5cm
      - right=2.5cm
    papersize: a4
    mainfont: TeX Gyre Pagella
    mathfont: TeX Gyre Pagella Math
bibliography: papers.bib
---

One of the goals of [*Empirical Research in Accounting: Tools and Methods*](https://iangow.github.io/far_book/) (co-authored with [Tony Ding](https://fbe.unimelb.edu.au/our-people/staff/accounting/tongqing-ding)) is to provide a pathway to mastery of the contents of [*R for Data Science*](https://r4ds.hadley.nz).
That is, in working through *Empirical Research in Accounting: Tools and Methods* (along with the occasional detour to *R for Data Science*) the reader will have covered the contents of *R for Data Science* in enough detail to do their own data analysis.^[*Empirical Research in Accounting: Tools and Methods* will be published in print form by CRC Press later in 2024 and will remain free online after publication.]


While we meet this goal for most of the contents of *R for Data Science*, a few gaps remain: spreadsheets, dates and times, and hierarchical data are three notable ones.
The omission of [spreadsheets](https://r4ds.hadley.nz/spreadsheets) is not accidental given our focus on academic research applications.^[Explaining my views on the (limited) role of spreadsheets in research probably requires a separate note, but a good starting point is @Broman:2018aa.]
While we do discuss dates extensively in *Empirical Research in Accounting: Tools and Methods*, we have no application involving date-times and thus our coverage of the the additional complexities of [dates and times](https://r4ds.hadley.nz/datetimes) discussed in *R for Data Science*---such as time zones---is limited.

Regarding hierarchical data, *Empirical Research in Accounting: Tools and Methods* does touch on some key ideas.
[Chapter 5](https://iangow.github.io/far_book/stat-inf.html) and [Chapter 16](https://iangow.github.io/far_book/earnings-mgt.html) discuss `unnest_wider()` and list-columns are used in a number of places, including [Chapter 10](https://iangow.github.io/far_book/ffjr.html).
However, we do not provide coverage of these topics as systematic as that of [Chapter 23](https://r4ds.hadley.nz/rectangling) of *R for Data Science*.
For example, we do not touch `unnest_longer()` at all.

The purpose of this note is two-fold.
First, I hope to provide a hands-on introduction to hierarchical data that complements and builds on [Chapter 23](https://r4ds.hadley.nz/rectangling) of *R for Data Science*.
Second, I aim to show how one can process JSON data provided by SEC EDGAR using submissions data as case study.

In writing this note, I use the packages listed below.^[Execute `install.packages(c("tidyverse", "DBI", "duckdb", "jsonlite", "farr"))` within R to install all the packages you need to run the code in this note.]
This note was written using [Quarto](https://quarto.org) and compiled with [RStudio](https://posit.co/products/open-source/rstudio/), an integrated development environment (IDE) for working with R.
The source code for this note is available [here](https://raw.githubusercontent.com/iangow/notes/main/sec_submissions.qmd).

```{r}
#| include: false
options(width = 75)
options(tibble.width = 75)
```

```{r}
#| message: false
library(tidyverse)
library(DBI)
library(jsonlite)
library(farr)
```

## JSON

JSON stands for "Javascript object notation" and is a common form of data on the internet.
The SEC provides data in JSON form, including the submissions data I focus on in this note, and here I provide a brief overview of JSON.

JSON is a text-based data format with four scalar data types:

 - `null`: Like `NA` in R, this represents the absence of data.
 - **string**, as in R, but always quoted using double quotes (`"`).
 - **number**, as in R, but no support for `Inf`, `-Inf`, or `NaN`. Can be formatted using integer, decimal, or scientific (e.g., `2.99e8`) notation.
 - **Boolean**, similar to R's logical. Either `true` or `false`.
 
These scalar types can be used to form **arrays** and **objects**.
An array is like an unnamed list, and is written with `[]`. For example `[2, 4, 6]` is an array containing 3 numbers, and `[null, 2, "text", true]` is an array that contains a null, a number, a string, and a Boolean. 
An object is like a named list, and is written with `{}`.
The names ("keys" in JSON terminology) are provided as strings. 
For example, `{"a": 2, "b": 4}` is an object that maps `a` to 2 and `b` to 4.

Note that  dates or date-times are not among the scalar data types supported by JSON.
@Wickham:2023aa [p. 418] note that dates are "often stored as strings, and you’ll need to use `readr::parse_date()` or `readr::parse_datetime()` to turn them into the correct data structure."

Often a JSON file contains a single top-level array that represents multiple items, e.g., multiple records or multiple results. 
In this case, `tibble(json)` will treat each item as a row.


A simplified version of the contents of a SEC JSON file is given below.

```{r}
json_example <- '{
  "cik":2809,
  "entityType":"other",
  "sic":"1040",
  "insiderOwner":false,
  "description":null,
  "filings":{
    "accessionNumber":["0001104659-24-053850", "0001104659-24-053618"],
    "filingDate":["2024-04-29", "2024-04-29"]}}'
```


Here we see that the JSON stored in `json_example` comprises a single top-level JSON object.
As such, we follow the advice of *R for Data Science* for such cases by wrapping the JSON in a list before putting it in a tibble.

```{r}
json_example_df <- tibble(json = list(parse_json(json_example)))
json_example_df
```

We see that `json` in `json_example_df` is a list-column that contains a single row.
List-columns can be either *named* or *unnamed* and `json` is a named list-column.
@Wickham:2023aa [p. 406] suggest that "when the children are named, they tend to have the same names in every row. ... Named list-columns naturally unnest into columns: each named element becomes a new named column."
We can apply this approach using `unnest_wider()`:

```{r}
json_example_df |>
  unnest_wider(json)
```
From the results of applying `unnest_wider()`, we see that `cik`, `entityType`, and `insiderOwner` have been correctly parsed as integer, text, and logical types, respectively.
The `null` for `description` has been understood as `NA`.
Finally, `filings` is a *named* list-column to which we can apply `unnest_wider()`.


```{r}
json_example_df |>
  unnest_wider(json) |>
  select(-description) |>
  unnest_wider(filings) 
```

Now we see that `accessionNumber` and `filingDate` are *unnamed* list columns.
@Wickham:2023aa [p. 407] advises that "when each row contains an unnamed list, it's most natural to put each element into its own row with `unnest_longer()`" and we follow that advice here.

```{r}
json_example_df |>
  unnest_wider(json) |>
  unnest_wider(filings) |>
  unnest_longer(c(accessionNumber, filingDate))
```

Reader familiar with database design principles might cringe at the output above, as we have duplicated a number of items that appear to have a one-to-one relationship with a company (here identified by `cik`)---`entityType`, `sic`, `insiderOwner`, and `description`---because of the multiplicity of the items stored in `filings`.
A better structure would split the company-level items and the filing-level items into separate tables.
To create the `companies` table, we simply drop the `filings` column.

```{r}
companies <-
  json_example_df |>
  unnest_wider(json) |>
  select(-filings)

companies
```

To create the filings table, we retain only `cik` and `filings`.
The `cik` column allows us to combine the `companies` and `filings` as necessary.
As discussed above, we need to apply `parse_date()` to the string stored in `filingDate` to convert it to R's `Date` type.

```{r}
filings <-
  json_example_df |>
   unnest_wider(json) |>
  select(cik, filings) |>
  unnest_wider(filings) |>
  unnest_longer(c(accessionNumber, filingDate)) |>
  mutate(filingDate = parse_date(filingDate))

filings
```

## SEC submissions data

According to the SEC's [webpage](https://www.sec.gov/edgar/sec-api-documentation), "each entity's current filing history is available at the following URL: `https://data.sec.gov/submissions/CIK##########.json` where the `##########` is the entity’s 10-digit Central Index Key (CIK), including leading zeros."
This means we will use CIKs to identify files to collect.
Most sources of CIKs provide them as integers, but we can easily construct the 10-digit strings with leading zeros using `str_pad()` and construct the desired URL from that.
To start we focus (arbitrarily) on Agnico Eagle Mines, a Canadian firm with CIK `2809`.
Running the following code should open a browser tab with the JSON data for submissions by Agnico Eagle Mines.

```{r}
#| cache: false
cik <- 2809
basename <- str_c("CIK", str_pad(cik, width = 10, pad = "0"), ".json")
url <- paste0("https://data.sec.gov/submissions/", basename)
```

```{r}
#| eval: false
browseURL(url)
```

We embed the logic for creating a URL from the file name (`basename`) and the first couple of steps of processing the JSON in a function.
The reasons for using the file name rather than the CIK and for the (as-yet-unused) `local_file` argument will become clear later.

```{r}
get_sec_json <- function(basename, local_file = NULL) {
  url <- paste0("https://data.sec.gov/submissions/", basename)
  tibble(json = list(read_json(url))) |>
    unnest_wider(json)
}
```

The PCAOB requires use to provide an email address when downloading data using a script and we can set this using the `HTTPUserAgent` option as in the R code below.^[Use your actual email address here.]

```{r}
#| include: false
options(HTTPUserAgent = "iandgow@gmail.com")
```

```{r}
#| eval: false
options(HTTPUserAgent = "your_name@some_email.com")
```

Once we have set `HTTPUserAgent`, we can execute the `get_sec_json()` function:

```{r}
#| cache: true
get_sec_json(basename)
```

From the output above, we see that there are four list columns: `tickers`, `exchanges`, `addresses`, and `filings`.
While we could dig deeped into the first three (e.g., creating an `addresses` table), we focus on reorganizing only the `filings` code, keeping the other three as list-columns in a `companies` table.

We embed code all to collect company-level data in a simple function `get_company_data()`.
We use `get_sec_json()` to get the JSON, then drop `filings` before processing the remaing columns as needed.
While `cik` was supplied as a number in the example above, the actual data on the SEC site present this as a string and we convert it to integer using `as.integer()` (and do likewise for `sic`).
Similarly, the two fields that are fundamentally Boolean are supplied as either `0` or `1` rather than `true` or `false` and we convert these to logical type using `as.logical()`.

```{r}
get_company_data <- function(cik, local_file = NULL) {
  basename <- str_c("CIK", str_pad(cik, width = 10, pad = "0"), ".json")
  
  get_sec_json(basename, local_file) |>
    select(-filings) |>
    mutate(across(c(cik, sic), as.integer),
           across(ends_with("Exists"), as.logical))
}
```

We can confirm that these type conversions behave as expected using the following code:

```{r}
#| cache: true
get_company_data(cik) |> select(cik, sic, ends_with("Exists"))
```

We can now Shift our focus to the filing-level data.

```{r}
#| cache: true
get_sec_json(basename) |>
  select(filings)
```

Noting that it's a named list, we look at the results of applying `unnest_wider()` to `filings`:

```{r}
#| cache: true
get_sec_json(basename) |>
  select(filings) |>
  unnest_wider(filings)
```

Here we see that there are two fields embedded in `filings`: `recent` and `files`.
According to the SEC's [webpage](https://www.sec.gov/edgar/sec-api-documentation), the "JSON data structure [of the submissions data] contains metadata such as current name, former name, and stock exchanges and ticker symbols of publicly-traded companies. 
The object's property path contains at least one year's of filing or to 1,000 (whichever is more) of the most recent filings in a compact columnar data array.
If the entity has additional filings, `files` will contain an array of additional JSON files and the date range for the filings each one contains."

In the current case, `files` contains only `NA`, suggesting that there are less than 1,000 filings in `recent` (so `recent` represents *all* filings for this issuer).
We put `files` aside for now and focus on `recent`.
As `recent` is a named-list, the natural next step is to apply `unnest_wider()` to this column.

```{r}
#| cache: true
get_sec_json(basename) |>
  select(filings) |>
  unnest_wider(filings) |>
  select(-files) |>
  unnest_wider(recent)
```
Seeing that all remaining columns are unnamed list-columns, we now apply `unnest_longer()`.

```{r}
#| cache: true
get_sec_json(basename) |>
  select(filings) |>
  unnest_wider(filings) |>
  select(-files) |>
  unnest_wider(recent) |>
  unnest_longer(everything())
```
This seems to work, as have a rectangular table of filing-level data.
However, we now need to consider a case where `files` is not `NA`.
Ashland Inc, a chemicals company with CIK of `7694`, is such a case.

```{r}
#| cache: false
cik <- 7694
basename <- str_c("CIK", str_pad(cik, width = 10, pad = "0"), ".json")
```

Retracing our steps above, we see that `recent` takes the same form as we saw for Agnico Eagle Mines, but that `files` is now an unnamed list-column.
Applying `unnest_longer()`, we get a named list-column.
Applying `unnest_wide()` to that named list-column, we get the following.

```{r}
#| cache: true
get_sec_json(basename) |>
  select(filings) |>
  unnest_wider(filings) |>
  select(files) |>
  unnest_longer(files) |>
  unnest_wider(files)
```

Thus we see that the **base file** `CIK0000007694.json` contains filings data inside the `recent` element  as well as a list of **supplemental files** in inside the `files` element.
We now examine the supplemental file identified above.

```{r}
#| cache: true
get_sec_json("CIK0000007694-submissions-001.json") |>
  unnest_longer(everything())
```

The output above suggests that the supplemental files (i.e., those identified in the `files` column) contain data in the same form as we get from applying `unnest_wider(recent)` to the base files.
The base files contain the column `filings`, which the supplemental files do not.
This suggests we could make one function that, depending on whether there is a `filing` column present, either extracts the contents of that field using (ultimately) `unnest_wider(recent)` or simply processes the data as is.
The following `extract_filings()` function does just that.

```{r}
extract_filings <- function(basename, local_file = NULL) {

  raw_data <- get_sec_json(basename, local_file)

  if ("filings" %in% colnames(raw_data)) {
    filings_raw <-
      raw_data |>
      select(filings) |>
      unnest_wider(filings) |>
      select(recent) |>
      unnest_wider(recent) 
  } else {
    filings_raw <- raw_data
  }

  filings_raw |>
    unnest_longer(everything()) |>
    mutate(across(c(filingDate, reportDate), parse_date),
           acceptanceDateTime = parse_date_time(acceptanceDateTime,
                                                orders = "ymdHMSz"),
           across(c(isXBRL, isInlineXBRL), as.logical)) |>
    mutate(cik = as.integer(cik)) |>
    select(cik, everything())
}
```

In addition, `extract_filings()` converts `filingDate` and `reportDate` to date type, `isXBRL` and `isInlineXBRL` to logical type, `cik` to integer and `acceptanceDateTime` to date-time.

We can now see the rationale for using `basename`, not `cik`, as the argument to `extract_filings()`: a single function can handle both base files (which can be identified using CIKs) and supplemental files, which require inspection of the base files to be identified.

Now we need to create a function that assembles the list of files (base files and supplemental files) for each CIK.
The following `get_submission_files()` function does just that.
It gets the JSON data for the base file and checks whether there are any non-null values in `files`.
If there are no non-null values in `files`, then just the name of the base file is returned.
Otherwise, the extracts the non-null values in `files` and adds them to the name of the base file.

```{r}
get_submission_files <- function(cik, local_file = NULL) {

  basename <- str_c("CIK", str_pad(cik, width = 10, pad = "0"), ".json")
  raw_data <- get_sec_json(basename, local_file)

  files_df <-
    raw_data |>
    select(filings) |>
    unnest_wider(filings) |>
    select(files) |>
    unnest_longer(files) |>
    filter(!is.na(files))

  if (nrow(files_df) > 0) {
    files_df |>
      unnest_wider(files) |>
      select(name) |>
      pull() |>
      union_all(basename)
  } else {
    basename
  }
}
```

```{r}
#| cache: true
get_submission_files(2809)
get_submission_files(7694)
```

The plan is for the output of `get_submission_files()` to be supplied to `extract_filings()`.
The following `cik_to_filings()` function gives effect to this plan with the list of tibbles being combined into a single tibble using `list_rbind()`.
Note that we use an anonymous function with `map()` so that we can pass along the value of the still mysterious `local_file` argument to `extract_filings()`.

```{r}
cik_to_filings <- function(cik, local_file = NULL) {
  get_submission_files(cik, local_file = local_file) |>
    map(\(x) extract_filings(x, local_file = local_file)) |>
    list_rbind()
}
```

We can check that the `cik_to_filings()` function behaves as we expect it to do for the two companies we have focused on so far.

```{r}
#| cache: true
cik_to_filings(2809)
cik_to_filings(7694)
```

Thus we have two core functions: `get_company_data()` to get the company-level data and `cik_to_filings()` to get the filing-level data.
In principle, if we had a universe of CIKs that we wanted submission data for, we could put these in a vector and `map()` each of these two functions to that vector to get data.

```{r}
ciks <- c(2809, 7694)
```

```{r}
#| cache: true
#| include: false
ciks |>
  map(get_company_data) |>
  list_rbind()
```

```{r}
#| cache: true
ciks |>
  map(cik_to_filings) |>
  list_rbind()
```

However, I do not advise this approach because it involves a lot of internet traffic and is likely to take several hours.
If there's a hiccup in the middle, we would finish up empty-handed and need to try all over again.

Regarding internet traffic, note that the code is fairly inefficient.
The [base file URL for Ashland Inc](https://data.sec.gov/submissions/CIK0000007694.json) is accessed three times: once for `get_company_data()` and twice for `cik_to_filings()`.
If internet traffic ends up being a bottleneck, then it might make sense to modify the code above to download data just once for each CIK.
But we shelve this issue for now.

## Bulk data

According to the SEC website, "the most efficient means to fetch large amounts of API data is the bulk archive ZIP files, which are recompiled nightly.
The `submission.zip` file contains the public EDGAR filing history for all filers from the Submissions API `https://www.sec.gov/Archives/edgar/daily-index/bulkdata/submissions.zip`."

Let's download this buk file:

```{r}
#| cache: true
bulk_url <- str_c("https://www.sec.gov/Archives/edgar/",
                  "daily-index/bulkdata/submissions.zip")
data_dir <- "data"
bulk_path <- file.path(data_dir, "submissions.zip")

if (!dir.exists(data_dir)) dir.create(data_dir, recursive = TRUE)

if (!file.exists(bulk_path)) {
  result <-
    download.file(bulk_url, bulk_path) |>
    system_time()
}
```

From the following output, we can see that `submissions.zip` contains nearly 900,000 files.
While we could simply "unzip" this file, this would take some time and we would have nearly 900,000 files in a single directory.

```{r}
#| cache: true
bulk_files <-
  unzip(bulk_path, list = TRUE) |>
  as_tibble()

bulk_files
```

It turns out that we can use the `unz()` function to unzip a single file and, as we can identify the files we need from CIKs and inspection of the base files for that CIK, we can elect to extract just the files we need as and when we need them.
It is at this point that we can reveal the purpose of the `local_file` argument that we included above, but neither explained nor used.
In short, we want to create a new version of `get_sec_json()` that gets the relevant data from a local file if `local_file` is supplied, but goes to the relevant URL if not.
Note that the version of `get_sec_json()` below uses `parse_json()` instead of `read_json` because the argument supplied represents a "connection" rather than an actual file.

```{r}
get_sec_json <- function(basename, local_file = NULL) {

  local <- !is.null(local_file)

  if (local) {
    t <- unz(local_file, basename, open = "rb")
    raw_data <-
      tibble(json = list(parse_json(t, null = 'list'))) |>
      unnest_wider(json)
    close(t)
    unlink(t)
  } else {
    url <- paste0("https://data.sec.gov/submissions/", basename)
    raw_data <-
      tibble(json = list(read_json(url))) |>
      unnest_wider(json)
  }
  raw_data
}
```

As we can see from the output below, our functions work exactly as they did before, but using the already-downloaded file.

```{r}
#| cache: true
cik_to_filings(7694, local_file = bulk_path)
```

## Getting a list of CIKs

To put our code to the test, we will look at collecting submissions data for a larger set of CIKs.
For most research applications, we would not be interested in all files in `submissions.zip`.
Instead, we will use data from SEC index files to get the population of CIKs for filers that ever filed on a Form 8-K.

```{r}
#| eval: true
db_path <- file.path(data_dir, "submissions.duckdb")
db <- dbConnect(duckdb::duckdb(dbdir = db_path, read_only = FALSE))

sec_index <- load_parquet(db, "sec_index*", "edgar")
ciks <-
  sec_index |>
  filter(str_detect(form_type, "^8-K")) |>
  distinct(cik) |>
  compute(name = "ciks", overwrite = TRUE, temporary = FALSE)
```

This yields a total of `r prettyNum(pull(count(ciks)), big.mark = ",")` CIKs.
To evaluate how long it will take to process submissions data for all of the CIKs, we can take a sample of 100 CIKs and time the processing for these and then extrapolate.

```{r}
#| eval: true
cik_sample <-
  ciks |>
  collect(n = 100) |>
  pull(cik)

dbDisconnect(db)
```

First, the company-level data:

```{r}
#| eval: true
#| cache: true
company_sample <-
  cik_sample |>
  map(\(x) get_company_data(x, local_file = bulk_path)) |>
  list_rbind() |>
  system_time()
```

Second, the filing-level data:

```{r}
#| eval: true
#| cache: true
filings_sample <-
  cik_sample |>
  map(\(x) cik_to_filings(x, local_file = bulk_path)) |>
  list_rbind() |>
  system_time()
```

So we will need around 40 seconds for 100 filings and thus roughly 5 hours for all filings.

## Processing CIKs in bulk

```{r}
get_cik_sample <- function(n_ciks = 100, new_table = "filings") {
  db <- dbConnect(duckdb::duckdb(dbdir = db_path, read_only = TRUE))

  ciks <- tbl(db, "ciks")

  if (new_table %in% dbListTables(db)) {
    to_update <- tbl(db, new_table)
    unprocessed <-
      ciks |>
      anti_join(to_update, by = "cik")
  } else {
    unprocessed <- ciks
  }
  
  cik_sample <-
    unprocessed |>
    collect(n = n_ciks) |>
    select(cik) |>
    pull()
  
  dbDisconnect(db)
  
  cik_sample
}
```

```{r}
convert_nulls <- function(x) {
  return(x)
  nulls <- unlist(map(x, is.null))
  x[nulls] <- NA
  x
}
```

```{r}
sample_n <- 0

while (length(cik_sample <- get_cik_sample(new_table = "companies")) > 0) {
  
  sample_n <- sample_n + 1
  cat("Processing sample number: ", sample_n, "\n")
  
  company_sample <-
    cik_sample |>
    map(\(x) get_company_data(x, local_file = bulk_path)) |>
    list_rbind() |> 
    mutate(across(where(is.list), convert_nulls))
  
  addresses <-
    company_sample |> 
    select(cik, addresses) |> 
    unnest_longer(addresses) |> 
    unnest_wider(addresses)
  
  db <- dbConnect(duckdb::duckdb(dbdir = db_path, read_only = FALSE))
  dbWriteTable(db, "companies", 
               company_sample |>
                 select(-any_of(c("addresses", "ownerOrg"))),
               append = TRUE)
  
  dbWriteTable(db, "addresses", addresses, append = TRUE)
  
  dbDisconnect(db)
}

# -tickers, -exchanges,  , -formerNames
```

```{r}
sample_n <- 0

while (length(cik_sample <- get_cik_sample(new_table = "filings")) > 0) {
  
  sample_n <- sample_n + 1
  cat("Processing sample number: ", sample_n, "\n")

  filings_sample <-
    cik_sample |>
    map(\(x) cik_to_filings(x, local_file = bulk_path)) |>
    list_rbind()
  
  db <- dbConnect(duckdb::duckdb(dbdir = db_path, read_only = FALSE))
  dbWriteTable(db, "filings", filings_sample,
               append = TRUE)
  dbDisconnect(db)
}
```


## References {-}
