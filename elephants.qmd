---
title: "Five elephants or one?"
author: "Ian D. Gow"
bibliography: papers.bib
format: 
    pdf:
        toc: false
        number-sections: true
        colorlinks: true
        mainfont: TeX Gyre Pagella
        sansfont: TeX Gyre Pagella
editor: visual
---

# Five elephants or one?

@Ohlson:2022aa discusses what he calls "elephants in the room" (or taboo topics) and discusses "five elephants". 
But perhaps there are not five elephants so much five alternative descriptions of the one elephant, much like the parable of the [blind men and the elephant](https://en.wikipedia.org/wiki/Blind_men_and_an_elephant).

What exactly is that elephant in the room? 
I posit that the predominant mode of research in academic accounting in 2022 is p-hacking and argue that the five elements can be viewed as alternative perspectives on the same elephant, i.e., p-hacking, which is the practice whereby researchers search for "significant" and "positive" results. 
Here "significant" refers to statistical significance and "positive" refers to results that reject so-called "null hypotheses" and thereby (purportedly) push human knowledge forward.

# A replication crisis? {#rep-crisis}

A [Financial Times article by Robert Wigglesworth](https://www.ft.com/content/9025393f-76da-4b8f-9436-4341485c75d0) quotes Campbell Harvey, professor of finance at Duke University, saying that "at least half of the 400 supposedly market-beating strategies identified in top financial journals over the years are bogus."

@Harvey:2017ux cites research suggesting that 90% of published studies report the "significant" and "positive" results that p-hacking searches for.
Reporting "positive" results is important not only for getting published, but also for attracting citations, which drive behaviour for both researchers and journals.

@Simmons:2011ux [p. 1359] describe what they term **researcher degrees of freedom**. 
"In the course of collecting and analyzing data, researchers have many decisions to make: Should more data be collected? Should some observations be excluded? 
Which conditions should be combined and which ones compared? Which control variables should be considered? 
Should specific measures be combined or transformed or both?" 
@Simmons:2011ux [p. 1364] identify another well-known researcher degree of freedom, namely that of "reporting only experiments that 'work'", which is known as the **file-drawer problem** (because experiments that don't "work" are put in a file-drawer).

@Simmons:2011ux provide analyses that "demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis" [p. 1359]. @Simmons:2011ux [p. 1359] conclude that "flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates."

@Bloomfield:2018va [p. 317] suggest that "almost all peer-reviewed articles in social science are published under" what they call  ... the Traditional Editorial Process (or TEP).
Under the TEP, "authors gather their data, analyze it, and write and revise their manuscripts repeatedly before sending them to editors." 

An alternative to the TEP is the Registration-based Editorial Process (REP). 
According to @Bloomfield:2018va [p. 317], "under REP, authors propose a plan to gather and analyze data to test their predictions. 
Journals send promising proposals to one or more reviewers and recommend revisions. 
Authors are given the opportunity to review their proposal in response, often multiple times, before the proposal is either rejected or granted in-principle acceptance ... regardless of whether [subsequent] results support their predictions."
The REP is designed to eliminate questionable research practices such as those identified by @Simmons:2011ux. 

The *Journal of Accounting Research* (JAR) conducted a trial of the REP for its [annual conference held in May 2017](https://research.chicagobooth.edu/arc/journal-of-accounting-research/jar-annual-conference/conference-web-casts/2017). 
@Bloomfield:2018va [p. 326] examine the results reported in  the 2017 JAR conference papers conclude that "of the 30 predictions made in the â€¦ seven proposals, we count 10 as being supported at $p \leq 0.05$ by at least one of the 134 statistical tests the authors reported."
But this is very close to the level of support expected if the null hypotheses for all 30 predictions were true.
This is particularly concerning in that it seems reasonable to expect that the alternative hypotheses considered in the 2017 JAR conference papers were deemed by the authors and reviewers to be worth pursuing before knowing their results, which is a higher bar than applied to hypotheses tested using the TEP.
In other words, the results of the 2017 JAR conference raised the prospect that many results produced by the TEP (i.e., almost all research in accounting) are false rejections of true null hypotheses.

Unfortunately, it is unlikely that the REP will replace the TEP to any great extent in the foreseeable future.
The REP is feasible when data are generated in randomized controlled trials (RCTs), as the data simply do not exist when the report is registered.
In contrast, most empirical accounting research uses existing archival data making it impossible to register a report before being able to look at the data.

One form of p-hacking addressed by the REP is **HARKing** (from "Hypothesizing After Results are Known"). 
In its extreme form, HARKing involves searching for a "significant" correlation and *then* developing a hypothesis to "predict" it. 
To illustrate, consider the [spurious correlations website](http://tylervigen.com/spurious-correlations) provided by Tyler Vigen. 
This site lists a number of evidently spurious correlations, such as the 99.26% correlation between the divorce rate in Maine and margarine consumption or the 99.79% correlation between US spending on science, space, and technology and suicides by hanging, strangulation and suffocation. 
The correlations are deemed spurious because normal human beings have strong prior beliefs that there is no underlying causal relation explaining these correlations. 
Instead, these are regarded as mere coincidence.

However, a creative academic can probably craft a story to "predict" any correlation. 
Perhaps increasing spending on science raises its perceived importance to society. 
But drawing attention to science only serves to highlight how the US has inevitably declined in relative stature in many fields, including science. 
While many Americans can carry on notwithstanding this decline, others are less sanguine about it and may go to extreme lengths as a result ... . 
This is clearly a silly line of reasoning, but if one added some references to published studies and fancy terminology, it would probably read a lot like the hypothesis development sections of some academic papers.

I consider each of the five elephants, starting with the one that @Ohlson:2022aa lists last: "Issues related to 'screen-picking' and 'data-snooping'". Terms like "data-snooping" are simply synonyms of p-hacking, which is well understood by many to be a major issue in the social sciences. I suggest that the insight of @Ohlson:2022aa is that merely suggesting the possibility of p-hacking is taboo ("unacceptable", "a personal assault", "too sordid", "testing ethical boundaries", and "a more or less painful private matter").

## Anecdotes

I conjecture that blind p-hacking is actually more common than motivated p-hacking and offer the following (paraphrased) anecdotes as examples:

*Anecdote #1*

Me: "Why did you look at $y_1$? Wouldn't $y_2$ or $y_3$ be the natural things to examine?"

Doctoral student(s) at a good school: "I already looked at those. They didn't work, so I dropped them."

*Anecdote #2*

Me: "Your paper is about $y_1$? Wouldn't $y_2$ or $y_3$ be the natural things to examine as consequences of $X$?"

Senior faculty at a good school: "We already looked at those. They didn't work, so we wrote a paper about $y_1$ instead."

*Anecdote #3*

Colleague: "I have found that $x$ and $y$ are associated and I think the reason is [some causal story]."

Me: "But [some causal story] just isn't plausible."

Colleague: "Oh. So you think I need to come up with some other story then?"

*Anecdote #4*

Colleague of colleague to colleague: 
"I have data on $X$, perhaps I can send you these data and you can regress various $y$ variables that you have on $X$. 
If something works, perhaps we could collaborate."

# What to do?

It is important to understand that if, say, 90% of research is p-hacked, that one cannot simply read the 10% that is not p-hacked and ignore the rest. If it were easy to detect the p-hacked papers, we could simply avoid publishing them. But I argue there are steps that could be taken to reduce p-hacking to an extent that research in aggregate has some value?

## Reject papers that ask silly questions

Accounting academics appear to adore "novelty", where novelty often means asking questions that no-one has even dreamed of asking before. This is problematic for two reasons. First, if questions are so novel that no-one has asked them, how can they be important? Second, the ability to simply make up "interesting" research questions is a p-hacker's dream.

Too often the bar seems to be "has someone [in prior research] asked this question before?" and if the answer is "no" then the novelty bar has been cleared. But after more than 50 years of modern empirical accounting research, the fact that no-one has asked a question should in most cases be a strike *against* a paper, not for it. If no-one has addressed the question, then it is perhaps because no-one cares what the answer is. If editors adopted a policy of desk-rejecting papers that ask silly questions, the pay-off to p-hacking would decline significantly.

*Model should be medicine, not physics*

*Concern about subjectivity*

## Increase emphasis on replication

# References {.unnumbered}
