---
title: "The elephant in the room: p-hacking and accounting research"
author: 
  - name: Ian D. Gow^[University of Melbourne, [ian.gow@unimelb.edu.au](mailto:ian.gow@unimelb.edu.au)]
    orcid: 0000-0002-6243-8409
    email: iandgow@gmail.com
abstract: "@Ohlson:2023aa draws on his experience in empirical accounting seminars to identify five \"elephants in the room\".
I interpret each of these elephants as either a variant or a symptom of p-hacking.
I provide evidence of the prevalence of p-hacking in accounting research that complements the observations made by @Ohlson:2023aa.
In this paper, I identify a number of steps that could be taken to reduce p-hacking in accounting research.
I conjecture that facilitating and encouraging replication alone could have profound effects on the quality and quantity of empirical accounting research."
date: today
date-format: "D MMMM YYYY"
bibliography: papers.bib
title-block-style: plain
csl: jfe.csl
format: 
    pdf:
        toc: false
        number-sections: true
        colorlinks: true
        mainfont: TeX Gyre Pagella
        sansfont: TeX Gyre Pagella
    aft-pdf:
        keep-tex: true
        toc: false
        number-sections: true
        colorlinks: true
        mainfont: TeX Gyre Pagella
        sansfont: TeX Gyre Pagella
---

# Five elephants or one?

@Ohlson:2023aa identifies five of what he calls "elephants in the room" (or topics considered taboo in seminars).
I read these not so much as five elephants, but as five alternative descriptions of the one elephant, much like the elephant in the parable of the blind men and an elephant [@wiki:elephant].

What exactly is that elephant in the room?
I argue that Ohlson's five elephants are simply alternative perspectives on the same elephant, which is p-hacking, a term for a set of practices engaged in by researchers searching for "significant" and "positive" results.^[Here "significant" refers to statistical significance and "positive" refers to results that reject so-called "null hypotheses" and thereby (purportedly) push human knowledge forward.]
To be sure, this is a very big elephant: I conjecture that p-hacking is the dominant mode of research in academic accounting in 2023, and below I provide (admittedly circumstantial) evidence consistent with this conjecture.

That the basic concern of @Ohlson:2023aa is with p-hacking is clearest with the last of his five elephants: "Issues Related to 'Screen-Picking' and 'Data-Snooping'", as terms like "data-snooping" are simply synonyms of p-hacking.^[I discuss in an appendix below (@sec-other-elephants) how the remaining four elephants relate to p-hacking.]
The key insight of @Ohlson:2023aa may be in highlighting how merely suggesting the possibility of p-hacking is taboo [@Ohlson:2023aa uses terms such as "unacceptable", "a personal assault", "too sordid", "testing ethical boundaries", and "a more or less painful private matter"].

Many researchers appear not to understand how p-hacking vitiates the whole research endeavour.
So if even suggesting the possibility p-hacking is taboo, it will be much more difficult to address and accounting research will continue to be a largely pointless exercise.^[Some researchers agree with the very limited value of accounting research with regard to expanding human knowledge, but argue that the real value of research is in deciding who gets tenure at top universities.
But this merely raises the question of the merits of making these decisions based on skills related to conducting and packaging p-hacked research, which seem unclear to say the least.]
I agree with @Ohlson:2023aa that we need to confront this "elephant in the room" and make a number of proposals for how we might do so.

In the rest of this paper, I first describe the practice of p-hacking.
I then offer some circumstantial evidence of its prevalence in accounting research.
Finally, I offer some ideas on how to address the "elephant(s) in the room" of accounting research.

# The practice of p-hacking 

According to @Wigglesworth:2021aa, Campbell Harvey, professor of finance at Duke University, suggests that "at least half of the 400 supposedly market-beating strategies identified in top financial journals over the years are bogus."
@Harvey:2017ux cites research suggesting that 90% of published studies report the "significant" and "positive" results.
Reporting "positive" results is important not only for getting published, but also for attracting citations, which drive behaviour for both researchers and journals.

@Simmons:2011ux [p. 1359] provide analyses that "demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis ... [how] flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates."
They attribute this flexibility to **researcher degrees of freedom**:
"In the course of collecting and analyzing data, researchers have many decisions to make: Should more data be collected?
Should some observations be excluded?
Which conditions should be combined and which ones compared?
Which control variables should be considered?
Should specific measures be combined or transformed or both?" [@Simmons:2011ux, p. 1359] 

@Bloomfield:2018va [p. 317] suggest that "almost all peer-reviewed articles in social science are published under" what they call ... the Traditional Editorial Process (or TEP).
Under the TEP, "authors gather their data, analyze it, and write and revise their manuscripts repeatedly before sending them to editors."
As such authors have access to many researcher degrees of freedom.

An alternative to the TEP is what @Bloomfield:2018va call the Registration-based Editorial Process (REP).
According to @Bloomfield:2018va [p. 317], "under REP, authors propose a plan to gather and analyze data to test their predictions.
Journals send promising proposals to one or more reviewers and recommend revisions.
Authors are given the opportunity to review their proposal in response, often multiple times, before the proposal is either rejected or granted in-principle acceptance ... regardless of whether [subsequent] results support their predictions." 
The REP is intended to eliminate research degrees of freedom and the questionable research practices that these permit.^[There are two important elements of the REP that affect p-hacking.
First, the requirement to specify analytical procedures in advance of having the data is intended to eliminate p-hacking.
However, in practice, it can be difficult to specify every detail of data analysis and some researcher degrees of freedom can remain.
For example, researchers might choose to conduct and include supplementary analyses if the pre-specified analyses do not yield statistically significant results.
Second, a journal will typically commit to publishing the resulting study whether there are statistically significant results or not.
This is intended to limit incentives for p-hacking (and also to produce a more faithful research record).
However, if authors are concerned about citations of their papers, they may still have incentives to p-hack if there are enough researcher degrees of freedom to do so.]

The *Journal of Accounting Research* (JAR) conducted a trial of the REP for its [annual conference held in May 2017](https://research.chicagobooth.edu/arc/journal-of-accounting-research/jar-annual-conference/conference-web-casts/2017).
Unfortunately, it is unlikely that the REP will replace the TEP to any great extent in the foreseeable future.
The REP is feasible when data are generated in randomized controlled trials (RCTs), as the data simply do not exist when the report is registered.^[See @Chambers:2014aa for more on the historical antecendents of the REP.]
In contrast, most empirical accounting research uses existing archival data, which often makes it impossible to register a report before being able to look at the data.^[For example, if I am testing a hypothesis using data from CRSP and Compustat, I cannot credibly promise that I have not looked at the data before submitting my report. 
In principle, one could propose a study that only uses future data from CRSP and Compustat, but this seems unlikely to be popular in a discipline accustomed to hundreds of thousands of observations.]

# Evidence of p-hacking

## Conversational evidence

The contention of @Ohlson:2023aa that raising issues related to p-hacking is taboo in empirical accounting seminars seems very plausible.
Outside of papers like @Simmons:2011ux that aim to demonstrate the "power" of p-hacking, we generally only see circumstantial evidence of p-hacking in the papers themselves.^[I discuss such evidence below.]
But sometimes (outside of seminars!) researchers can be fairly candid about their research process.

I must have had countless conversations where a colleague or student is examining the effect of $X$ on $y_1$ and my natural response has been to ask whether some other variables would be the more natural things to examine instead of $y_1$ and the response is something like "we looked at those other variables and they didn't work".
This practice of "reporting only experiments that 'work'" while discarding results that "don't work" is another well-known researcher degree of freedom discussed by @Simmons:2011ux [p.
1364], and is known as the **file-drawer problem** (because experiments that don't "work" are put in a file-drawer).

A more brazen form of p-hacking is trawling through a data set until correlations are found, at which point the challenge is to devise an "interesting" causal story to go with it.
I have had conversations suggesting that research for some involves searching for a "significant" correlation and *then* developing a hypothesis to "predict" it.
This form of p-hacking is known as **HARKing** (from "Hypothesizing After Results are Known").

To illustrate, consider the [spurious correlations website](http://tylervigen.com/spurious-correlations) provided by Tyler Vigen.^[Available at [http://tylervigen.com/spurious-correlations](http://tylervigen.com/spurious-correlations).]
This site lists a number of evidently spurious correlations, such as the 99.26% correlation between the divorce rate in Maine and margarine consumption or the 99.79% correlation between US spending on science, space, and technology and suicides by hanging, strangulation and suffocation.
The correlations are deemed spurious because normal human beings have strong prior beliefs that there is no underlying causal relation explaining these correlations.
Instead, these are regarded as mere coincidence.

However, a creative academic can probably craft a story to "predict" any correlation: Perhaps increasing spending on science raises its perceived importance to society.
But drawing attention to science only serves to highlight how the US has inevitably declined in relative stature in many fields, including science.
While many Americans can carry on notwithstanding this decline, others are less sanguine about it and may go to extreme lengths as a result ... .
This is a clearly silly line of reasoning, but if one added some references to published studies and fancy terminology, it would probably read a lot like the hypothesis development sections of academic papers presented in the empirical accounting seminars discussed by @Ohlson:2023aa.

Sherlock Holmes claims "it is a capital mistake to theorize before you have all the evidence." [@Doyle:2001aa, p. 27].
The modern equivalent might be that "it is a capital mistake to theorize before you have a statistically significant association to 'predict'", as there seems to be little value in devoting effort to predict a relation that is not supported by the data set you have.

## Evidence from the 2017 JAR REP trial

The 2017 JAR REP trial itself provides circumstantial evidence of p-hacking in papers produced using the TEP (i.e., almost all papers in accounting research).
@Bloomfield:2018va [p. 326] examine the results reported in the conference papers and conclude that "of the 30 predictions made in the ... seven proposals, we count 10 as being supported at $p \leq 0.05$ by at least one of the 134 statistical tests the authors reported." 
But this is very close to the level of support expected if the null hypotheses for all 30 predictions were true.^[See @far_book_21 for details of the calculation in support of this claim.]

This is particularly concerning in that it seems reasonable to expect that the alternative hypotheses considered in the 2017 JAR conference papers were deemed by the authors and reviewers to be worth pursuing before knowing their results, which is a higher bar than applied to hypotheses tested using the TEP.
In other words, the results of the 2017 JAR conference raise the uncomfortable prospect that many results produced by the TEP (i.e., almost all research in accounting) arise from p-hacking and are simply false rejections of true null hypotheses.

## Circumstantial evidence from replications

Another source of evidence on the prevalence of p-hacking is replications.
We expect that p-hacked papers will have results that are very fragile.
By definition, p-hacked results are not expected to be **reproducible**.
That is, we would not expect the results to hold if the same analytical procedures were applied to a new data set.^[Here I follow @Hail:2020aa in distinguishing replicability from reproducibility.
This is clearly related to Elephant #4, which is "Referring to the possibility of using a holdout sample".]

But p-hacked results should pass the test of **replicability**, which requires that the results can be produced by other authors using the same data sets and analytical procedures.
Given that much of the published research in accounting uses data sets that are available to most researchers and papers typically include descriptions of the analytical procedures used, other researchers should be able to replicate results independently even without access to the code and data files used by the original authors.

In practice, it seems that many papers cannot be replicated in this way, even approximately.
Ask another researcher whether she has tried to replicate results of a published paper and you are likely to hear that attempts have been made, but without success.^[Some researchers' replication experiences are limited to exercises assigned during PhD coursework, but there is a natural selection bias with these, as many instructors would look to assign exercises where results can be reproduced.]
One explanation for this difficulty is that small departures from the choices made by the authors along the dimensions described in @Simmons:2011ux can lead to apparent results disappearing (i.e., becoming statistically insignificant) and few papers describe these choices sufficiently clearly to allow precise replication.

I have extensive experience with attempted replication of papers.
My ill-fated PhD dissertation attempted to identify a causal mechanism underlying the numerous results in the literature suggesting a contracting value for firms' voluntary adoption of higher levels of conditional conservatism.
However, explaining results documented in research is difficult when those results cannot be replicated, and almost all replications I tried failed.
Since then I have undertaken many attempted replications in various areas and most have failed.

One explanation might be that I simply do not know how to analyse data properly and that a more skilled researcher would be able to reproduce published results more readily.
While it is difficult to rule out this explanation completely, I believe a significant recent project suggests that this is not a complete explanation.

In 2021, a University of Melbourne colleague (Tony Ding) and I started to pull together a course book [@Gow:2022aa] aimed at helping research students to develop the portfolio of skills needed to be good researchers in accounting.
As discussed in the book, a core element is material focused on data analysis skills and we have included many replication analyses to support this [@far_book_1].
It seems these replication efforts can be organized into two eras.

The first era covers 1968 through to about 1996.
The striking thing about this era is how robust the results appear to be.
Like many before us, we find that the key results of the seminal @Ball:1968ub are easily replicated [@far_book_13], and @Ball:2019wu show this is true in different markets and periods.
We find that key results of @Beaver:1968vf hold in any year we look at [@far_book_14].^[@Bamber:2000wv raise concerns about the reproducibility of the results in @Beaver:1968vf, but these concerns that do not appear to hold in years after @Beaver:1968vf was published.]
Not only can we generate the core results of @Bernard:1989uu, but we broadly replicate @Foster:1977wy along the way [@far_book_16].
Replications of @Dechow:1995wr and @Sloan:1996wd are also successful.

The second era covers papers from the current century and reveals a different story.
Our book provides replications of numerous papers from this era, including @Zhang:2007tv, @Fang:2016uy, @Li:2018tj, and @Bloomfield:2021va.

The first observation is that we can replicate all of the papers to some degree.
But it is important to note that our replications often benefit from access to code and data provided by the authors.
@Fang:2016uy posted code and data starting from original sources and continuing through the production of (some) key results in their paper.
@Bloomfield:2021va provided code under the journal's data policy.^[We did not have access to code for @Zhang:2007tv, but that paper is unusually straightforward and based on a standard data set (CRSP).]

The main purpose in selecting papers for replication in the book was pedagogical and being able to replicate results was an important criterion for inclusion.
In some cases we were not able to replicate papers---and authors were not responsive to requests for assistance---that had been considered for inclusion. 
If authors do not share their code and data, replication is often difficult.

The second observation is that the results can be very fragile.
The results in @Fang:2016uy on earnings management are robust to some alternative choices [see @Fang:2019tt], but less so to others.
For example, the main measure of earnings management used in @Fang:2016uy is one proposed by @Kothari:2005aa that matches firms with controls based on performance.
But @Kothari:2005aa use contemporary performance, while @Fang:2016uy use lagged performance; use contemporary performance and results vanish.^[See @Black:2022tz for an extensive analysis of the results of @Fang:2016uy.]
Additionally, a strong argument can be made for using measures of accruals that do not condition on post-treatment outcome, such as total accruals or even simply income, but using either makes results disappear.

@Li:2018tj present evidence of firms being less forthcoming with disclosure of customer identities after adoption of the inevitable disclosure doctrine in the states in which they are headquartered.
But these results rely on dubious research design choices.
As discussed in @far_book_23, almost any tweak of the choices causes results to disappear.

@Bloomfield:2021va claims to use regression discontinuity design (RDD), but actually does not.^[To be fair, many papers in accounting research that claim to use RDD generally do not do so. 
See @far_book_24 for details.]
Replace the simple difference-in-difference analysis of @Bloomfield:2021va with a proper RDD analysis and results vanish.

In short, none of the papers replicated in our book in the second era is anything but extremely fragile, just as we would expect p-hacked results to be.^[It is important to caveat that I do not claim to have proven that any one of these papers *is* a p-hacked paper.
Direct evidence of p-hacking is general impossible to come by and p-hacking is something more easily inferred for an area of research than for a single paper [@Ioannidis:2005aa].]
If other papers where authors do not provide detailed code and data are similarly fragile, then we would not expect to be able to replicate their results, as minor deviations from the data and analytical procedures of the original authors are likely to lead to null results.
Thus p-hacking provides an explanation for the difficulty many researchers have in replicating results in published papers.

Combining the evidence above with the concerns raised by @Ohlson:2023aa and it seems difficult to distinguish much of contemporaneous accounting research from what you would see if p-hacking were the modus operandi of most researchers.

# What to do?

If p-hacking is as prevalent as it seems to be, the natural question is what, if anything, can be done about it.
Before addressing this, it is important to note how pernicious p-hacking is to the value of research. 
If all research is p-hacked, then we should simply ignore research, as p-hacking does not produce information of value other that insights into the p-hacking skills of the authors.^[Of course, information about the p-hacking skills of the authors is arguably relevant if the ability to produce published papers is the sole research-related criterion for evaluating a researcher, as it is at many institutions.]

Some researchers appear to recognize the prevalence of p-hacking, yet remain sanguine about the research enterprise.
For example, one senior researcher broadly agrees with my assessment about p-hacking, but argues "there are some solid researchers doing some interesting papers."
But it is important to understand that if, say, 80% of research is p-hacked, that one cannot simply read the 20% that is not p-hacked and ignore the rest.
If it were easy to detect the p-hacked papers, we could simply avoid publishing them.

That said, I argue there are steps that could be taken to reduce p-hacking to an extent that research in aggregate might again have some value.
In this section, I discuss four ideas for addressing concerns about p-hacking.

## Reject papers that ask silly questions

Accounting academics appear to adore "novelty", where novelty often means asking questions that no-one has even dreamed of asking before.
This is problematic for two reasons.
First, if questions are so novel that no-one has asked them, how can they be important?
Second, the ability to simply make up "interesting" research questions is a p-hacker's dream.^[Think "What is effect of the decline in US science on suicide rates?" as a paper title based on the correlation from Tyler Vigen's website that I discussed above.]

Too often the bar seems to be "has someone [in prior research] asked this question before?" and if the answer is "no" then the novelty bar has been cleared.
But, after more than 50 years of modern empirical accounting research, the fact that no-one has asked a question should in most cases be a strike *against* a paper, not for it.
If no-one has addressed the question, then it is perhaps because no-one cares what the answer is.
If editors adopted a policy of desk-rejecting papers that ask silly questions, the pay-off to p-hacking would decline significantly.

Of course, one reason for researchers needing to seek out smaller and smaller questions is simply the weight of prior research.
Going first, researchers such as @Ball:1968ub and @Beaver:1968vf could pick the "low-hanging fruit" of more fundamental questions of the discipline, and test alternative hypotheses that were more likely to be true and hence easier to demonstrate empirically.
Later researchers were left to to explore the questions that remain.
As such, it is perhaps "unfair" to compare research in the period from, say, 2000 to today with that in the period 1968--1999.

But this argument would suggest that, as research progresses, we should be seeing more and more papers with null results, either because the alternative hypotheses tested are less likely to be true or because the empirical challenges faced in demonstrating them are greater.
This seems inconsistent with the reality that almost all published papers have "results", either ruling out the unfairness or suggesting that researchers compensate for the smallness of the (apparent) phenomena they study by simply looking harder (i.e., p-hacking).

## Increase emphasis on replication

We saw above that being able to replicate papers such as  @Fang:2016uy and @Li:2018tj makes it easy to see just how fragile their results are.
If it were easy to replicate papers, then the incentives for p-hacking might be dramatically reduced, as it would be easy to raise doubts about papers with the very fragile results that p-hacking usually produces.
One would hope that papers shown to be extremely fragile would be regarded as less reliable, and thus less likely to cited or to be evaluated positively by peers after publication, thereby reducing incentives for their production.^[Note that this reduction in incentives for producing p-hacked papers would be much reduced for researchers whose incentives focus on the number of papers produced, whether those papers are cited or regarded highly.
Such incentives are created by many institutions around the world, including my current one.]
Unfortunately, the salutary effects that replications can have on incentives for p-hacking are much diminished for a number of reasons.

First, replication is a costly exercise.
Most empirical researchers already spend a large portion of their research time in the critical pre-tenure phase of their careers writing code to analyse data.
Independently replicating others' papers is likely to be considered a poor use of very limited time.
And, as discussed above, a typical replication conducted without some cooperation from the original authors is likely to yield differences in results that are difficult to explain, requiring exhaustive checks and iterations to understand them.

While one solution to this issue is for authors to supply the data and code needed to replicate their results, very few authors do so.
Authors have essentially no incentive to provide data and code voluntarily.
Once a paper has been published, there is really only downside from sharing code and data for an author focused on publishing papers, as the results might be due to coding errors or fragile.
Thus authors have a natural incentive to make replication difficult.

On top of this lack of positive incentives is the reality that most researchers appear to be poor in organizing their code and that significant costs would need to be incurred to prepare code and data for sharing.^[This is likely to be especially true when results are derived from the often messy process of p-hacking.]

In the absence of incentives for voluntary disclosure of code, some kind of requirement for sharing seems necessary.
However, only one of the top three accounting journals (*Journal of Accounting Research*) imposes [requirements](https://onlinelibrary.wiley.com/page/journal/1475679x/homepage/ForAuthors.html) for data and code, but even then these requirements rarely yield files that permit easy replication of tables found in papers.^[The *Journal of Accounting and Economics* merely "[encourages](https://www.elsevier.com/journals/journal-of-accounting-and-economics/0165-4101/guide-for-authors)" authors to share replication files. 
And there is nothing on this issue in the [editorial policy](https://meridian.allenpress.com/DocumentLibrary/AAAN/TAR_Editorial_Policy.pdf) of *The Accounting Review*, which does not appear to provide any support for such sharing.]

There are two steps that journals could take to enhance the credibility of results.
First, journals could step up the data and code requirements for published papers.
While the *Journal of Accounting Research* is a clear leader in this regard in accounting research, journals in other disciplines have gone further and there is plenty of room for improvement.^[See for example the "data and code sharing policy" of the *Journal of Financial Economics* as outlined on its website [https://www.jfinec.com/data-and-code-sharing-policy](https://www.jfinec.com/data-and-code-sharing-policy).]

Second, journals could publish replications of papers when these provide insights on the questions in the original papers.
For example, @Guest:2021aa identified "six discrepancies in ... reporting, coding, and data" in replicating a previously published paper. 
The *Journal of Finance* published the replication and retracted the original paper.
Because journals generally do not publish replications or corrections, there is essentially no incentive to produce these in a world where counting published papers is a dominant (often the only) research-related performance measure.

## Decrease emphasis on "identification strategies"

It is widely understood that accounting research has become increasingly concerned about "identification strategies" in recent years.
Identification strategies---to use the term in common use---seek to enhance the credibility of causal inferences in empirical research by exploiting features of the research setting and purportedly appropriate statistical techniques.
By focusing on identification strategies, accounting research may have reduced its immunity to p-hacking.
The apparent obsession with papers with "clever" identification strategies seems to have led to a new kind of p-hacking in which a researcher starts with the identification strategy (often drawn from finance and economics) and then seeks statistically significant results using outcomes popular in accounting research, such as earnings management or voluntary disclosure.

The extremely fragile results of @Fang:2016uy and @Li:2018tj seem to be plausible candidates for this phenomenon.
@Fang:2016uy exploits the random assignment of elimination of short-selling restrictions, but so do 60 other papers exploring "indirect effects" of these restrictions in a setting where little or no evidence of direct effects was found.

Apart from the potential inducement of p-hacking, concerns about the obsession with identification strategies in accounting research are increased when one considers the credibility of these strategies in practice.
Many papers simply use difference-in-difference regressions---perhaps including "fixed effects"---which rely on the "assume a can-opener" assumption of "parallel trends".^[I label this an "assume a can-opener" assumption because its justification comes from the econometric benefits of making it and not at all from any underlying economic rationale for it. See  discussion of the implausibility of the "parallel trends" assumption in general in @far_book_21.]
Papers use instrumental variables, even though it's doubtful that any valid instruments exist in accounting research.^[See @far_book_22 for more on this point.]
Papers in accounting research that claim to use RDD generally do not.^[See @far_book_24 for a recent survey of the use of RDD in accounting research.]

## Incorporate discussion of p-hacking into research training

One hopes that accounting research training has not "evolved" to the point that PhD students are being instructed in how to do p-hacking.
Instead, students learn about p-hacking "on the job" in a sense.
Understanding the importance of "results", students learn to exercise researcher degrees of freedom in ways that eventually yield the "stars" denoting "statistically significant" coefficients.
Given these incentives, it seems important that the problems with p-hacking are addressed more forthrightly in training PhD students.

In this regard, the recent explosion of high-quality material on research methods is somewhat disappointing.
Recent years have seen the emergence of high-quality resources for students looking to understand causal inference using observational data, including @Angrist:2008vk, @Angrist:2014aa, @Cunningham:2021vk, and @Huntington-Klein:2021aa.
While these are excellent resources for helping researchers to understand subtle issues not explicitly addressed by more traditional texts, none of them even touches on the topic of p-hacking.
We offer an initial attempt to incorporate this topic into a PhD curriculum in our course book and hope that others find ways to build content on this topic into their PhD curricula so as to raise awareness of these issues [@    far_book_23].

## Encourage more descriptive research

@Gow:2016kn [p. 499] point out that "there are very few studies published in top accounting journals that focus on providing detailed descriptions of institutions in accounting research settings" and document that the vast majority of empirical accounting papers focus on providing causal inference, notwithstanding all the difficulties widely understood to be faced by that endeavour.
This focus on causal inference is accompanied by a desire to find "positive" results, which provides the incentives for p-hacking I have discussed.

In arguing that "accounting research can benefit substantially from more in-depth descriptive research", @Gow:2016kn [p. 499] suggest that "this type of research is essential to improve our understanding of causal mechanisms and [to] develop structural models."
An additional benefit of such descriptive research is that it would enhance the understanding of the research community of how the real world works, making it more difficult to pass off p-hacked results that are not consistent with actual business practices.

# Concluding comments

@Ohlson:2023aa draws on his experience in empirical accounting seminars to identify five "elephants in the room".
I interpret each of these elephants as either a variant or a symptom of p-hacking.
I provide evidence of the prevalence of p-hacking in accounting research that complements the observations made by @Ohlson:2023aa.

While I identify a number of steps that could be taken to reduce p-hacking in accounting research,
I conjecture that facilitating and encouraging replication alone could have profound effects on the quality and quantity of empirical accounting research.

# Appendix: The other elephants {#sec-other-elephants}

Above I explained that the basic Elephant #5 of @Ohlson:2023aa ("Issues Related to 'Screen-Picking' and 'Data-Snooping'") is synonymous with p-hacking.
For completeness, I close this paper with a brief discussion of how the other four elephants also reflect concerns with p-hacking.

Elephant #1 ("Referring to the Absence of a Fama-MacBeth Analysis") is likely to be seen when researchers are reluctant to adjust their standard errors in ways that make results disappear [see @Gow:2010ub for discussion of approaches to calculating standard errors in accounting research].

Elephant #2 ("Asking whether a Key Right-Hand-Side (RHS) Variable Contributes to Explaining the Dependent Variable") and #3 ("It Takes More than Stars to Settle the Matter") both raise uncomfortable questions when the results are p-hacked, as the explanatory value of the independent variable is likely to be low and the economic significance of any apparent relation is likely to be small when the sample is large.

Elephant #4 ("Referring to the Possibility of Using a Holdout Sample") is also an awkward idea when a paper is based on p-hacked results, as we do not expect those results to hold in a new sample, pretty much by definition.
Here @Ohlson:2023aa is effectively discussing the idea of replication by the authors of the paper themselves.

# References {-}
