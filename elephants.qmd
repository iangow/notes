---
title: "The elephant in the room: p-hacking and accounting research"
author: "Ian D. Gow"
bibliography: papers.bib
csl: jfe.csl
format: 
    pdf:
        toc: false
        number-sections: true
        colorlinks: true
        mainfont: TeX Gyre Pagella
        sansfont: TeX Gyre Pagella
---

# Five elephants or one?

@Ohlson:2022aa identifies five of what he calls "elephants in the room" (or topics considered taboo in seminars).
I read these not so much as five elephants, but as five alternative descriptions of the one elephant, much like the elephant in the parable of the [blind men and an elephant](https://en.wikipedia.org/wiki/Blind_men_and_an_elephant).

What exactly is that elephant in the room?
I argue that Ohlson's five elephants are simply alternative perspectives on the same elephant, i.e., p-hacking, which is the practice whereby researchers search for "significant" and "positive" results.^[Here "significant" refers to statistical significance and "positive" refers to results that reject so-called "null hypotheses" and thereby (purportedly) push human knowledge forward.]
To be sure, this is a very big elephant: I conjecture that p-hacking is the dominant mode of research in academic accounting in 2022.

That the basic concern of @Ohlson:2022aa is with p-hacking is clearest with the last of his five elephants: "Issues related to 'screen-picking' and 'data-snooping'".
Terms like "data-snooping" are simply synonyms of p-hacking.
The key insight of @Ohlson:2022aa may be in highlighting how merely suggesting the possibility of p-hacking is taboo (@Ohlson:2022aa uses terms such as "unacceptable", "a personal assault", "too sordid", "testing ethical boundaries", and "a more or less painful private matter").

Many researchers appear not to understand how p-hacking vitiates the whole research endeavour.
So if even suggesting the possibility p-hacking is taboo, it will be much more difficult to address and accounting research will continue to be a largely pointless exercise.^[Some researchers agree with the pointlessness of accounting research with regard to expanding human knowledge, but argue that the real value of research is in deciding who gets tenure at top universities.
But this merely raises the question of the merits of making these decisions based on skills related to conducting and packaging p-hacked research, which seem unclear to say the least.]

# The anatomy of p-hacking

In a recent [Financial Times article by Robert Wigglesworth](https://www.ft.com/content/9025393f-76da-4b8f-9436-4341485c75d0) Campbell Harvey, professor of finance at Duke University, suggests that "at least half of the 400 supposedly market-beating strategies identified in top financial journals over the years are bogus."
@Harvey:2017ux cites research suggesting that 90% of published studies report the "significant" and "positive" results.
Reporting "positive" results is important not only for getting published, but also for attracting citations, which drive behaviour for both researchers and journals.

@Simmons:2011ux [p. 1359] provide analyses that "demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis ... [due to] flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates."

@Simmons:2011ux [p. 1359] attribute this flexibility to **researcher degrees of freedom**.
"In the course of collecting and analyzing data, researchers have many decisions to make: Should more data be collected?
Should some observations be excluded?
Which conditions should be combined and which ones compared?
Which control variables should be considered?
Should specific measures be combined or transformed or both?" @Simmons:2011ux [p.
1364] identify another well-known researcher degree of freedom, namely that of "reporting only experiments that 'work'", which is known as the **file-drawer problem** (because experiments that don't "work" are put in a file-drawer).

Another form of p-hacking is **HARKing** (from "Hypothesizing After Results are Known").
In its extreme form, HARKing involves searching for a "significant" correlation and *then* developing a hypothesis to "predict" it.
To illustrate, consider the [spurious correlations website](http://tylervigen.com/spurious-correlations) provided by Tyler Vigen.
This site lists a number of evidently spurious correlations, such as the 99.26% correlation between the divorce rate in Maine and margarine consumption or the 99.79% correlation between US spending on science, space, and technology and suicides by hanging, strangulation and suffocation.
The correlations are deemed spurious because normal human beings have strong prior beliefs that there is no underlying causal relation explaining these correlations.
Instead, these are regarded as mere coincidence.

However, a creative academic can probably craft a story to "predict" any correlation: Perhaps increasing spending on science raises its perceived importance to society.
But drawing attention to science only serves to highlight how the US has inevitably declined in relative stature in many fields, including science.
While many Americans can carry on notwithstanding this decline, others are less sanguine about it and may go to extreme lengths as a result ... .
This is a clearly silly line of reasoning, but if one added some references to published studies and fancy terminology, it would probably read a lot like the hypothesis development sections of academic papers presented in the empirical accounting seminars discussed by @Ohlson:2022aa.

@Bloomfield:2018va [p. 317] suggest that "almost all peer-reviewed articles in social science are published under" what they call ... the Traditional Editorial Process (or TEP).
Under the TEP, "authors gather their data, analyze it, and write and revise their manuscripts repeatedly before sending them to editors." As such authors have access to many researcher degrees of freedom.

An alternative to the TEP is what @Bloomfield:2018va call the Registration-based Editorial Process (REP).
According to @Bloomfield:2018va [p. 317], "under REP, authors propose a plan to gather and analyze data to test their predictions.
Journals send promising proposals to one or more reviewers and recommend revisions.
Authors are given the opportunity to review their proposal in response, often multiple times, before the proposal is either rejected or granted in-principle acceptance ... regardless of whether [subsequent] results support their predictions." 
The REP is designed to eliminate research degrees of freedom and the questionable research practices that these permit.

The *Journal of Accounting Research* (JAR) conducted a trial of the REP for its [annual conference held in May 2017](https://research.chicagobooth.edu/arc/journal-of-accounting-research/jar-annual-conference/conference-web-casts/2017).
Unfortunately, it is unlikely that the REP will replace the TEP to any great extent in the foreseeable future.
The REP is feasible when data are generated in randomized controlled trials (RCTs), as the data simply do not exist when the report is registered.
In contrast, most empirical accounting research uses existing archival data making it impossible to register a report before being able to look at the data.

# Evidence of p-hacking

## Conversational evidence

@Ohlson:2022aa's notion that raising issues related to p-hacking is taboo in empirical accounting seminars seems very plausible.
Outside of papers like @Simmons:2011ux that aim to demonstrate the "power" of p-hacking, we generally only see circumstantial evidence of p-hacking in the papers themselves.^[I discuss such evidence below.]
But sometimes (outside of seminars!) researchers can be fairly candid about their research process, as the following anecdotes reveal.
While I present these anecdotes in a stylized fashion, they are based on actual conversations with colleagues.^[Also see my [LinkedIn post](https://www.linkedin.com/pulse/p-hacking-accounting-research-ian-gow) on this topic.]

### Anecdote #1 {-}

- Me: "Why did you look at $y_1$?
Wouldn't $y_2$ or $y_3$ be the natural things to examine?" 
- Doctoral student(s) at a good school: "I already looked at those.
They didn't work, so I dropped them."

### Anecdote #2 {-}

- Me: "Your paper is about $y_1$.
Wouldn't $y_2$ or $y_3$ be the natural things to examine as consequences of $X$?" 
- Senior faculty at a good school: "We already looked at those.
They didn't work, so we wrote a paper about $y_1$ instead."

### Anecdote #3 {-}

- Colleague: "I have found that $x$ and $y$ are associated and I think the reason is [some causal story]."
- Me: "But [some causal story] just isn't plausible." 
- Colleague: "Oh. So you think I need to come up with some other story then?"

### Anecdote #4 {-}

- Colleague of colleague to colleague: "I have data on $X$, perhaps I can send you these data and you can regress various $y$ variables that you have on $X$.
If something works, perhaps we could collaborate."

Outside the setting of seminars, gentle prodding can lead authors to reveal p-hacking.
The first two anecdotes are similar and reveal a kind of "old school" p-hacking of the kind underlying the file-drawer problem.
The last two reflect a seemingingly mindless search that only requires "predictions" when "results" have been found.

## Evidence from the 2017 JAR REP trial

@Bloomfield:2018va [p. 326] examine the results reported in the 2017 JAR conference papers and conclude that "of the 30 predictions made in the ... seven proposals, we count 10 as being supported at $p \leq 0.05$ by at least one of the 134 statistical tests the authors reported." 
But this is very close to the level of support expected if the null hypotheses for all 30 predictions were true.
This is particularly concerning in that it seems reasonable to expect that the alternative hypotheses considered in the 2017 JAR conference papers were deemed by the authors and reviewers to be worth pursuing before knowing their results, which is a higher bar than applied to hypotheses tested using the TEP.
In other words, the results of the 2017 JAR conference raised the prospect that many results produced by the TEP (i.e., almost all research in accounting) are simply false rejections of true null hypotheses.

## Circumstantial evidence from replications

Another source of evidence on the prevalence of p-hacking is replications.
We expect that p-hacked papers will have results that are very fragile.
By definition, p-hacked results are not expected to be **reproducible**.
That is, we would not expect the results to hold if the same analytical procedures were applied to a new data set.^[Here I follow @Hail:2020aa in distinguishing replicability from reproducibility.]

But p-hacked results should pass the test of **replicability**, which requires that the results can be produced by other authors using the same data sets and analytical procedures.
But in practice, it seems that few results can be replicated, as small departures from the choices made by the authors along the dimensions described in @Simmons:2011ux can lead to apparent results disappearing (i.e., becoming statistically insignificant) and few papers describe these choices sufficiently clearly to allow precise replication. 

While one solution to this issue is for authors to supply the data and code needed to replicate their results, for a number of reasons, very few authors do so.

Authors have essentially no incentive to provide data and code voluntarily.
Once a paper has been published, for an author focused on publishing papers, there is really only downside from sharing code and data.
The results in the published paper might result from coding errors.
And if the results have been p-hacked, sharing code and data dramatically lowers the cost of showing how fragile the results are.
On top of this lack of positive incentives is the reality that most researchers appear to be poor in organizing their code and that significant costs would need to incurred to prepare code and data for sharing.^[This likely to be especially true when results are derived from the often messy process of p-hacking.]

In the absence of incentives for voluntary disclosure of code, some kind of requirement for sharing seems necessary.
However, only one of the top three accounting journals (*Journal of Accounting Research*) imposes [requirements](https://onlinelibrary.wiley.com/page/journal/1475679x/homepage/ForAuthors.html) for data and code, but even then rarely do these requirements yield files that permit easy replication of tables found in papers.^[The *Journal of Accounting and Economics* merely "[encourages](https://www.elsevier.com/journals/journal-of-accounting-and-economics/0165-4101/guide-for-authors)" authors to share replication files. 
And there is nothing on this issue in the [editorial policy](https://meridian.allenpress.com/DocumentLibrary/AAAN/TAR_Editorial_Policy.pdf) of *The Accounting Review*, which does not appear to provide any support for such sharing.]

Given that much of the published research in accounting uses data sets that are available to most researchers and papers typically include descriptions of the analytical procedures used, other researchers should be able to replicate results independently even without access to code and data.
In practice, few papers are replicated in this way.

First, replication is a costly exercise.
Most empirical researchers already spend a large portion of their research time in the critical pre-tenure phase of their careers writing code to analyse data.
Independently replicating others' papers is likely to be considered a poor use of very limited time.

Second, even if one invests in replication, anecdotal evidence suggests that reproducing results in published papers can be very difficult.
Ask another researcher whether she has tried to replicate results of a published paper and you are likely to hear that attempts have been made, but without success.^[Some researchers' replication experiences are limited to exercises assigned during PhD coursework, but there is a natural selection bias with these, as many instructors would look to assign exercises where results can be reproduced.]

I have extensive experience with attempted replication of papers.
My ill-fated PhD dissertation attempted to identify a causal mechanism underlying the numerous results in the literature suggesting a contracting value for firms voluntary adoption of higher levels of conditional conservatism.
My paper sought to identify the mechanism that firms used to make these choices.
However, explaining results documented in research proved difficult when those results simply cannot be replicated, as almost all replications I tried failed.

Since then I have undertaken many attempted replications and most fail.
One theory might be that I simply do not know how to analyse data properly and that a more skilled researcher would be able to reproduce published results.
While it is difficult to refute this theory, I believe a significant recent project offers evidence against it.

In 2021, a University of Melbourne colleague (Tony Ding) and I started to pull together a course book [@Gow:2022aa] aimed at helping research students to develop the portfolio of skills needed to be good researchers in accounting.
As discussed [in the book](https://iangow.github.io/far_book/introduction.html#features-of-this-course), a core element is material focused on data analysis skills, included many replication analyses.
It seems these replication efforts can be organized into two eras.

The first era covers 1968 through to about 1996.
The striking thing about this era is how robust the results appear to be.
Like many before us, [we find](https://iangow.github.io/far_book/bb1968.html) that the key results of the seminal @Ball:1968ub are easily replicated, and @Ball:2019wu show this is true in different markets and periods.
[We find](https://iangow.github.io/far_book/beaver68.html) that key results of @Beaver:1968vf hold in any year we look at.^[@Bamber:2000wv raise concerns about the reproducibility of the results, concerns that do not appear to hold in years after @Beaver:1968vf was published.]
Not only can [we generate the core results](https://iangow.github.io/far_book/pead.html) of @Bernard:1989uu, but [we broadly replicate](https://iangow.github.io/far_book/pead.html#time-series-properties-of-earnings) @Foster:1977wy along the way.
Replications of @Sloan:1996wd and @Dechow:1995wr are also successful.

The second era covers papers from the current century and reveals a different story.
Our book provides replications of numerous papers from this era, including @Zhang:2007tv, @Fang:2016uy, @Li:2018tj, and @Bloomfield:2021va.

The first observation is that we can replicate all of the papers to some degree.
But it is important to note that our replications often benefit from access to code and data provided by the authors.
@Fang:2016uy posted code and data starting from original sources and continuing through the production of (some) key results in their paper.
@Bloomfield:2021va provided code under the journal's data policy.^[We did not have access to code for @Zhang:2007tv, but that paper is unusually straightforward and based on a standard data set (CRSP).]
The main purpose in selecting papers for replication and inclusion in the book was pedagogical, but in some cases we were not able to replicate papers and authors were not responsive to requests for assistance.
In general, if authors do make code and data available, replication can be difficult.

The second observation is that the results can be very fragile.
The results in @Fang:2016uy on earnings management are robust to some alternative choices [see @Fang:2019tt], but less so to others.
For example, the main measure of earnings management used in @Fang:2016uy is one proposed by @Kothari:2005aa that matches firms with controls based on performance.
But @Kothari:2005aa use contemporary performance, while @Fang:2016uy use lagged performance; use contemporary performance and results vanish.
Additionally, a strong argument can be made for using measures of accruals that do not condition on post-treatment outcome, such as total accruals or even simply income, but using either makes results disappear.

@Bloomfield:2021va claims to use regression discontinuity design (RDD), but actually does not.
Replace the @Bloomfield:2021va's simple difference-in-difference analysis with a proper RDD analysis and results vanish.
@Li:2018tj present evidence of firms being less forthcoming with disclosure of customer identities after adoption of the inevitable disclosure doctrine in the states in which they are headquartered.
But these results rely heavily on collapsing observations from three or more years after adoption of IDD into the same group as those $t+2$ (two years after adoption).
If one digs into the results on a by-year basis, one sees that the results are driven by positive coefficients in years beyond $t+5$ and negative coefficients in years $t - 12$ through $t - 10$.
For many observations in the sample, years such as $t - 12$ and $t + 5$ are not even defined.
Fixing a sample for which exist in $t + 4$ and results vanish.^[These analyses are explored in discussion questions found [here](https://iangow.github.io/far_book/panel-data.html).]

In short, none of the papers replicated in our book in the second era is anything but incredibly fragile, as we would expect p-hacked results to be.
Combining the evidence above with the concerns raised by @Ohlson:2022aa and it seems reasonable to conclude that p-hacking is the primary modus operandi of accounting researchers in 2022.

# What to do?

If p-hacking is as prevalent as it seems to be, the natural question is what, if anything, can be done about it.
Before addressing this, it is important to note how pernicious p-hacking is to the value of research. 
If all research is p-hacked, then we should simply ignore research, as p-hacking does not produce information of value other some insights to the p-hacking skills of the authors.^[Of course, information about the p-hacking skills of the authors is arguably relevant if the ability to produce published papers is the sole research-related criterion for evaluating a researcher, as it is at many institutions.]

But some researchers I have spoken to recognize the prevalence of p-hacking, but remain sanguine about the research enterprise.
For example, one senior researcher agrees with my criticism of accounting research, but argues "there are some solid researchers doing some interesting papers."
But it is important to understand that if, say, 90% of research is p-hacked, that one cannot simply read the 10% that is not p-hacked and ignore the rest.
If it were easy to detect the p-hacked papers, we could simply avoid publishing them.

That said, I argue there are steps that could be taken to reduce p-hacking to an extent that research in aggregate might again some value.

## Reject papers that ask silly questions

Accounting academics appear to adore "novelty", where novelty often means asking questions that no-one has even dreamed of asking before.
This is problematic for two reasons.
First, if questions are so novel that no-one has asked them, how can they be important?
Second, the ability to simply make up "interesting" research questions is a p-hacker's dream.

Too often the bar seems to be "has someone [in prior research] asked this question before?" and if the answer is "no" then the novelty bar has been cleared.
But after more than 50 years of modern empirical accounting research, the fact that no-one has asked a question should in most cases be a strike *against* a paper, not for it.
If no-one has addressed the question, then it is perhaps because no-one cares what the answer is.
If editors adopted a policy of desk-rejecting papers that ask silly questions, the pay-off to p-hacking would decline significantly.

In some ways, the comparison of the two eras above is unfair.
Papers in the fifty years after @Ball:1968ub and @Beaver:1968vf addressed the fundamental questions of the discipline, leaving more

## Increase emphasis on replication

There are two steps that journals could take to enhance the credibility of results.
First, journals could step up the data and code requirements for published papers.
While the *Journal of Accounting Research* is a clear leader in this regard in accounting research, journals in other disciplines have gone further and there is plenty of room for improvement.

Second, journals could publish replications of papers when these provide insights on the questions in the original papers.^[See [a LinkedIn post](https://www.linkedin.com/pulse/facilitating-replication-research-ian-gow) I made on this topic for more details.]
For example, @Guest:2021aa identified "six discrepancies in ... reporting, coding, and data" in replicating a previously published paper. 
The *Journal of Finance* published the replication and retracted the original paper.

## Decrease emphasis on "identification strategies"

It is widely understood that accounting research has become increasingly concerned about "identification strategies" in recent years.
Identification strategies---to use the term in common use---seek to enhance the credibility of causal inferences in empirical research by exploiting features of the research setting and purportedly appropriate statistical techniques.
By focusing on identification strategies, it seems that accounting research has developed a lack of immunity to p-hacking.
The apparent obsession with papers with "clever" identification strategies seems to have led to a new kind of p-hacking in which a researcher starts with the identification strategy (often drawn from finance and economics) and then seeks statistically significant results using outcome variables popular in accounting research, such as earnings management or voluntary disclosure.

The extremely fragile results of @Fang:2016uy and @Li:2018tj seem to be plausible candidates for this phenomenon.
@Fang:2016uy exploits the random assignment of elimination of short-selling restrictions, but so do 60 other papers exploring "indirect effects" of these restrictions in a setting where little or no evidence of direct effects was found.

Apart from its inducement of p-hacking, the misguided nature of the obsession with identification strategies in accounting research is made clear when one considers the credibility of these strategies.
Many papers simply use difference-indifference regressions---perhaps including "fixed effects"---that rely on an "assume a can-opener" assumption "parallel trends".^[I argue that this is assuming a can-opener because the justification for the assumption comes from the econometric benefits of making it and not at all from any underlying economic rationale for it. See [this discussion](https://iangow.github.io/far_book/natural-revisited.html#parallel-trends) of the implausibility of the "parallel trends" assumption in general.]
Papers use instrumental variables, even though it's doubtful that any valid instruments exist in accounting research.^[See [here](https://iangow.github.io/far_book/iv.html) for more on this point.]
Papers in accounting research that claim to use RDD generally do not.^[See [here](https://iangow.github.io/far_book/regression-discontinuity-designs.html#rdd-in-accounting-research) for a recent survey of the use of RDD in accounting research.]

## Incorporate discussion of p-hacking into research training

One hopes that accounting research training has not "evolved" to the point that PhD students are being instructed in how to do p-hacking.
Instead, students learn about p-hacking "on the job" in a sense.
Understanding the importance of "results", students learn to exercise reseacher degrees of freedom in ways that eventually yield the "stars" denoting "statistically significant coefficients.
Given these incentives, it seems important that the problems with p-hacking are addressed more forthrightly in explicit training of PhD students.

In this regard, the recent explosion of high-quality material on research methods is somewhat disappointing.
Recent years have seen the emergence of high-quality resources for students looking to understand causal inference using observational data, including @Angrist:2008vk, @Angrist:2014aa, @Cunningham:2021vk, and @Huntington-Klein:2021aa.
While these are excellent resources for helping researchers to understand subtle issues not explicitly addressed by more traditional texts, none of them even touches on the topic of p-hacking.^[We offer initial attempt to incorporate this topic into a PhD curriculum [here](https://iangow.github.io/far_book/natural-revisited.html) and hope that others find ways to incorporate the topic into the PhD curriculum.]

# References {-}
