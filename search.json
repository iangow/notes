[
  {
    "objectID": "sas_to_pd.html",
    "href": "sas_to_pd.html",
    "title": "Using SAS to create pandas data",
    "section": "",
    "text": "A strong point of pandas is its expressiveness. Its API allows users to explore data using succinct and (generally) intuitive code. However, some of this expressiveness relies on data being in forms (for example, with dates ready to serve as an index) that often differ from the data we have, and pandas can struggle to manipulate the data into those forms, especially with larger data sets.\nSAS might be another approach to manipulating data for pandas. My Python package wrds2pg offers a sas_to_pandas() function that can run code on the WRDS server and return the results as a pandas dataframe. While not quite as fast as using Ibis with the PostgreSQL server, SAS performs pretty well with this task.\n\n\n\n\n\n\nTip\n\n\n\nThe following command (run in the terminal on your computer) installs the packages you need.\n\npip install wrds2pg --upgrade\npip install pandas\n\nThe code assumes you have set the environment variable WRDS_ID to your WRDS ID.\nThis note was written using Quarto. The source code for this note is available here and the latest version of this PDF is here."
  },
  {
    "objectID": "dsf_polars.html",
    "href": "dsf_polars.html",
    "title": "The best of both worlds: Using modern data frame libraries to create pandas data",
    "section": "",
    "text": "A strong point of pandas is its expressiveness. Its API allows users to explore data using succinct and (generally) intuitive code. However, some of this expressiveness relies on data being in forms (for example, with dates ready to serve as an index) that often differ from the data we have, and pandas can struggle to manipulate the data into those forms, especially with larger data sets.\nA number of modern data frame libraries have emerged that address weaknesses of pandas. In this note, I use polars and Ibis to show how one can use these libraries to get the data into a form in which pandas can shine.\nWhile the underlying data would occupy over 10 GB in memory, the polars variant below runs in about half a second. This approach may have particular appeal to pandas experts because the code is likely more familiar to experienced analysts of data frames.\nI consider two Ibis alternatives. The first uses the same underlying parquet files used in the polars variant, but perhaps takes even less time than polars does. The second tweaks just a small portion of the earlier Ibis code to source the underlying data directly from the WRDS PostgreSQL database. Even so, it takes less than a second to run.\nSo we get the best of both worlds. We can use modern, lazy libraries to perform the heavy data manipulation, and then hand off a compact result to pandas for exploration and visualization.\n\n\n\n\n\n\nTip\n\n\n\nThis note uses several Python packages and parts of it rely on the existence of a local data repository of parquet files for WRDS data of the kind described in Appendix E of Empirical Research in Accounting: Tools and Methods. The following command (run in the terminal on your computer) installs the packages you need.\n\npip install 'ibis-framework[duckdb, postgres]' pandas polars db2pq\n\nThe code assumes you have set the environment variables DATA_DIR and WRDS_ID to point to the location of the parquet repository on your computer and your WRDS ID, respectively.\nThe necessary files for the parquet repository for this note can be created using the db2pq package using the following code. Note that wrds_update_pq(\"dsf\", \"crsp\") will take about 12 minutes with a fast connection to WRDS, but only runs if the file on your computer is not current with the data on the WRDS server.\n\nfrom db2pq import wrds_update_pq\n\nwrds_update_pq(\"dsf\", \"crsp\")  \nwrds_update_pq(\"stocknames\", \"crsp\");\n\ncrsp.dsf already up to date.\ncrsp.stocknames already up to date.\n\n\nThis note was written using Quarto. The source code for this note is available here and the latest version of this PDF is here."
  },
  {
    "objectID": "dsf_polars.html#sec-ibis",
    "href": "dsf_polars.html#sec-ibis",
    "title": "The best of both worlds: Using modern data frame libraries to create pandas data",
    "section": "3.1 Generating Figure 1 using Ibis and DuckDB",
    "text": "3.1 Generating Figure 1 using Ibis and DuckDB\nAn alternative to using polars would be to use Ibis and its default backend, DuckDB.\nI import Ibis and the _ placeholder, as the latter facilitates more succinct code. I also turn on interactive mode to make it easier to inspect the data in tables if I need to do so.\n\nimport ibis\nfrom ibis import _\nibis.options.interactive = True\n\nI next make a load_parquet_ibis() function that I can use to load data from my parquet repository.\n\ndef load_parquet_ibis(con, table, schema, *, data_dir=None):\n    if data_dir is None:\n        data_dir = Path(os.environ[\"DATA_DIR\"]).expanduser()\n\n    path = data_dir / schema / f\"{table}.parquet\"\n    # register the parquet file as an Ibis table\n    return con.read_parquet(str(path), table_name=f\"{schema}_{table}\")\n\nThe short time taken to “load” the data below suggests that, analogous to the results of pl.scan_parquet(), the tables created here are lazy Ibis expressions rather than materialized data frames.\n\n%%ptime\ncon = ibis.duckdb.connect()\ndsf = load_parquet_ibis(con, \"dsf\", \"crsp\")\nstocknames = load_parquet_ibis(con, \"stocknames\", \"crsp\")\n\nWall time: 99.88 ms\n\n\nA lot of the remaining code is a largely a straightforward translation of the polars code into Ibis equivalents. We start by creating tickers.\n\n%%ptime\nend_date = ibis.literal(data.index.max())\n\ntickers = (\n    stocknames\n    .filter(_.ticker.isin(clean_tickers))\n    .filter((_.namedt &lt;= end_date) & (end_date &lt;= _.nameenddt))\n    .select(\"permno\", \"ticker\")\n)\n\nWall time: 1.71 ms\n\n\nThen the Ibis version of dsf_sub from above.\n\n%%ptime\nstart_date = ibis.literal(data.index.min())\n\nnum_cols = [\"prc\", \"ret\", \"retx\"]\n\ndsf_sub = (\n    dsf\n    .inner_join(tickers, predicates=[dsf.permno == tickers.permno])\n    .filter(_.date.between(start_date, end_date))\n    .select(\"ticker\", \"date\", *num_cols)\n    .mutate(**{c: getattr(_, c).cast(\"float64\") for c in num_cols})\n)\n\nWall time: 2.18 ms\n\n\nNote that I am using some Python tricks here. First, *num_cols unpacks the list num_cols into positional arguments. So, .select(\"ticker\", \"date\", *num_cols) is equivalent to .select(\"ticker\", \"date\", \"prc\", \"ret\", \"retx\").\nSecond, ** unpacks a dictionary into keyword arguments. So .mutate(**{c: getattr(_, c).cast(\"float64\") for c in num_cols}) unpacks into the following:\n\n{\n    \"prc\": _.prc.cast(\"float64\"),\n    \"ret\": _.ret.cast(\"float64\"),\n    \"retx\": _.retx.cast(\"float64\"),\n}\n\nAnd finally, the equivalent of dsf_adj. Here I am using window functions, which are well-supported by DuckDB. Originally, I had attempted to use Ibis with a polars backend, but the polars backend does not support window functions with Ibis. It is not clear to me whether this is an inherent limitation of polars, or is simply a gap in the implementation of the polars backend for Ibis that may be addressed in future versions.\nThe code here seems pretty transparent (once you understand the prc = prc_last * growth / growth_last logic discussed above).\n\n%%ptime\n# window for cumulative calculations (up to current row)\nw = ibis.window(group_by=_.ticker, order_by=_.date)\n\ndef cumprod1p(x):\n    # cumprod(1 + x) = exp(cumsum(log(1 + x)))\n    return (1 + x).ln().cumsum().over(w).exp().cast(\"float64\")\n\ndsf_adj = (\n    dsf_sub\n    .mutate(growth = cumprod1p(_.retx))\n    .mutate(\n        prc_last    = _.prc.last().over(w),\n        growth_last = _.growth.last().over(w),\n    )\n    .mutate(prc = _.prc_last * _.growth / _.growth_last)\n    .drop(\"growth\", \"prc_last\", \"growth_last\")\n)\n\nWall time: 2.33 ms\n\n\nAnd the final step is very similar to what we saw with polars. We simply pivot_wider(), sort by date (important for the pandas index) and then execute() to create a pandas date frame, which we can apply .set_index(\"date\") to just like above. Note that pivot_wider() is translated into a group-by aggregate query in SQL. If we don’t specify values_agg=\"max\", it seems that values_agg=\"first\" by default. This works with DuckDB, which has a first() aggregate, but will not work with PostgreSQL, which does not. Because there should be just one prc for each (date, ticker) combination, the use of max() should not affect the output.\n\n%%ptime\ndata_alt = (\n    dsf_adj\n    .select(\"date\", \"ticker\", \"prc\")  \n    .pivot_wider(\n        names_from=\"ticker\",\n        values_from=\"prc\",\n        values_agg=\"max\", \n    )\n    .order_by(\"date\")\n    .execute()\n    .set_index(\"date\")\n)\n\nWall time: 168.11 ms\n\n\nAs before, we .reindex() to make the plot look better.\n\n%%ptime\ndata_alt = data_alt.reindex(data.index)\n\nWall time: 0.59 ms\n\n\nAnd the resulting plot in Figure 3 suggests that again we have successfully recreated the pandas data frame seen in Hilpisch (2019).\n\ndata_alt.plot(figsize=(8, 8), subplots=True);\n\n\n\n\n\n\n\nFigure 3: Stock prices for five firms: Using CRSP, Ibis and DuckDB"
  },
  {
    "objectID": "dsf_polars.html#sec-wrds",
    "href": "dsf_polars.html#sec-wrds",
    "title": "The best of both worlds: Using modern data frame libraries to create pandas data",
    "section": "3.2 Generating Figure 1 using Ibis and WRDS PostgreSQL",
    "text": "3.2 Generating Figure 1 using Ibis and WRDS PostgreSQL\nNow that we have Ibis code, it is a trivial matter to replace the DuckDB backend with a PostgreSQL backend using the WRDS PostgreSQL database. I have my WRDS ID in the environment variable WRDS_ID. Only the first chunk of code below is changed from the Ibis-with-DuckDB version in Section 3.1. This first chunk takes a (relatively!) long time to run because Ibis needs to connect to the WRDS database and query it for metadata about the two tables it is using.\nLooking at Figure 4, it seems that this code also effectively reproduces the data frame created by the pandas code in Section 2.\n\n%%ptime\nwrds_id = os.environ['WRDS_ID']\n\ncon = ibis.postgres.connect(\n    host='wrds-pgdata.wharton.upenn.edu',\n    database='wrds',\n    user=wrds_id,\n    port=9737\n)\n\ndsf = con.table('dsf', database='crsp')\nstocknames = con.table('stocknames', database='crsp')\n\nWall time: 975.74 ms\n\n\nThe remaining code is identical to that above for Ibis.\n\n%%ptime\nend_date = ibis.literal(data.index.max())\n\ntickers = (\n    stocknames\n    .filter(_.ticker.isin(clean_tickers))\n    .filter((_.namedt &lt;= end_date) & (end_date &lt;= _.nameenddt))\n    .select(\"permno\", \"ticker\")\n)\n\nWall time: 2.43 ms\n\n\n\n%%ptime\nstart_date = ibis.literal(data.index.min())\n\nnum_cols = [\"prc\", \"ret\", \"retx\"]\n\ndsf_sub = (\n    dsf\n    .inner_join(tickers, predicates=[dsf.permno == tickers.permno])\n    .filter(_.date.between(start_date, end_date))\n    .select(\"ticker\", \"date\", *num_cols)\n    .mutate(**{c: getattr(_, c).cast(\"float64\") for c in num_cols})\n)\n\nWall time: 1.77 ms\n\n\n\n%%ptime\n# window for cumulative calculations (up to current row)\nw = ibis.window(group_by=_.ticker, order_by=_.date)\n\ndef cumprod1p(x):\n    # cumprod(1 + x) = exp(cumsum(log(1 + x)))\n    return (1 + x).ln().cumsum().over(w).exp().cast(\"float64\")\n\ndsf_adj = (\n    dsf_sub\n    .mutate(growth = cumprod1p(_.retx))\n    .mutate(\n        prc_last    = _.prc.last().over(w),\n        growth_last = _.growth.last().over(w),\n    )\n    .mutate(prc = _.prc_last * _.growth / _.growth_last)\n    .drop(\"growth\", \"prc_last\", \"growth_last\")\n)\n\nWall time: 3.26 ms\n\n\nThe next chunk takes a little longer than the same code chunk took in Section 3.1, because the data need to be transported from the WRDS PostgreSQL server to my computer.\n\n%%ptime\ndata_alt = (\n    dsf_adj\n    .select(\"date\", \"ticker\", \"prc\")  \n    .pivot_wider(\n        names_from=\"ticker\",\n        values_from=\"prc\",\n        values_agg=\"max\",   \n    )\n    .order_by(\"date\")\n    .execute()\n    .set_index(\"date\")\n)\n\nWall time: 1.087 s\n\n\n\n%%ptime\ndata_alt = data_alt.reindex(data.index)\n\nWall time: 1.70 ms\n\n\n\ndata_alt.plot(figsize=(8, 8), subplots=True);\n\n\n\n\n\n\n\nFigure 4: Stock prices for five firms: Using CRSP, Ibis and PostgreSQL"
  },
  {
    "objectID": "dsf_polars.html#footnotes",
    "href": "dsf_polars.html#footnotes",
    "title": "The best of both worlds: Using modern data frame libraries to create pandas data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThough in Section 3.2, I effectively run SQL against the WRDS PostgreSQL database, but using the Ibis library to generate the SQL.↩︎\nThis was on an M4 Pro Mac mini with 24 GB of RAM.↩︎\nThe term “framework libraries” is one I made up. If you go to the Ibis website https://ibis-project.org, Ibis is described as “an open source dataframe library that works with any data system”. But Ibis relies on some other system to provide the execution engine, which it calls the backend. Most data frame libraries provide their own execution engines.↩︎\nNote, some of these data frame libraries are not limited to Python. For example, the polars documentation states that polars “is written in Rust, and available for Python, R and NodeJS [sic].” https://docs.pola.rs↩︎\nNote that I use a little custom “cell magic” %%ptime to produce execution times in a way that prints nicely in the PDF output. See the source code for this document for details.↩︎\nSee the source code for details.↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Notes",
    "section": "",
    "text": "This site publishes a curated set of notes. Use the category filters to find notes by topic.\n\n\n\n\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\n\nDate\n\n\n\nCategories\n\n\n\n\n\n\n\n\nData curation: The case of Call Reports\n\n\nFeb 7, 2026\n\n\nData curation, Polars, DuckDB\n\n\n\n\n\n\nThe best of both worlds: Using modern data frame libraries to create pandas data\n\n\nJan 20, 2026\n\n\nWRDS, Polars, Ibis, pandas\n\n\n\n\n\n\nUsing SAS to create pandas data\n\n\nJan 20, 2026\n\n\nSAS, pandas, wrds2pg\n\n\n\n\n\n\nShared code\n\n\nJan 15, 2026\n\n\nresearch, web data\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "shared_code.html",
    "href": "shared_code.html",
    "title": "Shared code",
    "section": "",
    "text": "Open-source software dominates in certain areas. Probably most internet sites run on Linux machines. Python and R are huge in data science and statistics. All of these systems rely on thousands of open-source packages that are continually being improved in part because anyone can see how they work.\nAcademic research is quite different. Most research papers are really software development projects in disguise. While the final output is a PDF rather than an app, a tremendous amount of coding is often involved and multiple authors can be involved.\nYet the open-source model has not taken off in academia. While some journals are including data-and-code repositories as part of their process, these do not yet dominate. Authors are generally reluctant to share their code and data. There are multiple reasons for this in my view. First, the code is often a “trade secret” of sorts; future papers may be produced and authors may want to retain a competitive edge in what is essentially a zero-sum game.1 Second, having code available risks making it easy to show how fragile results are. It is difficult to replicate most research papers without access to the code and data, and many papers’ results are “fragile” at best. Third, a lot of authors likely fear embarrassment if others could see their code. Academics’ code is often inefficient, difficult to read, perhaps even wrong.\nFor some reason, a lot of the publicly available code in accounting research relates to two seemingly obscure topics: Fama-French industries and winsorization. As we cover both topics in Empirical Research in Accounting: Tools and Methods, I discuss these a little below.\n\n\n\n\n\n\nTip\n\n\n\nIn writing this note, I use several packages including those listed below.2 This note was written using Quarto and compiled with RStudio, an integrated development environment (IDE) for working with R. The source code for this note is available here and the latest version of this PDF is here.\n\nlibrary(dplyr)\nlibrary(farr)"
  },
  {
    "objectID": "shared_code.html#footnotes",
    "href": "shared_code.html#footnotes",
    "title": "Shared code",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nZero-sum because papers need to be published in a finite list of journals, which publish a finite number of papers. Publishing your paper on some topic often means not publishing mine.↩︎\nExecute install.packages(c(\"dplyr\", \"farr\", \"haven\")) within R to install all the packages you need to run the code in this note.↩︎\nAccording to https://siccode.com, “Standard Industrial Classification Codes (SIC Codes) identify the primary line of business of a company. It is the most widely used system by the US Government, public, and private organizations.”↩︎\nThe choices relate to handling of ties and interpolating values. See ? quantile in R for details.↩︎\nWhile the inclusion of prob = 0.01 is not strictly necessary given that that’s the default used by the function, this code does illustrate how you could choose prob = 0.02 to get winsorization at 2% and 98% levels, a popular alternative choice.↩︎"
  },
  {
    "objectID": "curate_call_reports.html",
    "href": "curate_call_reports.html",
    "title": "Data curation: The case of Call Reports",
    "section": "",
    "text": "I recently (Gow, 2026) proposed an extension to the data science “whole game” of R for Data Science (Wickham et al., 2023). In Gow (2026), I used Australian stock price data to illustrate the data curation process and, in this note, I use US bank “Call Report” data as a second illustration. In effect, I provide complete instructions for building a high-performance data library covering all Call Reports data provided by the FFIEC Bulk Data website that can be constructed in less than ten minutes on fast hardware (or a couple of hours on an older machine). I also give a few brief demonstrations of how to use the curated data, with examples for both R and Python. I conclude by discussing challenges encountered during processing and offering some observations about AI and data curation.\nMy extension of the data science “whole game”—depicted in Figure 1 below—adds a persist step to the original version, groups it with import and tidy into a single process, which I call Curate. As a complement to the new persist step, I also add a load step to the Understand process.1\nFigure 1: A representation of the data science workflow\nIn this note, as in Gow (2026), I focus on the data curation (Curate) process. My rationale for separating Curate from Understand is that I believe that thinking about these separately clarifies certain best practices in the curation of data. In Gow (2026), I used the notion of a service-level agreement to explain how the two processes can be delineated. My conception of Curate (Gow, 2026) encompasses some tasks that are included in the transform step (part of the Understand process) of Wickham et al. (2023).\nWhile I will argue that even the sole analyst who will perform all three processes can benefit from thinking about Curate separate from Understand, it is perhaps easiest to conceive of the Curate and Understand processes as involving different individuals or organizational units of the “whole game” of a data analysis workflow. In Gow (2026), I used the idea of a service-level agreement to delineate the division of responsibilities between the Curate and Understand teams. In effect, I will act as a self-appointed, single-person, unpaid Curate team and I imagine potential users of call report data as my Understand clients."
  },
  {
    "objectID": "curate_call_reports.html#sec-raw-data",
    "href": "curate_call_reports.html#sec-raw-data",
    "title": "Data curation: The case of Call Reports",
    "section": "1.1 Getting the raw data",
    "text": "1.1 Getting the raw data\nThe FFIEC Bulk Data Download site provides the Call Report data in two forms. The first is as zipped tab-delimited data files, one for each quarter. The second is as zipped XBRL data files, one for each quarter. At the time of writing, the standard approach to getting the complete data archive amounts to pointing and clicking to download each of the roughly 100 files for each format.4\nThe FFIEC data sets are not as amenable to automated downloading as those offered by other government agencies such as the SEC (see my earlier note on XBRL data), the PCAOB (see my note on Form AP data), or even the Federal Reserve itself (I used data from the MDRM site in preparing this note). However, a group of individuals has contributed the Python package ffiec_data_collector that we can use to collect the data.5\nTo install this Python package, you first need to install Python and then install the ffiec_data_collector using a command like pip install ffiec_data_collector.\nAs discussed in Appendix E of Gow and Ding (2024), I organize my raw and processed data in a repository comprising a single parent directory and several sub-directories corresponding to various data sources and projects. For some data sets, this approach to organization facilitates switching code from using (say) data sources provided by Wharton Research Data Services (WRDS) to using local data in my data repository. I will adopt that approach for the purposes of this note.\nAs the location of my “raw data” repository is found in the the environment variable RAW_DATA_DIR, I can identify that location in Python easily. The following code specifies the download directory as the directory ffiec within my raw data repository.6\n\nimport os\nfrom pathlib import Path\nprint(os.environ['RAW_DATA_DIR'])\n\n/Users/igow/Dropbox/raw_data\n\ndownload_dir = Path(os.environ['RAW_DATA_DIR']) / \"ffiec\"\n\nHaving specified a location to put the downloaded files, it is a simple matter to adapt a script provided on the package website to download the raw data files.\n\nimport ffiec_data_collector as fdc\nimport time\n\ndownloader = fdc.FFIECDownloader(download_dir=download_dir)\n\nperiods = downloader.select_product(fdc.Product.CALL_SINGLE)\n\nresults = []\nfor period in periods[:4]:\n    \n    print(f\"Downloading {period.yyyymmdd}...\", end=\" \")\n    result = downloader.download(\n        product=fdc.Product.CALL_SINGLE,\n        period=period.yyyymmdd,\n        format=fdc.FileFormat.TSV\n    )\n    results.append(result)\n    \n    if result.success:\n        print(f\"✓ ({result.size_bytes:,} bytes)\")\n    else:\n        print(f\"✗ Failed: {result.error_message}\")\n    \n    # IMPORTANT: Be respectful to government servers\n    # Add delay between requests to avoid overloading the server\n    time.sleep(1)  # 1 second delay - adjust as needed\n\nDownloading 20251231... ✓ (6,402,952 bytes)\nDownloading 20250930... ✓ (5,687,172 bytes)\nDownloading 20250630... ✓ (6,231,175 bytes)\nDownloading 20250331... ✓ (5,704,772 bytes)\n\n# Summary\nsuccessful = sum(1 for r in results if r.success)\nprint(f\"\\nCompleted: {successful}/{len(results)} downloads\")\n\n\nCompleted: 4/4 downloads\n\n\nNote that the code above downloads just the most recent four files available on the site. Remove [:4] from the line for period in periods[:4]: to download all files. Note that the package website recommends using time.sleep(5) in place of time.sleep(1) to create a five-second delay and this may be a more appropriate choice if you are downloading all 99 files using this code. Note that the downloaded files occupy about 800 MB of disk space, so make sure you have that available if running this code.\n\n1.1.1 XBRL files\nWhile this note does not use the XBRL files, you can download them using ffiec_data_collector by simply replacing TSV with XBRL in the code above. These zip files are larger than the TSV zip files, occupying about 6 GB of disk space. The ffiec.pq package does offer some rudimentary ability to process these files, but working with them is slow. To illustrate I process just one XBRL zip file.\n\nzipfiles &lt;- ffiec_list_zips(type = \"xbrl\")\nffiec_process_xbrls(zipfiles$zipfile[1]) |&gt; system_time()\n\n   user  system elapsed \n216.845 153.658 412.516 \n\n\n# A tibble: 1 × 4\n  zipfile                               date_raw date       parquet             \n  &lt;chr&gt;                                 &lt;chr&gt;    &lt;date&gt;     &lt;chr&gt;               \n1 FFIEC CDR Call Bulk XBRL 03312001.zip 20010331 2001-03-31 xbrl_20010331.parqu…"
  },
  {
    "objectID": "curate_call_reports.html#processing-the-data",
    "href": "curate_call_reports.html#processing-the-data",
    "title": "Data curation: The case of Call Reports",
    "section": "1.2 Processing the data",
    "text": "1.2 Processing the data\nWith the raw data files in hand, the next task is to process these into files useful for analysis. For reasons I will discuss below, I will process the data into Parquet files. The Parquet format is described in R for Data Science (Wickham et al., 2023, p. 393) as “an open standards-based format widely used by big data systems.” Parquet files provide a format optimized for data analysis, with a rich type system. More details on the Parquet format can be found in Chapter 22 of Wickham et al. (2023) and every code example in Gow and Ding (2024) can be executed against Parquet data files created using my db2pq Python package as described in Appendix E of that book.\nThe easiest way to run the code I used to process the data is to install the ffiec.pq R package I have made available on GitHub. And the easiest way to use the package is to set the locations for the downloaded raw data files from above and for the processed data using the environment variables RAW_DATA_DIR and DATA_DIR, respectively. By default, the ffiec.pq package assumes that the raw data files can be found in a directory ffiec that is a subdirectory of RAW_DATA_DIR. Also, the ffiec.pq package will place the processed data it creates in a directory ffiec that is a subdirectory of DATA_DIR.\nI already have these environment variables set:\n\nSys.getenv(\"RAW_DATA_DIR\")\n\n[1] \"/Users/igow/Dropbox/raw_data\"\n\nSys.getenv(\"DATA_DIR\")\n\n[1] \"/Users/igow/Dropbox/pq_data\"\n\n\nBut, even if I did not, I could set them within R using commands like the following. You should set RAW_DATA_DIR to match what you used above in Python and you should set DATA_DIR to point to the location where you want to put the processed files. The processed files will occupy about 3 GB of disk space, so make sure you have room for these there.\n\nSys.setenv(RAW_DATA_DIR=\"/Users/igow/Dropbox/raw_data\")\nSys.setenv(DATA_DIR=\"/Users/igow/Dropbox/pq_data\")\n\nHaving set these environment variables, I can load my package and run a single command ffiec_process() without any arguments to process all the raw data files.7 This takes about five minutes to run (for me):\n\nresults &lt;- ffiec_process(use_multicore = TRUE) |&gt; system_time()\n\n   user  system elapsed \n 25.389  27.354 763.257 \n\n\nNote that, behind the scenes, the ffiec_process() extracts the data in two phases. In the first phase, it processes the data for each schedule for each quarter into Parquet file. This results in 3713 Parquet files. In the second phase, ffiec_process() proceeds to organize the data in the 3713 Parquet files by variable type to facilitate working with the data. Once the data have been organized, the 3713 schedule-and-quarter-specific Parquet files are discarded.\nThe results table returned by the ffiec_process() function above reflects the outcome of the first phase, as that is when any problems arising from malformed data are expected to arise. If there were any issues in reading the data for a schedule in a quarter, then the variable ok for the corresponding row of results will be FALSE. We can easily confirm that all rows have ok equal to TRUE:\n\nresults |&gt; count(ok)\n\n# A tibble: 1 × 2\n  ok        n\n  &lt;lgl&gt; &lt;int&gt;\n1 TRUE   3713\n\n\nThe results table also includes the field repairs that we can inspect to determine if any “repairs” were made to the data as it was processed in the first phase. As can be seen, a minority of the 3713 first-phase files needed repairs. I discuss these repairs in more detail in Section 3.1.1.\n\nresults |&gt;\n  unnest(repairs) |&gt;\n  count(repairs)\n\n# A tibble: 2 × 2\n  repairs          n\n  &lt;chr&gt;        &lt;int&gt;\n1 newline-gsub   102\n2 tab-repair       2"
  },
  {
    "objectID": "curate_call_reports.html#using-the-data-with-r",
    "href": "curate_call_reports.html#using-the-data-with-r",
    "title": "Data curation: The case of Call Reports",
    "section": "2.1 Using the data with R",
    "text": "2.1 Using the data with R\nSo what has the ffiec.pq package just done? In a nutshell, it have processed each of the nearly 100 zip files into seven Parquet files, and I discuss these in turn.\n\n2.1.1 “Panel of Reporters” (POR) data\nThe first file is the “Panel of Reporters” (POR) table, which provides details on the financial institutions filing in the respective quarter.\nTo access the data using the ffiec.pq functions, we just need to create a connection to an in-memory DuckDB database, which is a simple one-liner.\n\ndb &lt;- dbConnect(duckdb::duckdb())\n\nFrom there we have the option to load a single Parquet file using the pq_file argument of the ffiec_scan_pqs() function:8\n\npor_20250930 &lt;- ffiec_scan_pqs(db, pq_file=\"por_20250930.parquet\")\npor_20250930 |&gt;\n  select(IDRSSD, financial_institution_name, everything()) |&gt;\n  head() |&gt;\n  collect()\n\n# A tibble: 6 × 13\n  IDRSSD financial_institution_name    fdic_certificate_num…¹ occ_charter_number\n   &lt;int&gt; &lt;chr&gt;                         &lt;chr&gt;                  &lt;chr&gt;             \n1     37 BANK OF HANCOCK COUNTY        10057                  &lt;NA&gt;              \n2    242 FIRST COMMUNITY BANK XENIA-F… 3850                   &lt;NA&gt;              \n3    279 BROADSTREET BANK, SSB         28868                  &lt;NA&gt;              \n4    354 BISON STATE BANK              14083                  &lt;NA&gt;              \n5    457 LOWRY STATE BANK              10202                  &lt;NA&gt;              \n6    505 BALLSTON SPA NATIONAL BANK    6959                   1253              \n# ℹ abbreviated name: ¹​fdic_certificate_number\n# ℹ 9 more variables: ots_docket_number &lt;chr&gt;,\n#   primary_aba_routing_number &lt;chr&gt;, financial_institution_address &lt;chr&gt;,\n#   financial_institution_city &lt;chr&gt;, financial_institution_state &lt;chr&gt;,\n#   financial_institution_zip_code &lt;chr&gt;,\n#   financial_institution_filing_type &lt;chr&gt;,\n#   last_date_time_submission_updated_on &lt;dttm&gt;, date &lt;date&gt;\n\npor_20250930 |&gt; count() |&gt; collect()\n\n# A tibble: 1 × 1\n      n\n  &lt;dbl&gt;\n1  4435\n\n\nBut it will generally be more convenient to just read all files in one step, which we can do like this.\n\npor &lt;- ffiec_scan_pqs(db, \"por\")\npor |&gt;\n  select(IDRSSD, financial_institution_name, everything()) |&gt;\n  head() |&gt;\n  collect() \n\n# A tibble: 6 × 13\n  IDRSSD financial_institution_name    fdic_certificate_num…¹ occ_charter_number\n   &lt;int&gt; &lt;chr&gt;                         &lt;chr&gt;                  &lt;chr&gt;             \n1     37 BANK OF HANCOCK COUNTY        10057                  &lt;NA&gt;              \n2    242 FIRST NATIONAL BANK OF XENIA… 3850                   12096             \n3    279 MINEOLA COMMUNITY BANK, SSB   28868                  &lt;NA&gt;              \n4    354 BISON STATE BANK              14083                  &lt;NA&gt;              \n5    439 PEOPLES BANK                  16498                  &lt;NA&gt;              \n6    457 LOWRY STATE BANK              10202                  &lt;NA&gt;              \n# ℹ abbreviated name: ¹​fdic_certificate_number\n# ℹ 9 more variables: ots_docket_number &lt;chr&gt;,\n#   primary_aba_routing_number &lt;chr&gt;, financial_institution_address &lt;chr&gt;,\n#   financial_institution_city &lt;chr&gt;, financial_institution_state &lt;chr&gt;,\n#   financial_institution_zip_code &lt;chr&gt;,\n#   financial_institution_filing_type &lt;chr&gt;,\n#   last_date_time_submission_updated_on &lt;dttm&gt;, date &lt;date&gt;\n\npor |&gt; count() |&gt; collect()\n\n# A tibble: 1 × 1\n       n\n   &lt;dbl&gt;\n1 662363\n\n\n\n\n2.1.2 Item-schedules data\nThe second data set is ffiec_schedules. The zip files provided by the FFIEC Bulk Data site comprise several TSV files organized into “schedules” corresponding the particular forms on which the data are submitted by filers. While the ffiec.pq package reorganizes the data by data type, information about the original source files for the data are retained in ffiec_schedules. We can load this using the following code:\n\nffiec_schedules &lt;- ffiec_scan_pqs(db, \"ffiec_schedules\")\n\nAnd here are the first 10 rows of this data set.\n\nffiec_schedules |&gt; head(10) |&gt; collect()\n\n# A tibble: 10 × 3\n   item     schedule  date      \n   &lt;chr&gt;    &lt;list&gt;    &lt;date&gt;    \n 1 RCFD0010 &lt;chr [2]&gt; 2001-03-31\n 2 RCFD0022 &lt;chr [1]&gt; 2001-03-31\n 3 RCFD0071 &lt;chr [1]&gt; 2001-03-31\n 4 RCFD0073 &lt;chr [1]&gt; 2001-03-31\n 5 RCFD0074 &lt;chr [1]&gt; 2001-03-31\n 6 RCFD0081 &lt;chr [1]&gt; 2001-03-31\n 7 RCFD0083 &lt;chr [1]&gt; 2001-03-31\n 8 RCFD0085 &lt;chr [1]&gt; 2001-03-31\n 9 RCFD0090 &lt;chr [1]&gt; 2001-03-31\n10 RCFD0211 &lt;chr [1]&gt; 2001-03-31\n\n\nFocusing on one item, RIAD4230, we can see from the output below that this item was provided on both Schedule RI (ri) and Schedule RI-BII (ribii) from 2001-03-31 until 2018-12-31, but since then has only been provided on Schedule RI-BII.\n\nffiec_schedules |&gt; \n  filter(item == \"RIAD4230\") |&gt;\n  mutate(schedule = unnest(schedule)) |&gt;\n  group_by(item, schedule) |&gt;\n  summarize(min_date = min(date, na.rm = TRUE),\n            max_date = max(date, na.rm = TRUE),\n            .groups = \"drop\") |&gt;\n  collect()\n\n# A tibble: 2 × 4\n  item     schedule min_date   max_date  \n  &lt;chr&gt;    &lt;chr&gt;    &lt;date&gt;     &lt;date&gt;    \n1 RIAD4230 ribii    2001-03-31 2025-12-31\n2 RIAD4230 ri       2001-03-31 2018-12-31\n\n\nThe next question might be: What is RIAD4230? We can get the answer from ffiec_items, a data set included with the ffiec.pq package:\n\nffiec_items |&gt; filter(item == \"RIAD4230\")\n\n# A tibble: 1 × 5\n  item     mnemonic item_code item_name                           data_type\n  &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;                               &lt;chr&gt;    \n1 RIAD4230 RIAD     4230      Provision for loan and lease losses Float64  \n\n\nSchedule RI is the income statement and “Provision for loan and lease losses” is an expense we would expect to see there for a financial institution. Schedule RI-BII is “Charge-offs and Recoveries on Loans and Leases” and provides detail on loan charge-offs and recoveries, broken out by loan category, for the reporting period. As part of processing the data, the ffiec.pq package confirms that the value for any given item for a specific IDRSSD and date is the same across schedules for all items in the data.\nEach of the other five files represents data from the schedules for that quarter for a particular data type, as shown in Table 1:\n\n\n\nTable 1: Table keys, arrow types, and access code\n\n\n\n\n\nKey\nArrow type\nAccess code\n\n\n\n\nfloat\nFloat64\nffiec_scan_pqs(db, \"ffiec_float\")\n\n\nint\nInt32\nffiec_scan_pqs(db, \"ffiec_int\")\n\n\nstr\nUtf8\nffiec_scan_pqs(db, \"ffiec_str\")\n\n\ndate\nDate32\nffiec_scan_pqs(db, \"ffiec_date\")\n\n\nbool\nBoolean\nffiec_scan_pqs(db, \"ffiec_bool\")\n\n\n\n\n\n\nWe can use the data set ffiec_items to find out where a variable is located, based on its Arrow type.\n\nffiec_items\n\n# A tibble: 5,141 × 5\n   item     mnemonic item_code item_name                               data_type\n   &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;                                   &lt;chr&gt;    \n 1 RCFA2170 RCFA     2170      Total assets                            Float64  \n 2 RCFA3128 RCFA     3128      Allocated transfer risk reserves        Float64  \n 3 RCFA3792 RCFA     3792      Total qualifying capital allowable und… Float64  \n 4 RCFA5310 RCFA     5310      General loan and lease valuation allow… Float64  \n 5 RCFA5311 RCFA     5311      Tier 2 (supplementary) capital          Float64  \n 6 RCFA7204 RCFA     7204      Tier 1 leverage capital ratio           Float64  \n 7 RCFA7205 RCFA     7205      Total risk-based capital ratio          Float64  \n 8 RCFA7206 RCFA     7206      Tier 1 risk-based capital ratio         Float64  \n 9 RCFA8274 RCFA     8274      Tier 1 capital allowable under the ris… Float64  \n10 RCFAA223 RCFA     A223      Risk-weighted assets (net of allowance… Float64  \n# ℹ 5,131 more rows\n\n\nAs might be expected, most variables have type Float64 and will be found in the ffiec_float tables.\n\nffiec_items |&gt; count(data_type, sort = TRUE)\n\n# A tibble: 5 × 2\n  data_type     n\n  &lt;chr&gt;     &lt;int&gt;\n1 Float64    4909\n2 Int32       115\n3 String       73\n4 Boolean      42\n5 Date32        2\n\n\n\n\n2.1.3 Example 1: Do bank balance sheets balance?\nIf we were experts in Call Report data, we might know that domestic total assets is reported as item RCFD2170 (on Schedule RC) for banks reporting on a consolidated basis and as item RCON2170 for banks reporting on an unconsolidated basis. We might also know about RCFD2948 and RCFD3210 and so on. But we don’t need to be experts to see what these items relate to:\n\nbs_items &lt;- c(\"RCFD2170\", \"RCON2170\",\n              \"RCFD2948\", \"RCON2948\",\n              \"RCFD3210\", \"RCON3210\",\n              \"RCFD3000\", \"RCON3000\")\n\nffiec_items |&gt; filter(item %in% bs_items)\n\n# A tibble: 8 × 5\n  item     mnemonic item_code item_name                                data_type\n  &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;                                    &lt;chr&gt;    \n1 RCFD2170 RCFD     2170      Total assets                             Float64  \n2 RCFD2948 RCFD     2948      Total liabilities and minority interest  Float64  \n3 RCFD3000 RCFD     3000      Minority interest in consolidated subsi… Float64  \n4 RCFD3210 RCFD     3210      Total equity capital                     Float64  \n5 RCON2170 RCON     2170      Total assets                             Float64  \n6 RCON2948 RCON     2948      Total liabilities and minority interest  Float64  \n7 RCON3000 RCON     3000      Minority interest in consolidated subsi… Float64  \n8 RCON3210 RCON     3210      Total equity capital                     Float64  \n\n\nThe output above suggests we can make a “top level” balance sheet table using these items. The following code uses the DuckDB instance we created above (db) and the code provided in Table 1, to create ffiec_float, a “lazy” data table. Here “lazy” is a good thing, as it means we have access to all the data without having to load anything into RAM. As a result, this operation takes almost no time.\n\nffiec_float &lt;- ffiec_scan_pqs(db, \"ffiec_float\") |&gt; system_time()\n\n   user  system elapsed \n  0.038   0.014   0.015 \n\nffiec_float\n\n# A query:  ?? x 4\n# Database: DuckDB 1.4.4 [root@Darwin 25.4.0:R 4.5.2/:memory:]\n   IDRSSD date       item        value\n    &lt;int&gt; &lt;date&gt;     &lt;chr&gt;       &lt;dbl&gt;\n 1     37 2001-03-31 RCON3562   0     \n 2     37 2001-03-31 RCON7701   0     \n 3     37 2001-03-31 RCON7702   0     \n 4    242 2001-03-31 RCON3562 329     \n 5    242 2001-03-31 RCON7701   0.08  \n 6    242 2001-03-31 RCON7702   0.085 \n 7    279 2001-03-31 RCON3562  27     \n 8    279 2001-03-31 RCON7701   0.0745\n 9    279 2001-03-31 RCON7702   0.08  \n10    354 2001-03-31 RCON3562   4     \n# ℹ more rows\n\n\nAs can be seen, the data in ffiec_float are in a long format, with each item for each bank for each period being a single row.\nI then filter() to get data on just the items in bs_items and then use the the convenience function ffiec_pivot() from the ffiec.pq package to turn the data into a more customary wide form. I then use coalesce() to get the consolidated items (RCFD) where available and the unconsolidated items (RCON) otherwise. Because I compute() this table (i.e., actually calculate the values for each row and column), this step takes a relatively long time, but not too long given that the underlying data files are in the order of tens of gigabytes if loaded in RAM.9\n\nbs_data &lt;-\n  ffiec_float |&gt; \n  ffiec_pivot(items = bs_items) |&gt;\n  mutate(total_assets = coalesce(RCFD2170, RCON2170),\n         total_liabilities = coalesce(RCFD2948, RCON2948),\n         equity = coalesce(RCFD3210, RCON3210),\n         nci = coalesce(RCFD3000, RCON3000)) |&gt;\n  mutate(eq_liab = total_liabilities + equity + nci) |&gt;\n  compute() |&gt;\n  system_time()\n\n   user  system elapsed \n  5.635   0.949   0.886 \n\n\nSo, do balance sheets balance? Well, not always.10\n\nbs_data |&gt;\n  count(bs_balance = eq_liab == total_assets) |&gt;\n  collect()\n\n# A tibble: 2 × 2\n  bs_balance      n\n  &lt;lgl&gt;       &lt;dbl&gt;\n1 TRUE       640570\n2 FALSE       21793\n\n\nWhat’s going on? Well, one possibility is simply rounding error. So in the following code, I set imbalance_flag to TRUE only if the gap is more than 1 (these are in thousands of USD).\n\nbalanced &lt;-\n  bs_data |&gt;\n  mutate(imbalance = total_assets - eq_liab,\n         imbalance_flag = abs(total_assets - eq_liab) &gt; 1) |&gt;\n  select(-starts_with(\"RC\"), -total_liabilities) |&gt;\n  collect()\n\nThis helps a lot. Now it seems that balance sheets usually balance, but not always.\n\nbalanced |&gt;\n  count(imbalance_flag)\n\n# A tibble: 2 × 2\n  imbalance_flag      n\n  &lt;lgl&gt;           &lt;int&gt;\n1 FALSE          662164\n2 TRUE              199\n\n\nThe vast majority of apparent imbalances are small …\n\nbalanced |&gt;\n  filter(imbalance_flag) |&gt;\n  select(IDRSSD, date, total_assets, imbalance) |&gt;\n  arrange(desc(imbalance))\n\n# A tibble: 199 × 4\n    IDRSSD date       total_assets imbalance\n     &lt;int&gt; &lt;date&gt;            &lt;dbl&gt;     &lt;dbl&gt;\n 1 1362246 2002-06-30       278586      5334\n 2  664653 2002-03-31        25490       224\n 3 2821825 2001-03-31        55220        30\n 4  536554 2001-03-31         4624        10\n 5  647414 2002-12-31        83399        10\n 6  178150 2004-03-31       103939        10\n 7 3097243 2005-06-30        40939        10\n 8  545604 2001-06-30       246223        10\n 9  678931 2001-09-30        31237        10\n10  842358 2002-06-30       202388        10\n# ℹ 189 more rows\n\n\n… and they’re all at least twenty years ago.\n\nbalanced |&gt;\n  filter(imbalance_flag) |&gt;\n  select(IDRSSD, date, total_assets, imbalance) |&gt;\n  arrange(desc(date))\n\n# A tibble: 199 × 4\n    IDRSSD date       total_assets imbalance\n     &lt;int&gt; &lt;date&gt;            &lt;dbl&gt;     &lt;dbl&gt;\n 1  773546 2005-06-30        97558         2\n 2  475756 2005-06-30        68158         2\n 3  131940 2005-06-30        27432         8\n 4 3097243 2005-06-30        40939        10\n 5   20147 2005-06-30        48130       -10\n 6   22954 2005-06-30        65027       -10\n 7   33549 2005-06-30        43865         3\n 8  827953 2005-03-31        50080         5\n 9  587574 2005-03-31         4705         2\n10 2646327 2005-03-31        98814        -8\n# ℹ 189 more rows\n\n\n\n\n2.1.4 Example 2: When do banks submit their Call Reports?\nWorking with dates and times (temporal data) can be a lot more complicated than is generally appreciated. Broadly speaking we might think of temporal data as referring to points in time, or instants, or to time spans, which include durations, periods, and intervals.11 Instants might refer to moments in time (e.g., in UTC) or as times of day (in local time).\nSuppose I were interested in understanding the times of day at which financial institutions submit their Call Reports. It seems that financial institutions file Call Reports with their primary federal regulator through the FFIEC’s Central Data Repository. So I am going to use the America/New_York time zone as the relevant local time zone for this analysis, as the FFIEC is based in Washington, DC.\nTo illustrate some subtleties of working with time zones, I will set my computer to a different time zone from that applicable to where I am: Australia/Sydney, as seen in Figure 2.12\n\n\n\n\n\n\n\n\nFigure 2: Setting my computer to a different time zone\n\n\n\n\n\nNow, R sees my time zone as Australia/Sydney:\n\nSys.timezone()\n\n[1] \"America/New_York\"\n\n\nIf we look at the underlying zip file for 2025-09-30, you will see that last_date_time_submission_updated_on for the bank with IDRSSD of 37 is \"2026-01-13T10:13:21\". In creating the ffiec.pq package, I assumed that this is a timestamp in America/New_York time.\nHow does that show up when I look at the processed data in R using DuckDB?\n\ndb &lt;- dbConnect(duckdb::duckdb())\n\npor &lt;- ffiec_scan_pqs(db, \"por\")\n\npor_default &lt;-\n  por |&gt; \n  filter(IDRSSD == 37, date == \"2025-09-30\") |&gt; \n  rename(dttm = last_date_time_submission_updated_on) |&gt;\n  mutate(dttm_text = as.character(dttm)) |&gt; \n  select(IDRSSD, date, dttm, dttm_text) |&gt; \n  collect()\n\ndbDisconnect(db)\n\npor_default\n\n# A tibble: 1 × 4\n  IDRSSD date       dttm                dttm_text             \n   &lt;int&gt; &lt;date&gt;     &lt;dttm&gt;              &lt;chr&gt;                 \n1     37 2025-09-30 2026-01-13 15:13:21 2026-01-13 15:13:21+00\n\npor_default$dttm[1]\n\n[1] \"2026-01-13 15:13:21 UTC\"\n\n\nBy default, R/DuckDB is showing this to me as UTC. This is fine, but I want to analyse this as a local time. Here is how I can achieve this. First, I set the R variable tz to \"America/New_York\".\n\ntz &lt;- \"America/New_York\"\n\nSecond, I connect to DuckDB anew, but I tell it I want it to use \"America/New_York\" as the time zone of output. But it’s important to note that this is just a “presentation layer” and doesn’t change how the database itself “thinks about” timestamps.\n\ndb &lt;- dbConnect(duckdb::duckdb(), timezone_out = tz)\n\nThird, I make DuckDB a “time zone wizard” but installing and loading the icu extension. This extension enables region-dependent collations and time zones. The icu extension is probably not installed and enabled by default because it is large and not all applications need these features. This allows me to set the DuckDB server’s time zone to America/New_York.\n\nrs &lt;- dbExecute(db, \"INSTALL icu\")\nrs &lt;- dbExecute(db, \"LOAD icu\")\nrs &lt;- dbExecute(db, str_glue(\"SET TimeZone TO '{tz}'\"))\n\nThen I run the query from above again.\n\npor &lt;- ffiec_scan_pqs(db, \"por\")\npor_ny &lt;-\n  por |&gt; \n  filter(IDRSSD == 37, date == \"2025-09-30\") |&gt; \n  rename(dttm = last_date_time_submission_updated_on) |&gt;\n  mutate(dttm_text = as.character(dttm)) |&gt; \n  select(IDRSSD, date, dttm, dttm_text) |&gt;\n  collect()\n\npor_ny\n\n# A tibble: 1 × 4\n  IDRSSD date       dttm                dttm_text             \n   &lt;int&gt; &lt;date&gt;     &lt;dttm&gt;              &lt;chr&gt;                 \n1     37 2025-09-30 2026-01-13 10:13:21 2026-01-13 10:13:21-05\n\npor_ny$dttm[1]\n\n[1] \"2026-01-13 10:13:21 EST\"\n\n\nNow, we see that everything is in America/New_York local time, including the way the server sees the data (dttm_text) and how it’s presented to the R user.\nNow that we have things working in local time, I will make a couple of plots of submission times. To show times on a single scale, I use the fudge of making them all times on a given day, which I somewhat arbitrarily choose to be 2025-01-01.13\nIn the first plot—Figure 3—I present submission times divided by whether banks are located in “western states” or not. It does seem that western banks file later.\n\nwestern_states &lt;- c(\"HI\", \"WA\", \"CA\", \"AK\", \"OR\", \"NV\")\n\nplot_data &lt;-\n  por |&gt; \n  rename(last_update = last_date_time_submission_updated_on) |&gt;\n  mutate(\n    q4 = quarter(date) == 4,\n    offset = last_update - sql(\"last_update AT TIME ZONE 'UTC'\"),\n    offset = date_part(\"epoch\", offset) / 3600,\n    tzone = if_else(offset == -4, \"EDT\", \"EST\"),\n    west = financial_institution_state %in% western_states,\n    ref = sql(str_glue(\"TIMESTAMPTZ '2025-01-01 00:00:00 {tz}'\")),\n    sub_date = date_trunc('days', last_update)) |&gt;\n  mutate(time_adj = last_update - sub_date + ref) |&gt;\n  select(IDRSSD, date, last_update, time_adj, west, q4, offset, tzone) |&gt;\n  collect()\n\n\n\n\n\n\n\n\n\nFigure 3: Submission times by year and region\n\n\n\n\n\nIn the second plot—Figure 4—I present submission times divided by whether America/New_York is on Eastern Daylight Time (EDT) or Eastern Standard Time (EST). Looking at the plot, it seems that submission times have a similar distribution across the two time zones, suggesting that banks do not follow UTC, in which case there should be a difference in distributions for EDT and EST.\nOne can definitely see a “lunch hour” and the submissions appear more likely to involve someone clicking a “Submit” button in some software package rather than IT setting up some overnight automated submission.\n\n\n\n\n\n\n\n\nFigure 4: Submission times by year and US/Eastern time zone"
  },
  {
    "objectID": "curate_call_reports.html#using-the-data-with-python",
    "href": "curate_call_reports.html#using-the-data-with-python",
    "title": "Data curation: The case of Call Reports",
    "section": "2.2 Using the data with Python",
    "text": "2.2 Using the data with Python\n\n2.2.1 Example 3: Plotting trends in total assets for the biggest banks\nLest you think that, because ffiec.pq is an R package, the processed data are of no interest to others, I now provide some basic analysis using Python. For this, I am going to use the Polars package rather than pandas.\nWhile pandas is the dominant data frame library in Python, it would struggle to work with Parquet data on the scale of what ffiec.pq has produced, even though it’s a fairly modest amount of data. Loading 50-100 GB of data into RAM is not fun for most people’s computer set-ups. Even if you have RAM in the hundreds of GBs, not loading it will save you time.\nAs we shall see, Polars does fine with the data and, if anything, is noticeably faster than DuckDB (using dplyr) for the queries I use in this note.\n\nfrom pathlib import Path\nimport polars as pl\nimport os\n\nBecause there is no ffiec.pq package for Python, I mimic the ffiec_scan_pqs() function using the following code.\n\ndef ffiec_scan_pqs(schedule=None, *, \n                   schema=\"ffiec\", data_dir=None):\n    if data_dir is None:\n        data_dir = Path(os.environ[\"DATA_DIR\"]).expanduser()\n\n    path = data_dir / schema if schema else data_dir\n\n    if schedule is None:\n        raise ValueError(\"You must supply `schedule`.\")\n    files = list(path.glob(f\"{schedule}_*.parquet\"))\n    if not files:\n        raise FileNotFoundError(\n          f\"No Parquet files found for schedule '{schedule}' in {path}\"\n        )\n    \n    return pl.concat([pl.scan_parquet(f) for f in files])\n\nNow I can “load” the data much as I did with R.\n\nffiec_float = ffiec_scan_pqs(\"ffiec_float\")\npor = ffiec_scan_pqs(schedule=\"por\")\n\nWhile I am going to focus on total assets in this analysis, I show the parallels between the R code and the Polars code by collecting data on the same items. I don’t need ffiec_pivot() with Polars because the built-in .pivot() method does everything I need. Polars is even faster than R/DuckDB.\n\nimport time\n\nbs_items = [\"RCFD2170\", \"RCON2170\",\n            \"RCFD2948\", \"RCON2948\",\n            \"RCFD3210\", \"RCON3210\",\n            \"RCFD3000\", \"RCON3000\"]\nstart = time.perf_counter()\nbs_data = (\n    ffiec_float\n    .filter(pl.col(\"item\").is_in(bs_items))\n    .pivot(\n        on = \"item\",\n        on_columns = bs_items,\n        index = [\"IDRSSD\", \"date\"],\n        values = \"value\")\n    .with_columns(\n        total_assets = pl.coalesce(pl.col(\"RCFD2170\"), pl.col(\"RCON2170\")),\n        total_liabs = pl.coalesce(pl.col(\"RCFD2948\"), pl.col(\"RCON2948\")),\n        equity = pl.coalesce(pl.col(\"RCFD3210\"), pl.col(\"RCON3210\")),\n        nci = pl.coalesce(pl.col(\"RCFD3000\"), pl.col(\"RCON3000\")),\n    )\n    .with_columns(\n        eq_liab = pl.col(\"total_liabs\") + pl.col(\"equity\") + pl.col(\"nci\")\n    )\n    .collect()\n)\nend = time.perf_counter()\nelapsed = end - start\nprint(f'Time taken: {elapsed:.6f} seconds')\n\nTime taken: 0.279855 seconds\n\n\nWe can peek at the data. So Polars has processed GBs of data and created a table with over 600,000 rows in well under a second. Don’t try this at home … if you’re using pandas.\nNote that ffiec_float is a pl.LazyFrame, but .collect() creates a non-lazy pl.DataFrame.\n\nimport polars.selectors as cs\n\nbs_data.select(cs.exclude(\"^(RCON|RCFD).*$\"))\n\n\nshape: (662_363, 7)\n\n\n\nIDRSSD\ndate\ntotal_assets\ntotal_liabs\nequity\nnci\neq_liab\n\n\ni32\ndate\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n869346\n2019-09-30\n207279.0\n187589.0\n19690.0\n0.0\n207279.0\n\n\n2807173\n2004-12-31\n16058.0\n13844.0\n2214.0\n0.0\n16058.0\n\n\n950758\n2006-03-31\n87901.0\n80397.0\n7504.0\n0.0\n87901.0\n\n\n819855\n2023-09-30\n1.013687e6\n937222.0\n76465.0\n0.0\n1.013687e6\n\n\n680813\n2013-03-31\n1.130088e6\n1.026017e6\n104071.0\n0.0\n1.130088e6\n\n\n…\n…\n…\n…\n…\n…\n…\n\n\n439730\n2006-09-30\n50448.0\n42163.0\n8285.0\n0.0\n50448.0\n\n\n2934930\n2011-03-31\n169623.0\n154310.0\n15313.0\n0.0\n169623.0\n\n\n811279\n2018-03-31\n976781.0\n847651.0\n129130.0\n0.0\n976781.0\n\n\n2705112\n2012-06-30\n126200.0\n113478.0\n12722.0\n0.0\n126200.0\n\n\n662453\n2004-03-31\n306702.0\n286479.0\n20223.0\n0.0\n306702.0\n\n\n\n\n\n\nI identify the top 5 banks by assets on 2025-09-30. Because I will want to merge this with information on por, which is a lazy data frame (pl.LazyFrame), I make it top_5 “lazy” by appending .lazy() at the end.\n\ntop_5 = (\n    bs_data\n    .filter(pl.col(\"date\") == pl.date(2025, 9, 30))\n    .sort(\"total_assets\", descending=True)\n    .with_row_index(\"ta_rank\", offset=1)\n    .filter(pl.col(\"ta_rank\") &lt;= 5)\n    .lazy()\n)\n\nI then grab the names of the banks from por.\n\ntop_5_names = (\n    top_5\n    .join(\n        por.select([\"IDRSSD\", \"date\", \"financial_institution_name\"]),\n        on=[\"IDRSSD\", \"date\"],\n        how=\"inner\",\n    )\n    .sort(\"ta_rank\")\n    .select([\"IDRSSD\", \"financial_institution_name\", \"ta_rank\"])\n    .rename({\"financial_institution_name\": \"bank\"})\n    .collect()\n)\n\n\ntop_5_names\n\n\nshape: (5, 3)\n\n\n\nIDRSSD\nbank\nta_rank\n\n\ni32\nstr\nu32\n\n\n\n\n852218\n\"JPMORGAN CHASE BANK, NATIONAL …\n1\n\n\n480228\n\"BANK OF AMERICA, NATIONAL ASSO…\n2\n\n\n476810\n\"CITIBANK, N.A.\"\n3\n\n\n451965\n\"WELLS FARGO BANK, NATIONAL ASS…\n4\n\n\n504713\n\"U.S. BANK NATIONAL ASSOCIATION\"\n5\n\n\n\n\n\n\nI can combine the names with bs_data using .join().\n\nbs_panel_data = bs_data.join(top_5_names, on=\"IDRSSD\", how=\"inner\")\n\nUsers of pandas who are unfamiliar might be impressed by the performance of Polars, but wonder how they can fit it into their workflows. Of course, it is easy enough to call .to_pandas() and create a pandas pd.DataFrame:\n\npdf = (\n    bs_panel_data\n    .filter(pl.col(\"date\") &gt;= pl.date(2020, 1, 1))\n    .to_pandas()\n)\n\nThis means a pandas user can use all the familiar tools used for plotting or statistical analysis. Because the names are a little long for plotting purposes, I use a little dictionary to replace them.\n\nbank_names = {\n    476810: \"Citibank\",\n    504713: \"US Bank\",\n    852218: \"JPMorgan Chase\",\n    451965: \"Wells Fargo\",\n    480228: \"Bank of America\",\n}\n\npdf[\"bank\"] = pdf[\"IDRSSD\"].map(bank_names)\n\nAnd then I use Seaborn and Matplotlib to make a small (but uninspiring) plot.\n\n\n\n\n\n\n\n\nFigure 5: Total assets for top 5 banks"
  },
  {
    "objectID": "curate_call_reports.html#reading-the-data",
    "href": "curate_call_reports.html#reading-the-data",
    "title": "Data curation: The case of Call Reports",
    "section": "3.1 Reading the data",
    "text": "3.1 Reading the data\nEach quarter’s zip file (zipfile) actually contains dozens of text files (.txt) in TSV (“tab-separated values”) form. The TSV is a close relative of the CSV (“comma-separated values”) and the principles applicable to one form apply to the other.\nI have seen code that just imports from these individual files (what I call inner_file) in some location on the user’s hard drive. This approach is predicated on the user having downloaded the zip files and unzipped them. While we have downloaded all the zip files—assuming you followed the steps outlined in Section 1.1—I don’t want to be polluting my hard drive (or yours) with thousands of .txt files that won’t be used after reading them once.\nInstead, R allows me to simply say:\n\ncon &lt;- unz(zipfile, inner_file)\n\nThe resulting con object is a temporary read-only connection to a single text file (inner_file) stored inside the zip file zipfile. The object allows R to stream the file’s contents directly from the zip archive, line by line, without extracting it. Given con, the core function used to read the data into R has the following basic form, where read_tsv() comes from the readr package, part of the Tidyverse:\n\ndf &lt;- read_tsv(\n  con,\n  col_names = cols,\n  col_types = colspec,\n  skip = skip,\n  quote = \"\",\n  na = c(\"\", \"CONF\"),\n  progress = FALSE,\n  show_col_types = FALSE\n)\n\n\n3.1.1 Handling embedded newlines and tabs\nExperienced users of the readr package might wince a little at the quote = \"\" argument above. What this means is that the data are not quoted. Wickham et al. (2023, p. 101) points out that “sometimes strings in a CSV file contain commas. To prevent them from causing problems, they need to be surrounded by a quoting character, like \" or '. By default, read_csv() assumes that the quoting character will be \".”\nAdapting this to our context and expanding it slightly, I would say: “sometimes strings in a TSV file contain tabs (\\t) and newline characters (\\n).14 To prevent them from causing problems, they need to be surrounded by a quoting character, like \" or '.” While this is a true statement, the TSV files provided on the FFIEC Bulk Data website are not quoted, which means that tabs and newlines characters embedded in strings will cause problems.\nThe approach taken by the ffiec.pq package is to attempt to read the data using a call like that above, which I term the “fast path” (in part because it is indeed fast). Before making that call, the code has already inspected the first row of the file to determine the column names (stored in cols) and used those column names to look up the appropriate type for each column (stored in colspecs). Any anomaly caused by embedded newlines or embedded tabs will almost certainly cause this first read_tsv() call to fail. But if there are no issues, then we pretty much have the data as we want them and can return df to the calling function.15 Fortunately, over 95% of files can be read successfully on the “fast path”.\nIt turns out that if the “fast path” read fails, the most likely culprit is embedded newlines. Let’s say the table we’re trying to read has seven columns and the text field that is the fourth field in the file contains, in some row of the data, an embedded newline, because the data submitted by the reporting financial institution contained \\n in that field. Because read_tsv() processes the data line by line and lines are “delimited” by newline characters (\\n), it will see the problematic line as terminating part way through the fourth column and, because cols tells read_tsv() to expect seven columns, read_tsv() will issue a warning.\nWhen a warning occurs on the “fast path”, the read function in ffiec.pq moves to what I call (unimaginatively) the “slow path”. A “feature” (it turns out) of the TSV files provided on the FFIEC Bulk Data website is that each line ends with not just \\n, but \\t\\n. This means we can assume that any \\n not preceded by \\t is an embedded newline, not a line-terminating endline.16 So I can read the data into the variable txt using readLines() and use a regular expression to replace embedded newlines with an alternative character. The alternative I use is a space and I use the gsub() function to achieve this: gsub(\"(?&lt;!\\\\t)\\\\n\", \" \", txt, perl = TRUE) The regular expression here is (?&lt;!\\\\t)\\\\n is equivalent to (?&lt;!\\t)\\n in Perl or r\"(?&lt;!\\t)\\n\" in Python.17 In words, the regular expression literally means “any newline character that is not immediately preceded by a tab” and the gsub() function will replace such characters with spaces (\" \") because that is the second argument to gsub().\nThis fix addresses almost all the problematic files. “Almost all” means “all but two”. The issue with the remaining two files is (as you might have guessed) embedded tabs. Unfortunately, there’s no easy “identify the embedded tabs and replace them” fix, because there’s no easy way to distinguish embedded tabs from delimiting tabs. However, we can detect the presence of embedded tabs in an affected from the existence of too many tabs in that row.\nFor one of the two “bad” files, there is only one text field, so once we detect the presence of the embedded tab, we can assume that the extra tab belongs in that field: Problem solved. For the other “bad” file, there are several text fields and, while there is just one bad row, we cannot be sure which text field has the embedded tab. The ffiec.pq package just assumes that the embedded tab belongs in the last field and moves on. This means that the textual data for one row (i.e., one financial institution) for one schedule for one quarter cannot be guaranteed to be completely correct.18 Such is life.\nEven without addressing the issue, given that only two files are affected, it’s possible to measure the “damage” created by embedded tabs.\nLet’s look at the earlier file, which turns out to affect the Schedule RIE data for The Traders National Bank (IDRSSD of 490937) for June 2004.19 Traders National Bank in Tennessee was “Tullahoma’s second oldest bank”, until changing its name to Wellworth Bank (seemingly after some mergers), and it listed total assets of $117,335,000 in the affected Call Report.\nLet’s look at the textual data we have in our Parquet files for this case:20\n\nffiec_str &lt;- ffiec_scan_pqs(db, schedule = \"ffiec_str\") \n\nffiec_str |&gt; \n  filter(IDRSSD == \"490937\", date == \"2004-06-30\") |&gt; \n  select(IDRSSD, item, value) |&gt; \n  filter(!is.na(value)) |&gt;\n  collect() |&gt;\n  system_time()\n\n   user  system elapsed \n  0.021   0.036   0.021 \n\n\n# A tibble: 6 × 3\n  IDRSSD item     value                                                   \n   &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;                                                   \n1 490937 TEXT4464 ATM Fees, Exam Fees, Dues & Chairitable Cont            \n2 490937 TEXT4467 Telephone, BanClub, Federal Res Fees, NSF and Other Loss\n3 490937 TEXT4468 Courier, Audit Tax, Deff Comp, Other                    \n4 490937 TEXT4469 ns Exp, Bus Dev, P                                      \n5 490937 TEXT3549 IENC SECURITIES                                         \n6 490937 TEXT3550 IENC CD'S                                               \n\n\nWe can compare this with what we see in the Call Report in Figure 6, where we see that the value of TEXT4468 should be something like \"Courier, Audit Tax, Deff Comp, Other\\tns Exp, Bus Dev, P\". The embedded tab has split this into \"Courier, Audit Tax, Deff Comp, Other\" for TEXT4468 and \"ns Exp, Bus Dev, P\" for TEXT4469, which should be NA. If the values for TEXT4468and TEXT4469 for Traders National Bank in June 2004 are important to your analysis, you could fix this “by hand” easily enough.\n\n\n\n\n\n\n\n\nFigure 6: Extract from June 2004 Call Report for The Traders National Bank\n\n\n\n\n\nLooking at the later file, there were embedded tabs in two rows of Schedule NARR for December 2022. I compared the values in the Parquet file with those in the Call Reports for the two affected banks and the values in the Parquet file match perfectly.21 Because Schedule NARR (“Optional Narrative Statement Concerning the Amounts Reported in the Consolidated Reports of Condition and Income”) has just one text column (TEXT6980), the fix employed by the ffiec.pq package will work without issues.22\n\n\n3.1.2 Handling missing-value sentinels\nUsers familiar with both readr and Call Report data might also have noticed the use of na = c(\"\", \"CONF\") in the call to read_tsv() above. The default value for this function is na = c(\"\", \"NA\") means that empty values and the characters NA are treated as missing values. As I saw no evidence that the export process for FFIEC Bulk Data files used \"NA\" to mark NA values, I elected not to treat \"NA\" as NA. However, a wrinkle is that the reporting firms some times populate text fields—but not numerical fields—with the value \"NA\".23 While the most sensible interpretation of such values is as NA, without further investigation it is difficult to be sure that \"NA\" is the canonical form in which firms reported NA values rather than \"N/A\" or \"Not applicable\" or some other variant.\nThis approach seems validated by the fact that I see the value \"NR\" in text fields of PDF versions of Call Reports and these values show up as empty values in the TSV files, suggesting that \"NR\", not \"NA\" is the FFIEC’s canonical way of representing NA values in these files, while \"NA\" is literally the text value \"NA\", albeit perhaps one intended by the reporting firm to convey the idea of NA. Users of the FFIEC data created by the ffiec.pq package who wish to use textual data should be alert to the possibility that values in those fields may be intended by the reporting firm to convey the idea of NA, even if they are not treated as such by the FFIEC’s process for creating the TSV files.\nThe other value in the na argument used above is \"CONF\", which denotes that the the reported value is confidential and therefore not publicly disclosed. Ideally, we might distinguish between NA, meaning “not reported by the firm to the FFIEC” or “not applicable to this firm” or things like that, from \"CONF\", meaning the FFIEC has the value, but we do not. Unfortunately, the value \"CONF\" often appears in numeric fields and there is no simple way to ask read_tsv() to record the idea that “this value is confidential”, so I just read these in as NA.24\nI say “no simple way” because there are probably workarounds that allow \"CONF\" to be distinguished from true NAs. For example, I could have chosen to have read_tsv() read all numeric fields as character fields and then convert the value CONF in such fields to a sentinel value such as Inf (R’s way of saying “infinity” or \\(\\infty\\)).25 This would not be terribly difficult, but would have the unfortunate effect of surprising users of the data who (understandably) didn’t read the manual and starting finding that the mean values of some fields are Inf. Perhaps the best way to address this would allow the user of ffiec.pq to choose that behaviour as an option, but I did not implement this feature at this time.\nIn addition to these missing values, I discovered in working with the data that the FFIEC often used specific values as sentinel values for NA. For example, \"0\" is used for some fields, while \"00000000\" is used to mark dates as missing, and \"12/31/9999 12:00:00 AM\" is used for timestamps.26 I recoded such sentinel values as NA in each case."
  },
  {
    "objectID": "curate_call_reports.html#storage-format",
    "href": "curate_call_reports.html#storage-format",
    "title": "Data curation: The case of Call Reports",
    "section": "4.1 Storage format",
    "text": "4.1 Storage format\nIn principle, the storage format should fairly minor detail determined by the needs of the Understand team. For example, if the Understand team works in Stata or Excel, then perhaps they will want the data in some kind of Stata format or as Excel files. However, I think it can be appropriate to push back on notions that data will be delivered in form that involves downgrading the data or otherwise compromises the process in a way that may ultimately add to the cost and complexity of the task for the Curate team. For example, “please send the final data as an Excel file attachment as a reply email” might be a request to be resisted because the process of converting to Excel can entail the degradation of data (e.g., time stamps or encoding of text).27 Instead it may be better to choose a more robust storage format and supply a script for turning that into a preferred format.\nOne storage format that I have used in the past would deliver data as tables in a (PostgreSQL) database. The Understand team could be given access data from a particular source organized as a schema in a database. Accessing the data in this form is easy for any modern software package. One virtue of this approach is that the data might be curated using, say, Python even though the client will analyse it using, say, Stata.28 I chose to use Parquet files for ffiec.pq, in part because I don’t have a PostgreSQL server to put the data into and share with you. But Parquet files offer high performance, are space-efficient, and can be used with any modern data analysis tool."
  },
  {
    "objectID": "curate_call_reports.html#good-database-principles",
    "href": "curate_call_reports.html#good-database-principles",
    "title": "Data curation: The case of Call Reports",
    "section": "4.2 Good database principles",
    "text": "4.2 Good database principles\nWhile I argued that one does not want to get “particularly fussy about database normalization”, if anything I may have pushed this further than some users might like. However, with ffiec_pivot(), it is relatively easy (and not too costly) to get the data into a “wide” form if that is preferred. The legacy version of Call Reports data offered by WRDS went to the other extreme with a “One Big Table” approach, which meant that this data set never moved to PostgreSQL because of limits there.29"
  },
  {
    "objectID": "curate_call_reports.html#primary-keys",
    "href": "curate_call_reports.html#primary-keys",
    "title": "Data curation: The case of Call Reports",
    "section": "4.3 Primary keys",
    "text": "4.3 Primary keys\nIn Gow (2026), I suggested that “the Curate team should communicate the primary key of each table to the Understand team. A primary key of a table will be a set of variables that can be used to uniquely identify a row in that table. In general a primary key will have no missing values. Part of data curation will be confirming that a proposed primary key is in fact a valid primary key.”\n\n\n\n\nTable 2: Primary key checks\n\n\n\n\n\n\nSchedule\nPrimary key\nCheck\n\n\n\n\npor\nIDRSSD, date\nTRUE\n\n\nffiec_float\nIDRSSD, date, item\nTRUE\n\n\nffiec_int\nIDRSSD, date, item\nTRUE\n\n\nffiec_str\nIDRSSD, date, item\nTRUE\n\n\nffiec_bool\nIDRSSD, date, item\nTRUE\n\n\nffiec_date\nIDRSSD, date, item\nTRUE\n\n\nffiec_schedules\nitem, date\nTRUE\n\n\n\n\n\n\n\n\nValid primary keys for each schedule are shown in Table 2. To checking these, I used the function ffiec_check_pq_keys(), which checks the validity of a proposed primary key for a schedule. That every column except value forms part of the primary key is what allows us to use ffiec_pivot() to create unique values in the resulting “wide” tables."
  },
  {
    "objectID": "curate_call_reports.html#data-types",
    "href": "curate_call_reports.html#data-types",
    "title": "Data curation: The case of Call Reports",
    "section": "4.4 Data types",
    "text": "4.4 Data types\nIn Gow (2026), I proposed that “each variable of each table should be of the correct type. For example, dates should be of type DATE, variables that only take integer values should be of INTEGER type. Date-times should generally be given with TIMESTAMP WITH TIME ZONE type. Logical columns should be supplied with type BOOLEAN.”30\nThis element is (to the best of my knowledge) satisfied with one exception. The Parquet format is a bit like the Model T Ford: it supports time zones, and you can use any time zone you want, so long as it is UTC.31 As discussed above, there is only one timestamp in the whole set-up, last_date_time_submission_updated_on on the POR files and I discussed this field above."
  },
  {
    "objectID": "curate_call_reports.html#no-manual-steps",
    "href": "curate_call_reports.html#no-manual-steps",
    "title": "Data curation: The case of Call Reports",
    "section": "4.5 No manual steps",
    "text": "4.5 No manual steps\nWhen data vendors are providing well-curated data sets, much about the curation process will be obscure to the user. This makes some sense, as the data curation process has elements of trade secrets. But often data will be supplied by vendors in an imperfect state and significant data curation will be performed by the Curate team working for or within the same organization as the Understand team.\nFocusing on the case where the data curation process transforms an existing data set—say, one purchased from an outside vendor—into a curated data set in sense used here, there are a few ground rules regarding manual steps.\n“First, the original data files should not be modified in any way.” Correct. The ffiec.pq package does not modify the FFIEC Bulk Data files after downloading them. I do make some corrections to the item_name variable in the ffiec_items package, but these “manual steps [are] extensively documented and applied in a transparent, automated fashion.” The code for these steps can be found on the GitHub page for the ffiec.pq package."
  },
  {
    "objectID": "curate_call_reports.html#documentation",
    "href": "curate_call_reports.html#documentation",
    "title": "Data curation: The case of Call Reports",
    "section": "4.6 Documentation",
    "text": "4.6 Documentation\n“The process of curating the data should be documented sufficiently well that someone else could perform the curation steps should the need arise.” I regard that having the ffiec.pq package do all the work of processing the data satisfies this requirement.\nA important idea here is that the code for processing the data is documentation in its own right. Beyond that the document you are reading now is a form of documentation, as is the documentation in the ffiec.pq package."
  },
  {
    "objectID": "curate_call_reports.html#update-process",
    "href": "curate_call_reports.html#update-process",
    "title": "Data curation: The case of Call Reports",
    "section": "4.7 Update process",
    "text": "4.7 Update process\nIf a new zip file appears on the FFIEC Bulk Data website, you can download it using the process outlined in Section 1.1. Just changing the [:4] to [0] and the script downloads the latest file.\nThen run the following code and the data will be updated:\n\nresults &lt;-\n  ffiec_list_zips() |&gt;\n  filter(date == max(date)) |&gt;\n  select(zipfile) |&gt;\n  pull() |&gt;\n  ffiec_process() |&gt;\n  system_time()\n\n   user  system elapsed \n 13.206   1.385  12.195 \n\nresults |&gt; count(date, ok)\n\n# A tibble: 1 × 3\n  date       ok        n\n  &lt;date&gt;     &lt;lgl&gt; &lt;int&gt;\n1 2025-12-31 TRUE     39"
  },
  {
    "objectID": "curate_call_reports.html#data-version-control",
    "href": "curate_call_reports.html#data-version-control",
    "title": "Data curation: The case of Call Reports",
    "section": "4.8 Data version control",
    "text": "4.8 Data version control\nWelch (2019) argues that, to ensure that results can be reproduced, “the author should keep a private copy of the full data set with which the results were obtained.” This imposes a significant cost on the Understand team to maintain archives of data sets that may run to several gigabytes or more and it would seem much more efficient for these obligations to reside with the parties with the relevant expertise. Data version control is a knotty problem and one that even some large data providers don’t appear to have solutions for.\nI am delivering the Call Report data not as the data files, but as an R package along with instructions for obtaining the zip files from the FFIEC Bulk Data website. So I cannot be said to be providing much version control of data here. That said, if a user retains the downloaded zip files, the application of the ffiec.pq functions to process these into Parquet files should provide a high degree of reproducibility of the data for an individual researcher.32\nFor my own purposes, I achieve a modest level of data version control by using Dropbox, which offers the ability to restore some previous versions of data files."
  },
  {
    "objectID": "curate_call_reports.html#footnotes",
    "href": "curate_call_reports.html#footnotes",
    "title": "Data curation: The case of Call Reports",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAs will be seen this, this load step will not generally be an elaborate one. The inclusion of a separate load step serves more to better delineate the distinction between the Curate process and the Understand process.↩︎\nExecute install.packages(c(\"tidyverse\", \"farr\", \"DBI\", \"duckdb\", \"pak\", dbplyr\") within R to install all the packages other than ffiec.pq that you will need to run the R code in this note.↩︎\nI already included pak in the command in the footnote to the previous paragraph.↩︎\nAt the time of writing, there are 99 files, but each quarter will bring a new file to be processed.↩︎\nI discovered this package after writing most of this note. In my case, I pointed-and-clicked to get many of the files and Martien Lubberink of Victoria University of Wellington kindly provided the rest.↩︎\nI recommend that readers follow a similar approach if following along with this note, as it makes subsequent steps easier to implement. A reader can simply specify os.environ['RAW_DATA_DIR'] = \"/Users/igow/Dropbox/raw_data\", substituting a location where the data should go on his or her computer.↩︎\nNote I am using the argument use_multicore = TRUE, which gives me about a five-time improvement in performance, but you will see different results depending on your system. You might test the code on a few files before running the whole thing. The ffiec_process() can accept a list of fully qualified paths to zipfiles, which can be created with the help of ffiec_list_zips().↩︎\nNote that nothing is being “loaded” into RAM, the file is merely being scanned by DuckDB.↩︎\nParquet files are compressed, so they use less space on disk than they do when they are loaded into RAM.↩︎\nNote that collect() here actually brings the data into R; compute() merely materializes the data as a table in the DuckDB database.↩︎\nSee Wickham et al. (2023), p. 311-315, for discussion of time spans.↩︎\nI am writing this from the Boston area, so it would be insufficiently confusing if I did not make this change to my settings. In effect, I want to ensure that the code works for someone in a different time zone.↩︎\nI say somewhat arbitrarily because you probably don’t want to choose a day that is missing an hour due to shift from EST to EDT.↩︎\nTabs and newlines are what are sometimes called invisibles because their presence is not apparent from viewing their usual representation as text (for example, a tab might look the same as a series of spaces). The \\t and \\n representations are quite standard ways of making these characters visible to humans.↩︎\nIn practice, there’s a little clean-up to be done before returning df, as I will explain shortly.↩︎\nUnfortunately, the read_tsv() function does not allow us to specify an alternative to the default for line-terminating characters. It seems that other R and Python packages also do not offer this option.↩︎\nThere are extra backslashes in the R version to specify that the backslashes represent backslashes to be passed along to gsub(), not special characters to be interpreted by R itself.↩︎\nThis is not a completely insoluble problem in that we could inspect the XBRL data to determine the correct form of the data in this case. This is “above my pay grade” in this setting. (I’m not being paid to do this!)↩︎\nNote that it would be somewhat faster to use ffiec_text &lt;- load_parquet(db, \"ffiec_str_20040630\", \"ffiec\"), but this code doesn’t take too long to run.↩︎\nOnly the textual data will have embedded tabs and, because such data is arranged after numerical data, only text data will be affected by embedded tabs.↩︎\nSee here for the gory details. Interestingly, the WRDS Call Report data have the same issue with the earlier case and have incorrect data for one of the banks in the latter case. This seems to confirm that WRDS uses the TSV data itself in creating its Call Report data sets.↩︎\nRecall that the “fix” assumes that embedded tab belongs in last available text column.↩︎\nIf \"NA\" appeared in a numeric field, my code would report an error. As I detected no errors in importing the data, I know there are no such values.↩︎\nThis is the kind of situation where SAS’s approach to coding missing values would be helpful.↩︎\nIn a sense, this would be doing the opposite of what the Python package pandas did in treating np.NaN as the way of expressing what later became pd.NA; I’d be using Inf to distinguish different kinds of missing values.↩︎\nNote that this timestamp sentinel appears in the “MDRM” data from the Federal Reserve that I used to construct ffiec_items, not in the FFIEC data sets themselves.↩︎\nI discuss some of the issues with Excel as a storage format below.↩︎\nOne project I worked on involved Python code analysing text and putting results in a PostgreSQL database and a couple of lines of code were sufficient for a co-author in a different city to load these data into Stata.↩︎\nA rule of thumb might be that, if you cannot store your fairly standard data in PostgreSQL, then perhaps you need to revisit the structure of the data.↩︎\nGow (2026) is referring to PostgreSQL types. The ffiec.pq package uses (logical) Parquet types DATE, INT32, TIMESTAMP(isAdjustedToUTC = true), and BOOLEAN types, respectively for these types. Floating-point numbers are stored as FLOAT64 and strings as STRING.↩︎\nHenry Ford famously said of the Model T that “any customer can have a car painted any color that he wants so long as it is black.” Strictly speaking, the Parquet format supports timezone-aware timestamps, but only as UTC instants, as other time zones are not supported.↩︎\nNote that a researcher might need to use a specific version of the ffiec.pq package to achieve full reproducibility, but the pak package allows for that.↩︎"
  }
]