[
  {
    "objectID": "published/yahoo-returns.html",
    "href": "published/yahoo-returns.html",
    "title": "Stock returns on Yahoo Finance",
    "section": "",
    "text": "There appear to be few easy-to-use sources of free stock price data out there, but one venerable source is Yahoo Finance, which was a source of such data before Google was even a twinkle in the eye of Larry Page and Sergey Brin. So you may have used Yahoo Finance yourself to calculate stock returns. Yahoo Finance offers two versions of daily closing prices: close (“close price adjusted for splits”) and adjusted (“adjusted for splits and dividend and/or capital gain distributions”). If I denote close and adjusted on date \\(t\\) as \\(c_t\\) and \\(a_t\\), respectively, you likely calculated returns as\n\\[ r_t = \\frac{a_t}{a_{t-1}} - 1 \\] And denoting dividends (including capital gain distributions) on date \\(t\\) as \\(d_t\\), you likely figured that the above was equivalent to the standard formula:\n\\[ r_t = \\frac{c_t + d_t}{c_{t-1}} - 1 \\] But I have discovered that this is not true. Instead, the adjusted stock price is calculated so that returns are calculated using the following expression:\n\\[ r_t = \\frac{a_t}{a_{t-1}} - 1 =  \\frac{c_t}{c_{t-1} - d_t} - 1 \\] I’m guessing that many finance experts would regard the latter formula as simply wrong. Interestingly, Investopedia suggests that one adjusts stock prices for dividends in precisely the way implied by the Yahoo Finance calculation: that is, you adjust \\(c_{t-1}\\) by subtracting \\(d_t\\) from it.1 I’m inclined to label it as unorthodox rather than simply wrong. While the good news is that the differences are not large, I wonder how many realize that this is how Yahoo Finance is doing things.\nIn effect, the standard calculation assumes that you buy for cash at close one day and sell the next day, getting the proceeds of the sale and the associated dividends at that time. In contrast, the Yahoo Finance calculation assumes that you only need to supply cash to buy the shares at close one day net of dividends and sell the next day. The former seems a bit easier to describe and perhaps to pull off. Try asking your broker if you can do the latter! Of course, there’s a bit of fiction in all these calculations with trades at closing prices and dividends being paid on the ex-dividend date, but they are useful benchmark.\nI think this case illustrates the reality that many data items are not well-documented and, even if they are, it makes sense to check that the data line up with the documentation."
  },
  {
    "objectID": "published/yahoo-returns.html#comparing-with-crsp-returns",
    "href": "published/yahoo-returns.html#comparing-with-crsp-returns",
    "title": "Stock returns on Yahoo Finance",
    "section": "2.1 Comparing with CRSP returns",
    "text": "2.1 Comparing with CRSP returns\nThe Center for Research in Security Prices, LLC (CRSP) is the de facto standard source of stock returns for US stocks in academic finance.4 I have a significant subset of CRSP in a repository of parquet files along the lines described in Appendix E of Empirical Research in Accounting: Tools and Methods.\nHere I collect data from the CRSP daily stock file (crsp.dsf) for Coca-Cola and store it in crsp_rets_ko, a remote data frame.5\n\ndb &lt;- dbConnect(duckdb::duckdb())\n\nstocknames &lt;- load_parquet(db, \"stocknames\", \"crsp\")\ndsf &lt;- load_parquet(db, \"dsf\", \"crsp\")\n\ncrsp_rets_ko &lt;- \n  stocknames |&gt;\n  filter(ticker == \"KO\") |&gt; \n  distinct(permno) |&gt; \n  inner_join(dsf, by = \"permno\") |&gt;\n  select(permno, date, prc, ret)\n\nThe standard measure of returns on CRSP is ret and I compare this with ret_yahoo (calculated using adjusted stock prices) and with ret_std, calculated using the “standard” formula discussed above. As can be seen below, the CRSP calculation (ret) and the standard formula (ret_std) line up pretty much perfectly.\n\ncrsp_rets_ko |&gt;\n  inner_join(ko_rets, by = \"date\", copy = TRUE) |&gt;\n  mutate(ret_std = (close + adj_amt) / lag_close - 1) |&gt;\n  filter(adj_amt != 0) |&gt;\n  select(date, ret_yahoo, ret_std, ret) |&gt;\n  arrange(desc(date)) |&gt;\n  collect()\n\n# A tibble: 32 × 4\n   date        ret_yahoo    ret_std       ret\n   &lt;date&gt;          &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;\n 1 2024-11-29  0.002111   0.002095   0.002095\n 2 2024-09-13  0.009400   0.009336   0.009336\n 3 2024-06-14  0.0007198  0.0007144  0.000714\n 4 2024-03-14 -0.002226  -0.002209  -0.002209\n 5 2023-11-30  0.01160    0.01151    0.01151 \n 6 2023-09-14  0.008279   0.008214   0.008214\n 7 2023-06-15  0.01374    0.01364    0.01364 \n 8 2023-03-16  0.005503   0.005461   0.005461\n 9 2022-11-30  0.02531    0.02513    0.02513 \n10 2022-09-15 -0.01359   -0.01349   -0.01349 \n# ℹ 22 more rows"
  },
  {
    "objectID": "published/yahoo-returns.html#footnotes",
    "href": "published/yahoo-returns.html#footnotes",
    "title": "Stock returns on Yahoo Finance",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis approach to calculating adjusted stock price works only on the day the ex-dividend date when the previous values of close and adjusted are equal.↩︎\nExecute install.packages(c(\"tidyquant\", \"tidyverse\", \"farr\", \"DBI\", \"duckdb\", \"dbplyr\") within R to install all the packages you need to run the code in this note.↩︎\nNote that I use round(., 4) to eliminate quirky issues related to less-significant digits with double-precision numbers.↩︎\nSee Section 7.2 of Empirical Research in Accounting: Tools and Methods for more on CRSP.↩︎\nFor more on remote data frames see Chapter 6 of Empirical Research in Accounting: Tools and Methods.↩︎"
  },
  {
    "objectID": "published/weather.html",
    "href": "published/weather.html",
    "title": "Defining winter and summer in Melbourne",
    "section": "",
    "text": "Figure 1: Average daily temperatures for 91 days following indicated date for period 2001–2023\nIn the United States, one often hears people speak of the “official” start of seasons. Ironically, there seems to be nothing that is official about these dates. However, there is consensus about the dates in the US. The “official” start of summer is the summer solstice (for 2024: 21 December in Melbourne, 20 June in Boston) and the “official” start of winter is the winter solstice (for 2024: 21 June in Melbourne, 21 December in Boston).4\nIn Australia, the usual convention is to divide seasons by months. On this basis, winter starts on 1 June and summer starts on 1 December.5\nIs there a sense in which one approach is more correct than the other? Focusing on summer and winter, one definition for these seasons would be that winter starts on the first day of the 91-day period that is the coldest such period for a year averaged over a number of years. Similarly, summer should starts on the first day of the 91-day period that is the hottest such period for a year averaged over a number of years.\nWe answer this question focusing on Melbourne, Australia (latitude of -37.814, longitude: 144.96332).\nDaily temperature data from Open-Meteo comprise a maximum and minimum temperature. So immediately we have two possible definitions of each season according to the temperature we use (e.g., summer could be the 91-day period that has the highest average minimum temperature or it could be the period that has the highest average maximum temperature). Here we consider both.\nThe start of winter based on the 91-day period with the lowest average maximum temperature is 27 May. The start of winter based on the 91-day period with the lowest average minimum temperature is 09 June.\nThe start of summer based on the 91-day period with the highest average maximum temperature is 19 December. The start of summer based on the 91-day period with the highest average minimum temperature is 24 December. So using maximums, we get close to the Australian convention for winter and close to the US convention for summer.\nInterestingly, it seems that using average maximums for summer and winter gets closest to the current approach in Australia. However, even using these we have the issue that spring begins on 26 August and autumn begins on 20 March. This implies a spring of 115 days and an autumn of 68 days."
  },
  {
    "objectID": "published/weather.html#footnotes",
    "href": "published/weather.html#footnotes",
    "title": "Defining winter and summer in Melbourne",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nUniversity of Melbourne, ian.gow@unimelb.edu.au↩︎\nUniversity of Melbourne, ian.gow@unimelb.edu.au↩︎\nUniversity of Melbourne, ian.gow@unimelb.edu.au↩︎\nSeasons reckoned in this way are known as astronomical seasons. See here.↩︎\nSeasons reckoned in this way are known as meteorological seasons. See here.↩︎"
  },
  {
    "objectID": "published/trading_dates.html",
    "href": "published/trading_dates.html",
    "title": "Trading days per year (crsp.dsf)",
    "section": "",
    "text": "The conventional notion is that there are (on average) about 252 trading days per year on US stock exchanges. We don’t have to accept this, as we can use data on CRSP’s daily stock file (crsp.dsf) to count trading dates per year.\nIn this note, we examine the number of trading days in each year using date from crsp.dsf. To start, we calculate n_days, the number of trading days in that year, for each year.\nFrom Table 1, we see that 1925 is clearly an odd year (and we will exclude it from subsequent analysis). Also, 1968 is an outlier.\nTable 1: Years with the fewest trading dates\n\n\n\n\n\n\nyear\nn_days\n\n\n\n\n1925\n1\n\n\n1968\n226\n\n\n2001\n248\n\n\n1961\n250\n\n\n1969\n250\nFrom Table 2, it can be seen that there is a cluster of years with between 251 and 253 trading days.\nTable 2: Years with days between 249 and 260\n\n\n\n\n\n\nn_days\nn\n\n\n\n\n250\n4\n\n\n251\n10\n\n\n252\n32\n\n\n253\n21\n\n\n254\n3\nBut, looking at Figure 1, we can see an unexpected cluster of years with more than 280 trading days.\nFigure 1: Distribution of number of trading days per year\nIn Figure 2, we see that the years with an unexpectedly high number of trading days are in the earlier part of the sample.\nFigure 2: Trading days per year over time\nFrom Figure 4 we learn that some trading days are actually Saturdays. We can do a version of Figure 2 that includes information about the days of the week. As we can see in Figure 3, the Saturdays are in the earlier part of the sample. It also seems that the “issue” with 1968 is concentrated in Wednesdays.\nFigure 3: Trading days per year over time with days of the week\nAccording to tradinghours.com, “in May 1887, the trading hours were officially set to Monday to Friday 10am to 3pm and Saturday from 10am to noon. … In [September] 1952, the Saturday trading session was finally retired.”\nFigure 4: Trading days: Days of the week\nWe can identify “missing” dates in 1968 by doing an anti_join() of a table of non-weekend dates with the list of trading dates.1 It turns out that a crisis in managing trading volumes known as the “paperwork crisis” forced the NYSE to restrict trading to four days a week. According to Market Memoir, “for months the exchange closed on Wednesdays, and sometimes needed to close early on other days to give firms additional time to combat severe backlogs.” The missing Wednesdays are quite apparent in Figure 5.\nFigure 5: Weekdays of dates ‘missing’ from 1968 data"
  },
  {
    "objectID": "published/trading_dates.html#footnotes",
    "href": "published/trading_dates.html#footnotes",
    "title": "Trading days per year (crsp.dsf)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote that some of these “missing” dates would be public holidays.↩︎"
  },
  {
    "objectID": "published/tax_target.html",
    "href": "published/tax_target.html",
    "title": "Does Beardsley et al. (2021) show anything?",
    "section": "",
    "text": "A recurring problem in accounting research is poorly thought-out research designs where a null hypothesis is assumed to imply zero coefficients in some regression without any effort to check that this is the case. Beardsley et al. (2021) illustrates this problem nicely. Beardsley et al. (2021, p. 1) state that “using year-end effective tax rate (ETR) manipulation as our setting, we find that firms decrease ETRs from 3rd to 4th quarter to meet or beat a greater percentage of individual forecast. … Our study highlights the strategic nature of earnings management by providing evidence that managers consider individual forecasts to calibrate earnings management decisions.”\nThe main result from Beardsley et al. (2021, p. 11) is provided by Table 5, which presents two regressions, one for each of two sub-samples. The key results are a positive coefficient on PREBEAT_AMT for PREBEAT firms and a negative coefficient on PREMISS_AMT for PREMISS firms.\nBut I can simulate precisely these results with no earnings management. I assume nothing more than random variation in ETRs from Q3 to Q4 and that analysts know the Q4 effective tax rate.\nIn writing this note, I use the packages listed below.1 This note was written using Quarto and compiled with RStudio, an integrated development environment (IDE) for working with R. The source code for this note is available here.\nlibrary(tidyverse)\nlibrary(modelsummary)\nIn the code below, I generate data for 10,000 firms and draw ETRs for Q3 and Q4 independently from a uniform distribution over the range 33%–37%.\nn &lt;- 10000\nn_analysts &lt;- 10\nset.seed(20241001)\netr_3 &lt;- runif(n = n, min = 0.33, max = 0.37)\netr_4 &lt;- runif(n = n, min = 0.33, max = 0.37)\nI then draw forecasts of pretax earnings from a normal distribution. Forecasts of after-tax earnings are simply pre-tax earnings times a factor equal to one minus Q4 ETR.\nebt_actual &lt;- rnorm(n = n, mean = 1, sd = 0.1)\nni_actual &lt;- ebt_actual * (1 - etr_4)\nRealized pre-tax earnings are simply the forecast plus noise and realized after-tax earnings are realized pre-tax earnings times a factor equal to one minus Q4 ETR.\ndf_pre &lt;- \n  tibble(firm_id = 1:n, ebt_actual,\n            etr_3, etr_4, ni_actual) |&gt;\n  cross_join(tibble(analyst_id = 1:n_analysts)) |&gt;\n  mutate(ebt_forecast = ebt_actual + rnorm(n = n * n_analysts, sd = 0.2),\n         ni_forecast = ebt_forecast * (1 - etr_4),\n         ni_premanaged = ebt_actual * (1 - etr_3)) |&gt;\n  group_by(firm_id) |&gt;\n  summarize(percent_miss = mean(ni_premanaged &lt; ni_forecast),\n            disp = sd(ni_forecast),\n            ni_forecast = mean(ni_forecast),\n            .groups = \"drop\")\nI combine the relevant data from above into a data frame and calculate variables for the regression following the descriptions in Appendix A of Beardsley et al. (2021, pp. 14–15).\ndf &lt;- \n  df_pre |&gt;\n  mutate(amount = ni_forecast - ebt_actual * (1 - etr_3),\n         premiss_amt = amount,\n         premiss = premiss_amt &gt; 0,\n         prebeat = premiss_amt &lt; 0,\n         prebeat_amt = -amount) |&gt;\n  mutate(premiss_amt = if_else(premiss, premiss_amt, 0),\n         prebeat_amt = if_else(prebeat, prebeat_amt, 0))\nI then run the regressions shown in Table 5 of Beardsley et al. (2021, p. 11). Results are shown in Table 1. There you can see I have reproduced the key results of Beardsley et al. (2021). However, the null result of “no earnings management” is true here. As such, it is not clear that Beardsley et al. (2021) show anything.\nI have written elsewhere that the scourge of near-ubiquitous p-hacking makes accounting research of very dubious merit. If null hypotheses can be rejected simply because it is assumed that they imply zero coefficients, then this only adds to these concerns.2\nBeardsley et al. (2021) was published in the Journal of Accounting and Economics, one of the top journals in accounting. The acknowledgement note thanks an editor, a referee, at least three discussants, and many luminous members of the accounting firmament. This suggests that plausibly mechanical nature of the results in Beardsley et al. (2021) never occurred to any of these people.3 From conceiving this simulation to writing this note took about half an hour; so the bar is not high!\nfms &lt;- list(\n  lm(etr_4 - etr_3 ~ prebeat_amt + percent_miss + disp, data = df, subset = prebeat),\n  lm(etr_4 - etr_3 ~ premiss_amt + percent_miss + disp, data = df, subset = premiss),\n  lm(etr_4 - etr_3 ~ percent_miss, data = df))\nTable 1: Replication of Table 5 of Beardsley et al. (2021)\n\n\n\n\n\n    \n\n    \n\n    \n    \n    \n      \n        \n        \n              \n                 \n                (1)\n                (2)\n                (3)\n              \n        \n        \n        \n                \n                  (Intercept)\n                  0.001\n                  0.007***\n                  0.014***\n                \n                \n                  \n                  0.896\n                  3.929\n                  29.326\n                \n                \n                  prebeat_amt\n                  0.141***\n                  \n                  \n                \n                \n                  \n                  13.358\n                  \n                  \n                \n                \n                  percent_miss\n                  0.003\n                  -0.005**\n                  -0.029***\n                \n                \n                  \n                  1.154\n                  -2.394\n                  -31.353\n                \n                \n                  disp\n                  -0.022***\n                  -0.031***\n                  \n                \n                \n                  \n                  -2.972\n                  -4.398\n                  \n                \n                \n                  premiss_amt\n                  \n                  -0.118***\n                  \n                \n                \n                  \n                  \n                  -11.364\n                  \n                \n                \n                  Num.Obs.\n                  4980\n                  5020\n                  10000\n                \n                \n                  R2\n                  0.054\n                  0.057\n                  0.090"
  },
  {
    "objectID": "published/tax_target.html#footnotes",
    "href": "published/tax_target.html#footnotes",
    "title": "Does Beardsley et al. (2021) show anything?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nExecute install.packages(c(\"tidyverse\", modelsummary\")) within R to install all the packages you need to run the code in this note.↩︎\nOne doesn’t even need to p-hack if one runs regressions where the null hypothesis implies non-zero coefficients and one assumes that they do not.↩︎\nOne has to be careful here, as one should not interpret any kind of endorsement of a paper from mentions in the acknowledgements.↩︎"
  },
  {
    "objectID": "published/spreadsheets.html",
    "href": "published/spreadsheets.html",
    "title": "Data collection (with spreadsheets)",
    "section": "",
    "text": "This note also completes the three-part series of notes written to address gaps in the coverage of Gow and Ding (2024a) relative to Wickham et al. (2023).1 Previous notes covered hierarchical data and dates and times and this note covers spreadsheets. As discussed in a post on LinkedIn, one goal of Empirical Research in Accounting: Tools and Methods (co-authored with Tony Ding) is to provide a pathway to mastery of the contents of R for Data Science.2\nIn writing about data science there is a definite tendency to emphasize the “sexy” aspects, such as machine learning algorithms or fancy statistics. This is the case even though it seems to be widely understood that a lot of the critical work involves unsexy tasks related to the organization of data. One resource that bucks the trend is R for Data Science (Wickham et al., 2023), which eschews the sexy stuff in providing excellent materials for learning how to tidy and transform data to generate insights.\nBut even Wickham et al. (2023) draws the line at data collection.3 Figure 1-1 of Wickham et al. (2023) suggests that the “whole game” of data science kicks off with Import.4 But where do these imported data come from? Who collected them? Often data analysts are blessed with ready-to-import data sources. But sometimes the only way to get data is to Collect them ourselves.5\nAt first impression, it might seem that data collection is too dull a topic for data scientists to think much about and there is not much for writers of educational materials (e.g., textbooks) to say on the topic. But Broman and Woo (2018) seems to suggest otherwise. While its title is “Data organization in spreadsheets”, Broman and Woo (2018) could equally well be entitled “Data collection (with spreadsheets)” (and I have used the latter as the title for this note). Broman and Woo (2018) is a concise, well-written paper that provides a number of best practices for data collection and I highly recommend it.\nHowever, when I first saw Broman and Woo (2018) online in 2016, I had a number of thoughts. I dutifully filed these as (somewhat obnoxious) issues on Karl Broman’s GitHub repository, but had no real expectation that Karl would address these.6 Instead I figured that I would write a short follow-up paper digging more deeply into issues that interested me. This document is that follow-up paper … a mere nine years later! In this note, I provide a different perspective on data collection—albeit one that I hope is complementary to that in Broman and Woo (2018)—with a detailed extended case study to illustrate the ideas I discuss.\nThis note also completes the three-part series of notes written to address gaps in the coverage of Empirical Research in Accounting: Tools and Methods (Gow and Ding, 2024b) relative to R for Data Science.7 Previous notes covered hierarchical data and dates and times and this note covers spreadsheets. As discussed in a post on LinkedIn, one goal of Empirical Research in Accounting: Tools and Methods (co-authored with Tony Ding) is to provide a pathway to mastery of the contents of R for Data Science.8\nThe application I study here uses spreadsheets as part of a process of linking data on IPOs in the United States with auditor IDs, as found in Form AP data.9 I also provide brief sketches of the use of spreadsheets in data collection drawing on research projects I have participated in.\nIn writing this note, I use the packages listed below.10 This note was written using Quarto and compiled with RStudio, an integrated development environment (IDE) for working with R. The source code for this note is available here and the latest version of this PDF is here.\nlibrary(tidyverse)\nlibrary(DBI)\nlibrary(farr)\nlibrary(googlesheets4)\nlibrary(arrow)\nlibrary(dbplyr)"
  },
  {
    "objectID": "published/spreadsheets.html#scenario-1-importing-spreadsheet-data",
    "href": "published/spreadsheets.html#scenario-1-importing-spreadsheet-data",
    "title": "Data collection (with spreadsheets)",
    "section": "1.1 Scenario 1: Importing spreadsheet data",
    "text": "1.1 Scenario 1: Importing spreadsheet data\nChapter 20 of Wickham et al. (2023) covers spreadsheets with an emphasis on importing data provided in spreadsheet form—either in a Microsoft Excel file or in a Google Sheets document. While the chapter in Wickham et al. (2023) does cover writing data to Excel or Google Sheets, it does not discuss why you would want to do this.11\nThe coverage provided in Wickham et al. (2023) is excellent and I refer readers to that book for details on importing spreadsheet data. However, a weakness of Chapter 20 of Wickham et al. (2023) is its limited guidance on what I would regard as essential best practices for importing spreadsheet data.12 So I provide some guidance on these here.\n\n\n\n\n\n\n\n\nFigure 1: An expanded representation of the data science workflow\n\n\n\n\n\nFirst, the import process should be done as much as possible in a script that allows the whole process to be repeated. Importing spreadsheets falls clearly into the Curate portion of the extended “whole game” of a data analysis workflow depicted in Figure 1.13 The whole Curate process should be documented and scripted as much as possible. For example, if the spreadsheet data come from a URL for a Microsoft Excel file, then the download step should be [if possible] part of the script.\nSecond, the source data files should not be modified in any way. While this point is closely related to the previous one, I believe that it warrants specific emphasis. Clearly one should not be opening Excel files and modifying data by deleting columns or reformatting values, as these steps are undocumented and not repeatable in an automated fashion. My advice here mirrors that of Broman and Woo (2018, p. 7) in a slightly different context: “Your primary data file should be a pristine store of data. Write-protect it, back it up, and do not touch it.”\nEven simply opening the source Excel files in Excel and clicking save can modify the underlying files in ways that affect later parts of the data science workflow. I had one project where the source data appeared to be .xls Excel files, but were in fact simply structured text in HTML format. Scripts written on this basis performed better than ones that read them as Excel data.14 However, when updated spreadsheets were received from the vendor they first passed through the data team at the institution I then worked at and my scripts no longer worked. The data team had “helpfully” opened the files in Excel and saved them before sending them along to me. I had to insist that the data team pass along the data files exactly as they had received them without “helpful” steps in between.15\nThe deleterious effects of “simply” opening a file in Excel and saving it can be seen by comparing Table 1 with Table 2. Table 1 shows the original data, while Table 2 shows the file created by Excel. As can be seen, Excel changed the first column so that the data are no longer in a consistent (d/m/yyyy) format, mangled the first cusip value, and messed up the encoding (and I wasn’t even using Windows!) so that the Korean name in the third row is no longer legible.\n\n\n\nTable 1: Original CSV data\n\n\n\n\n\ntest_date\ncusip\nresult\nname\n\n\n\n\n1/7/2006\n“945324E7”\n1.7\nBob\n\n\n8/3/2013\n“12345678”\n2.6\nSarah\n\n\n28/4/2016\n“23456789”\nNA\n철수\n\n\n\n\n\n\n\n\n\nTable 2: Excel-exported CSV data\n\n\n\n\n\ntest_date\ncusip\nresult\nname\n\n\n\n\n1/7/06\n9.45E+12\n1.7\nBob\n\n\n8/3/13\n“12345678”\n2.6\nSarah\n\n\n28/4/2016\n“23456789”\nNA\n“_Êö÷"
  },
  {
    "objectID": "published/spreadsheets.html#scenario-2-using-spreadsheets-for-data-collection",
    "href": "published/spreadsheets.html#scenario-2-using-spreadsheets-for-data-collection",
    "title": "Data collection (with spreadsheets)",
    "section": "1.2 Scenario 2: Using spreadsheets for data collection",
    "text": "1.2 Scenario 2: Using spreadsheets for data collection\nWhile Chapter 20 of Wickham et al. (2023) focuses on importing data from spreadsheets, Broman and Woo (2018) focuses on the use of spreadsheets for data collection. Interestingly, Wickham et al. (2023) does not discuss primary data collection in any detail and Broman and Woo (2018) does not explicitly cover the process of importing data into statistical analysis software. However, it does seem implicit in Broman and Woo (2018) that importing data into such software will be part of the process. For example, Broman and Woo (2018, p. 9) suggest that “data rearrangement is best accomplished via code (such as with an R, Python, or Ruby script) so you never lose the record of what you did to the data.” In terms of Figure 1, Broman and Woo (2018) is largely about Collect and Wickham et al. (2023) picks up with Import.16\nI view this note as building on Broman and Woo (2018) by adding some additional recommendations, linking data collection with data importation à la Wickham et al. (2023), and providing a more comprehensive use case to highlight some issues and ideas not addressed by either Broman and Woo (2018) or Wickham et al. (2023).\nBroman and Woo (2018) appears to emphasize cases where the user of the data is entering the data directly. However, a very common use case I have encountered is one where I (along with collaborators) will be using the data, but the data are being entered by someone else (perhaps several others).\nAdditionally, references in Broman and Woo (2018) to things such as Serum_batch1_2015-01-30.csv, Mouse153 and glucose_6_weeks suggest a setting of collection of observations and measurements from scientific experiments. In contrast, a setting more familiar to me is that of collecting relatively large amounts of somewhat messy data from documents found on the internet. Typically, researchers need other people to collect these data because of time constraints and because either the data are too messy to be collected in an automated fashion with sufficiently low cost and high quality or their collection requires some element of judgement.\nThis note aims to expand on and refine the advice provided by Broman and Woo (2018) for situations where others—such as data entry specialists or research assistants—are entering data into spreadsheets and where the data are coming from messy sources rather than measurements from lab experiments. Broman and Woo (2018) provide a list of twelve recommendations. In this note, I will provide guidance specific to the setting I examine here and then follow that with comments on the recommendations in Broman and Woo (2018) in light of the different context.\nConsidering this context helps address the question that many users might have at this point: Why use spreadsheets at all? Given that spreadsheets have all these problems, it might seem better to use some other approach. For example, why not use a web form that is used to populate a database? I think the answer lies in a couple of points.\nFirst, many researchers are collecting data but do not have access to the skills needed to stand up a web page that allows data to be collected and put into a database. For all their problems, using spreadsheets removes the need for such skills.\nSecond, many data collection tasks are unclear. Precisely what data will be collected? What values are valid in certain columns? What strange cases need to be addressed and how? In such situations, often the best approach is to just start collecting data and adapt over time. Spreadsheets are often the best tool for such situations. Columns can be added later on. Data validation rules can be tightened and loosened as circumstances dictate. This can be much easier than bringing the web-and-database developer back in to add new fields to the forms and new columns to the database tables.\nAdditionally, data collection often needs to be quite free-form. For example, maybe it make sense to drop a link to a document to explain a classification decision made in another column. Spreadsheets allow this easily."
  },
  {
    "objectID": "published/spreadsheets.html#other-use-cases-for-spreadsheets",
    "href": "published/spreadsheets.html#other-use-cases-for-spreadsheets",
    "title": "Data collection (with spreadsheets)",
    "section": "1.3 Other use cases for spreadsheets",
    "text": "1.3 Other use cases for spreadsheets\nBefore moving on to consider data collection using spreadsheets in more detail, I briefly discuss other ways that spreadsheets are often used by data analysts.\n\n1.3.1 Spreadsheets for modelling\nSpreadsheet tools such as Microsoft Excel and Google Sheets are the mainstay of certain kinds of analysis, including financial modelling and valuation. Spreadsheets also allow people who use them for other reasons to easily run Monte Carlo simulations and the like. I view these uses as legitimate, but place them slightly outside the scope of data science in the sense implied by Figure 1, hence beyond the scope of this note.\nThe one word of caution I would offer is that one should not fall into the trap of thinking “I use Excel for modelling, so maybe I should also use Excel for statistical analysis.” The shortcomings of Excel for statistical analysis are well-documented and need not be repeated here (see Broman and Woo, 2018 for some leads).\nI will also note a couple of additional points. First, even if (say) Microsoft Excel is your preferred tool for producing financial or valuation models, there can be value in replicating analysis in something like Python or R. In 2005 or 2006, I was working with Morgan Stanley to produce a valuation model that could be used by institutional investor clients with data generated by Morgan Stanley research analysts. I would produce a version of the model in Excel and programmers would translate it into C++ code. In one conference call, one of the programmers mentioned that they had detected the change in formula after a certain number of years and implemented that in the code. Aargh! This change in formula was simply a spreadsheet error on my part that became all the more obvious when the logic was transcribed from Excel formulas to C++ code. The error was quickly rectified.\nSecond, there is a world of best practices to be followed with the use of spreadsheets in financial and valuation modelling that are deserving of (at least) a note of their own. When I worked at corporate finance consulting firm Stern Stewart in the late 1990s, a big part of the training of new analysts and associates was inculculating some of these best practices, such as separating inputs from calculations and documentating data sources and rationales for approaches. A lot of ideas used there are analogues of ideas found in discussions of reproducibility in data science.\n\n\n1.3.2 Spreadsheets for data analysis\nWhile Broman and Woo (2018) does mandate “No Calculations in the Raw Data Files”, this is coupled with a more lenient piece of advice (Broman and Woo, 2018, p. 7): “If you want to do some analyses in Excel, make a copy of the file and do your calculations and graphs in the copy.” I argue that this is actually not a good idea at all.\nIf the idea is that the analysis will be the data analytical form of brainstorming that will not form the basis of anything beyond that, I think that is fine. But if the calculations and graphs in the copy end up going in output of some kind (e.g., a report or a paper), then the data in the copy become the de facto canonical version of the data. If the original data are updated or corrected, there will often be no easy way to incorporate the associated changes into the analysis. Also it will often be difficult to be sure that the raw data have been unchanged. Having the original data as a “pristine store of data” (Broman and Woo, 2018, p. 7) does not help much if the data actually used have been corrupted in some way (e.g., by accidentally typing a value into a cell).17\n\n\n1.3.3 Spreadsheets for organizing output\nThe final use case for spreadsheets that I will discuss is the use of spreadsheets for producing output, such as tables and graphs. Many researchers who use statistical packages such as SAS or Stata reach the limits of their skills with those packages when it comes to producing plots or tables of statistical analyses for inclusion in papers or reports. Such users will often turn to packages such as Microsoft Excel to produce this kind of output.\nI would recommend that readers of this note consider working on their data science skills to obviate the use of Excel for this “last mile” of the data analysis journey for a number of reasons.\nFirst, using spreadsheets for this part of the process inevitably breaks the reproducibility of any analysis. The Excel workflow for making a plot is roughly:\n\nSave data for analysis\nOpen data in Excel spreadsheet.\nUsing your mouse, highlight the data to be used in graph.\nClick one of the graph icons.\nMake a number of seclections in various dropdown menus and checkboxes to produce the desired plot.\nCopy plot to Microsoft Word document.\n\nNow, suppose the data change. The whole process needs to be repeated again and it is very unlikely that any step (save perhaps the first one) is associated with any documentation whatsoever.\nOne consequence of using a manual workflow is that analyses can quickly become inconsistent with each other, as it is often seems not worthwhile to update earlier analyses when data change.\nSecond, using spreadsheets for this part of the process can lead to errors. Manual steps are easy to mess up and I’ve seen cases where column labels are switched or the signs of coefficients are wrong because they were updated manually.\nFinally, the plotting functionality of Excel will generally pale in comparison to what is available in Python or R (or even Stata).\nA lot of researchers seem unaware that it’s even possible for modern statistical packages to produce production-quality output without manual intervention at every step."
  },
  {
    "objectID": "published/spreadsheets.html#ipo-data",
    "href": "published/spreadsheets.html#ipo-data",
    "title": "Data collection (with spreadsheets)",
    "section": "3.1 IPO data",
    "text": "3.1 IPO data\nNasdaq provides information about IPOs on its website. I have organized these data in two parquet files: ipos.parquet and ipo_experts.parquet. A copy of the ipos.parquet file can be found at https://go.unimelb.edu.au/2rq8 and a copy of the ipos_experts.parquet file can be found at https://go.unimelb.edu.au/hrq8. If you want to follow along with the code below, you could download these data files into the nasdaq schema of a data repository organized along the lines of that I discuss in Appendix E of Empirical Research in Accounting: Tools and Methods and then load them into a DuckDB database using code like this:\n\nipo_experts &lt;- load_parquet(db, \"ipo_experts\", \"nasdaq\")\nipos &lt;- load_parquet(db, \"ipos\", \"nasdaq\")\n\nAlternatively, you could put these two files in, say ~/ipo_auditors, and then load them as follows:\n\nipo_experts &lt;- load_parquet(db, \"ipo_experts\", data_dir = \"~/ipo_auditors\")\nipos &lt;- load_parquet(db, \"ipos\", \"nasdaq\", data_dir = \"~/ipo_auditors\")\n\nFor example, details about the IPO of Galmed Pharmaceuticals Ltd. (GLMD) on 13 March 2014 can be found at https://www.nasdaq.com/market-activity/ipos/overview?dealId=926632-74581 and these data are also found in the ipo.parquet file. There are 4,409 rows in ipos.parquet and each row represents an IPO identified by the primary key column dealID.\nThe Nasdaq page also includes information about the “experts” associated with each IPO and this information is contained in ipo_experts.parquet. I collect some statistics on these deals in expert_counts and show the results in Table 3. As can be seen from the n_deals column, every IPO has an auditor, a company counsel, and a transfer agent, with some deals having more than one of these. It can be inferred from Table 3 that the typical IPO has multiple underwriters.\n\nexpert_counts &lt;-\n  ipo_experts |&gt; \n  group_by(role) |&gt; \n  summarize(n_deals = n_distinct(dealID), \n            n_rows = n(),\n            .groups = \"drop\") |&gt; \n  arrange(desc(n_deals))\n\n\n\n\n\nTable 3: Statistics for different kinds of experts\n\n\n\n\n\n\nrole\nn_deals\nn_rows\n\n\n\n\nTransferAgent\n4,409\n5,901\n\n\nAuditor\n4,409\n4,621\n\n\nCompanyCounsel\n4,409\n4,874\n\n\nLeadUnderwriter\n4,313\n10,110\n\n\nUnderwriterCounsel\n4,296\n4,682\n\n\nUnderwriter\n2,973\n13,190\n\n\n\n\n\n\n\n\nNot every deal has a lead underwriter, but from Table 4, it can be seen that all deals without a lead underwriter have at least one underwriter.\n\n\n\n\nTable 4: Statistics for IPOs without lead underwriters\n\n\n\n\n\n\nrole\nn_deals\nn_rows\n\n\n\n\nAuditor\n96\n115\n\n\nCompanyCounsel\n96\n109\n\n\nUnderwriter\n96\n249\n\n\nTransferAgent\n96\n148\n\n\nUnderwriterCounsel\n1\n1\n\n\n\n\n\n\n\n\nThe information for GLMD in the ipo_experts.parqueet file is shown in Table 5, where we see a number of things.\n\n\n\n\nTable 5: Experts associated with GLMD IPO\n\n\n\n\n\n\nexpertName\nrole\n\n\n\n\nBrightman Almagor Zohar & Co., a member firm of Deloitte Touc...\nAuditor\n\n\nBrightman Almagor Zohar & Co., a member firm of Deloitte...\nAuditor\n\n\nGreenberg Traurig, P.A.\nCompanyCounsel\n\n\nMaxim Group, LLC\nLeadUnderwriter\n\n\nVStock Transfer, LLC\nTransferAgent\n\n\nFeltl and Company, Inc\nUnderwriter\n\n\nMLV & Co. LLC\nUnderwriter\n\n\nEllenoff Grossman & Schole LLP\nUnderwriterCounsel\n\n\n\n\n\n\n\n\nFirst, there appear to be “two auditors”, albeit with names that appear to differ only in the point at which the names are truncated with “...”. Second, there are no identifiers for the experts other than their names. Names are problematic for many reasons. Firms can change names. Names can be abbreviated in different ways or misspelt or change in subtle ways that are not obvious to a human reader, but that make them unusable as identifiers in code. Third, at least for some expert types, there can be more than one expert for a given IPO.\nBefore moving onto the data on auditors provided on Form APs, I compile data on auditors and IPOs from the IPO data and store it in ipo_auditors.\n\nipo_auditors &lt;-\n  ipos |&gt; \n  select(dealID, companyName, CIK, pricedDate, CompanyAddress) |&gt; \n  inner_join(ipo_experts, by = \"dealID\") |&gt;\n  filter(role == \"Auditor\") |&gt;\n  rename(cik = CIK) |&gt;\n  select(-role) |&gt;\n  compute()"
  },
  {
    "objectID": "published/spreadsheets.html#form-ap-data",
    "href": "published/spreadsheets.html#form-ap-data",
    "title": "Data collection (with spreadsheets)",
    "section": "3.2 Form AP data",
    "text": "3.2 Form AP data\nAccording to the PCAOB, “registered audit firms are required to submit Form AP, Auditor Reporting of Certain Audit Participants, to disclose the names of engagement partners and other accounting firms that participated in their audits of public companies.” The PCAOB provides a database constructed from Form AP filings on its website and I have written about this in a previous note. The Form AP database is contained in the file form_aps.parquet that I have made available at https://go.unimelb.edu.au/4iq8.\nAgain you could download this data file into the pcaob schema of a data repository organized along the lines of that I discuss in Appendix E of Empirical Research in Accounting: Tools and Methods and then load it into a DuckDB database using code like this:\n\nform_aps &lt;- load_parquet(db, \"form_aps\", \"pcaob\")\n\nAlternatively, you could put this file in (say) ~/ipo_auditors and then load it as follows:\n\nform_aps &lt;- load_parquet(db, \"form_aps\", data_dir = \"~/ipo_auditors\")\n\nWe can collect auditor IDs associated with each CIK from form_aps. Of course, firms change auditors over time, but if we are interested in the auditors at the time of IPO, it might make sense to pick the first auditor associated with each CIK.23 I close this subsection by creating first_auditors, a data frame with information on the first auditor for each issuer:\n\nfirst_auditors &lt;-\n  form_aps |&gt;\n  group_by(issuer_cik) |&gt;\n  filter(audit_report_date == min(audit_report_date, na.rm = TRUE)) |&gt;\n  ungroup() |&gt;\n  rename(cik = issuer_cik) |&gt;\n  mutate(cik = if_else(cik == \"9999999999\", NA, cik)) |&gt;\n  select(cik, firm_id, firm_name, firm_country, audit_report_date) |&gt;\n  compute()"
  },
  {
    "objectID": "published/spreadsheets.html#creating-the-link-table",
    "href": "published/spreadsheets.html#creating-the-link-table",
    "title": "Data collection (with spreadsheets)",
    "section": "3.3 Creating the link table",
    "text": "3.3 Creating the link table\nI now move onto the task of creating a link table between the IPO data and the Form AP data. Even if the ultimate goal is to produce a data set drawing on data on both IPOs and auditors, it is important to view the production of a link table as the final product of the linking task. In its simplest form, a link table contains primary-key values from each of the two data sources with the appropriate primary key being driven by the level at which observations are being linked.24 In the current context, we want to link each IPO—identified by dealID—with the auditor in the Form AP database.\nIn the Form AP database, an auditor is identified by firm_id. It interesting to note that this is the case even though firm_id is in no way a primary key for the form_aps table. Instead, we might think of firm_id as the primary key of a hypothetical auditors table and we can create this table ourselves as follows:\n\nauditors &lt;-\n  form_aps |&gt;\n  group_by(firm_id) |&gt;\n  window_order(desc(audit_report_date)) |&gt;\n  mutate(firm_name = first(firm_name),\n         firm_country = first(firm_country)) |&gt;\n  distinct(firm_id, firm_name, firm_country) |&gt;\n  compute()\n\nBy construction, firm_id is the primary key for this auditors table. I include the names and countries for each audit firm to facilitate data collection later on. Because firms can change names over time, I use the latest name on form_aps for each firm_id.25\n\n3.3.1 Name-based links\nI start by matching the data on IPOs with data on auditors using CIKs. I conjecture that, if a match between expertName (from the IPO data) and firm_id and firm_name (from the Form AP data) shows up many times, there’s a good chance that it’s valid match. In constructing top_matches, I require that a match be present at least five times. This threshold is arbitrary, but it’s just a starting point in two ways. First, I will apply some judgement to the matches before flagging them as valid. Second, I will come back to additional candidate matches in subsequent analyses.\n\ntop_matches &lt;-\n  ipo_auditors |&gt;\n  left_join(first_auditors, by = \"cik\") |&gt; \n  filter(!is.na(firm_name)) |&gt; \n  count(firm_id, expertName, firm_name, sort =  TRUE) |&gt;\n  collect() |&gt;\n  filter(n &gt;= 5)\n\nI will use a Google Sheets document to collect the data. In the past in similar situations, I have often taken data such as that in top_matches and exported it to a CSV file that I then open in Google Sheets. Here I instead chose to create a new Google Sheets file using the gs4_create() from googlesheets4:\n\noptions(gargle_oauth_email = TRUE)\ngs_ipo_auditors &lt;- gs4_create(name = \"ipo-auditors\")\n\nTyping gs_ipo_auditors at the R console provides some information about the (initially empty) Google Sheets document:\n\ngs_ipo_auditors\n\n── &lt;googlesheets4_spreadsheet&gt; ─────────────────────────────────────────────────────────────────────────\nSpreadsheet name: ipo-auditors                                \n              ID: 1m2F4nnhJyg81gj17-h3JWeeSbd4eyRP_SUYlX1OzR-8\n          Locale: en_US                                       \n       Time zone: Etc/GMT                                     \n     # of sheets: 1                                           \n\n── &lt;sheets&gt; ────────────────────────────────────────────────────────────────────────────────────────────\n(Sheet name): (Nominal extent in rows x columns)\n      Sheet1: 1000 x 26\nAfter I’ve created the Google Sheets file, I can access it using the key seen in the output above:\n\ngs_ipo_auditors &lt;- as_sheets_id(\"1m2F4nnhJyg81gj17-h3JWeeSbd4eyRP_SUYlX1OzR-8\")\n\nI can put the data from top_matches in the Google Sheets spreadsheet using write_sheet(). Note that this code will not work for you with my spreadsheet, as I have not given you writexaccess to that spreadsheet.\n\nwrite_sheet(top_matches, ss = gs_ipo_auditors, sheet = \"top_matches\")\n\n✔ Writing to ipo-auditors.\n✔ Writing to sheet top_matches.\nThe next step I took was to open the spreadsheet in a browser:\n\ngs4_browse(gs_ipo_auditors)\n\nI then inserted a new column valid to the right of the existing columns. I also renamed the sheet from top_matches to name_matches.26 I inspected the matches and classified those rows that looked good as valid by putting the value TRUE in the valid column. Most of the first 30 or so rows looked good. But a match of the expertName of “Ernst & Young LLP” with the firm_name of “KPMG LLP” is clearly not valid, so I put FALSE in the valid column for this case.27\nLater I added more candidate matches to the name_matches sheet and marked as valid all those that looked good. In some cases it made sense to use the AuditorSearch functionality provided by the PCAOB or to investigate data in SEC filings or the Form AP database.28 Note that I retained all variants of expertName in the IPO data so that name-based matches can be found for such variants.\n\n\n3.3.2 Problematic name-based links\nThe next step I took was to look at cases where expertName was associated with more than one distinct firm_id value.\n\ntop_matches_processed &lt;- read_sheet(ss = gs_ipo_auditors, sheet = \"top_matches\")\n\ntop_match_problems &lt;-\n  top_matches_processed |&gt;\n  filter(valid) |&gt;\n  group_by(expertName) |&gt;\n  filter(n_distinct(firm_id) &gt; 1) |&gt;\n  ungroup() |&gt;\n  select(-valid) |&gt;\n  arrange(expertName, desc(n))\n\nLooking at Table 6, we see that there are multiple firm_id values for some values of expertName even when we have deemed a match to be valid.29\n\n\n\n\nTable 6: Contents of top_match_problems\n\n\n\n\n\n    \n\n    \n\n    \n    \n    \n      \n        \n        \n              \n                firm_id\n                expertName\n                firm_name\n                n\n              \n        \n        \n        \n                \n                  34\n                  Deloitte & Touche LLP\n                  Deloitte &  Touche LLP\n                  368\n                \n                \n                  1208\n                  Deloitte & Touche LLP\n                  Deloitte LLP   (French Translation:  Deloitte S.E.N.C.R.L./s.r.l.)\n                  8\n                \n                \n                  1147\n                  Deloitte & Touche LLP\n                  Deloitte LLP\n                  5\n                \n                \n                  42\n                  Ernst & Young LLP\n                  Ernst & Young LLP\n                  650\n                \n                \n                  1438\n                  Ernst & Young LLP\n                  Ernst & Young LLP\n                  11\n                \n                \n                  185\n                  KPMG LLP\n                  KPMG LLP\n                  387\n                \n                \n                  1118\n                  KPMG LLP\n                  KPMG LLP\n                  10\n                \n                \n                  85\n                  KPMG LLP\n                  KPMG LLP\n                  8\n                \n                \n                  5395\n                  Marcum Asia CPAs LLP\n                  Marcum Asia CPAs LLP\n                  18\n                \n                \n                  711\n                  Marcum Asia CPAs LLP\n                  Friedman LLP\n                  17\n                \n                \n                  238\n                  PricewaterhouseCoopers LLP\n                  PricewaterhouseCoopers LLP\n                  415\n                \n                \n                  876\n                  PricewaterhouseCoopers LLP\n                  PricewaterhouseCoopers LLP\n                  11\n                \n        \n      \n    \n\n\n\n\n\n\nSome additional digging (using data in form_aps) confirms that the main explanation is that the different firm_id values relate to different national partnerships of Big Four audit firms. For example, as seen in Table 7, firm_id of 34 is the US arm of Deloitte, while 1147 and 1208 refer to the UK and Canadian operations, respectively.\n\n\n\n\nTable 7: Countries of firms in top_match_problems\n\n\n\n\n\n    \n\n    \n\n    \n    \n    \n      \n        \n        \n              \n                firm_id\n                firm_name\n                firm_country\n              \n        \n        \n        \n                \n                  34\n                  Deloitte &  Touche LLP\n                  United States\n                \n                \n                  1147\n                  Deloitte LLP\n                  United Kingdom\n                \n                \n                  1208\n                  Deloitte LLP   (French Translation:  Deloitte S.E.N.C.R.L./s.r.l.)\n                  Canada\n                \n                \n                  1438\n                  Ernst & Young LLP\n                  United Kingdom\n                \n                \n                  42\n                  Ernst & Young LLP\n                  United States\n                \n                \n                  711\n                  Friedman LLP\n                  United States\n                \n                \n                  85\n                  KPMG LLP\n                  Canada\n                \n                \n                  185\n                  KPMG LLP\n                  United States\n                \n                \n                  1118\n                  KPMG LLP\n                  United Kingdom\n                \n                \n                  5395\n                  Marcum Asia CPAs LLP\n                  United States\n                \n                \n                  876\n                  PricewaterhouseCoopers LLP\n                  United Kingdom\n                \n                \n                  238\n                  PricewaterhouseCoopers LLP\n                  United States\n                \n        \n      \n    \n\n\n\n\n\n\n\n\n3.3.3 Manual links by dealID\nOne approach to handling these cases would be to get information on countries from the IPO data and use that to distinguish the firm_id values to be used for cases with multiple apparent name-based matches. However, given the relatively small number of non-US matches to be handled, I instead chose to grab the dealID values for those cases and proceed to validate matches on a deal-by-deal basis.\nTo start with, I identified the most common firm_id value for each expertName. In effect, I will set these as the default for the name-based matches.30\n\ntop_match_problem_top_ids &lt;-\n  top_match_problems |&gt;\n  group_by(expertName) |&gt;\n  filter(n == max(n, na.rm = TRUE)) |&gt;\n  ungroup() |&gt; \n  select(firm_id) |&gt;\n  pull()\n\ntop_match_problem_top_ids\n\n[1]   34   42  185 5395  238\n\n\nI then collect data on all other cases where expertName is in top_match_problems and there is some match to Form AP data (i.e., firm_name is present).\n\ntop_match_problems_countries &lt;-\n  ipo_auditors |&gt;\n  left_join(first_auditors, by = \"cik\") |&gt;  \n  filter(expertName %in% top_match_problems$expertName,\n         !firm_id %in% top_match_problem_top_ids,\n         !is.na(firm_name)) \n\nI then export data related to these matches to an additional tab deal_matches_add that I can use to verify candidate matches for each dealID.\n\ntop_match_problems_countries |&gt;\n  select(dealID, cik, CompanyAddress, firm_id, firm_name, \n         firm_country, audit_report_date) |&gt;\n  collect() |&gt;\n  write_sheet(ss = gs_ipo_auditors, sheet = \"deal_matches_add\")\n\nIn this case, I renamed this sheet to deal_matches, added a valid column much as before, and also added additional columns such as sec_filings and notes. I use sec_filings to create a formula-based hyperlink to the SEC filings around the time of the IPO that I can use to check cases where the information in the spreadsheet does not suffice to give me confidence in validating a match. Looking at the formula for cells with “SEC Filings” in the sec_filings column of the deal_matches sheet, you will see that the formula keys off values in cik and pricedDate to make it easy to get to the relevant filings.31 When I locate a filing with the necessary supporting information to make a call on whether a candidate match is value, I can include a link to that filing in the notes column of the spreadsheet.32 It is relatively straightforward to inspect the IPO prospectus and look for the auditor’s sign-off, which will mention the office (e.g., “London, United Kingdom”) that applies and the associated value can be entered under firm_id and valid can be set to TRUE.\nI repeated variants on the steps above until the tabs name_matches and deal_matches contained a fairly complete set of data on both name-based and deal-level matches of each IPO with the firm_id value for the auditor at the time of the IPO.\n\n\n3.3.4 Combining data to produce the link table\nHaivng populated name_matches and deal_matches with data on valid matches, the next step is to use the data in these sheets to create a link table between IPOs and firm_id values. We can start by loading data from the two key sheets in our Google Sheets file.\n\nname_matches &lt;- read_sheet(ss = gs_ipo_auditors, sheet = \"name_matches\")\ndeal_matches &lt;- read_sheet(ss = gs_ipo_auditors, sheet = \"deal_matches\")\n\nI start by loading the valid deal-level matches of IPOs to auditors. I label these as \"manual\" matches and store them in deal_matches_processed.\n\ndeal_matches_processed &lt;-\n  deal_matches |&gt;\n  filter(valid) |&gt;\n  select(dealID, firm_id) |&gt;\n  mutate(match_type = \"manual\")\n\nI then process the name-based matches that I will use later on. Note that I give priority to deal-level matches: if an IPO has a valid match in deal_matches, I do not consider a potential match using names in name_matches. This is accomplished by putting all valid matches from deal_matches in deal_matches_processed and then using an anti-join when constructing name_matches_processed.\n\nname_matches_processed &lt;-\n  name_matches |&gt;\n  filter(valid) |&gt;\n  select(firm_id, expertName) |&gt;\n  inner_join(ipo_auditors, by = \"expertName\", copy = TRUE) |&gt;\n  mutate(match_type = \"name-based\") |&gt;\n  anti_join(deal_matches_processed, by = \"dealID\") |&gt;\n  select(dealID, firm_id, match_type)\n\nHaving constructed deal_matches_processed and name_matches_processed, we can combine them using union_all().\n\nall_matches &lt;-\n  deal_matches_processed |&gt;\n  union_all(name_matches_processed) |&gt;\n  copy_to(dest = db, df = _, name = \"all_matches\",\n          overwrite = TRUE)\n\nBased on the data in the spreadsheet, all_matches is effectively the final link table.\n\n\n3.3.5 Perfecting the matches\nSo far we have gone through expertName values and linked them to firm_id values. The results of this work are found in the name_matches data frame, which is based on the name_matches sheet in our ipo-auditors Google Sheets file. We have also manually matched IPOs to firm_id values. Because we manually verified these matches, these should be beyond reproach.\nHowever, we might want to go back through the name-based matches to check that these are valid. One idea is to look at name-based links of IPO firms with their auditors that do not have corresponding entries in the Form APs database. It is certainly not the case that every IPO firm (i.e., issuer_cik) will have a Form AP filing made by the auditor (firm_id) it had at IPO. Many firms will change auditors after IPO before making filings such as 10-Ks that are expected to trigger a corresponding Form AP filing. Nonetheless, by looking more closely at these non-matches we should be able to identify cases where, for example, we have matched a Canadian firm with a US auditor rather than the Canadian audit firm that makes the relevant filings on Form AP.\nI then compile data on all auditors associated with each issuer CIK, including the names and countries, and store the result in all_ipo_auditors. I store the firm_id, firm_name and firm_country values for each auditor for each firm in list columns.\nList columns in DuckDB are very much like their counterparts in R.33 Unfortunately, dbplyr does not provide the interface we need to directly invoke the list() aggregate with an ORDER BY clause. Fortunately, we can use sql() to inject precisely the SQL we need.34\n\nall_ipo_auditors &lt;-\n  form_aps |&gt;\n  distinct(issuer_cik, firm_id) |&gt;\n  inner_join(auditors, by = \"firm_id\") |&gt;\n  group_by(issuer_cik) |&gt;\n  summarize(\n    firm_ids = sql(\"list(firm_id ORDER BY firm_id)\"),\n    firm_names = sql(\"list(firm_name ORDER BY firm_id)\"),\n    firm_countries = sql(\"list(firm_country ORDER BY firm_id)\")\n    ) |&gt;\n  compute()\n\nI collect data on all name-based matches where the matched auditor in firm_id is not among the corresponding firm_ids found in all_ipo_auditors and store the result in no_form_ap_matches.\n\nno_form_ap_matches &lt;-\n  all_matches |&gt;\n  filter(match_type == \"name-based\") |&gt;\n  inner_join(ipos, by = \"dealID\") |&gt;\n  select(dealID, CIK, CompanyAddress, firm_id, pricedDate) |&gt;\n  distinct() |&gt;\n  rename(issuer_cik = CIK) |&gt;\n  inner_join(all_ipo_auditors, by = \"issuer_cik\") |&gt;\n  filter(list_contains(firm_ids, firm_id)) |&gt;\n  compute()\n\nNote that once IPOs have been added to the deal_matches tab, it probably makes sense to exclude them from no_form_ap_matches by adding a line such as anti_join(deal_matches, copy = TRUE, by = \"dealID\") after the inner_join() above.\nWe can dump the data from no_form_ap_matches into a Google Sheets tab so that we can dig into them more deeply.\n\nno_form_ap_matches |&gt;  \n  mutate(across(c(firm_ids, firm_names, firm_countries), as.character)) |&gt;\n  arrange(dealID) |&gt;\n  collect() |&gt;\n  write_sheet(ss = gs_ipo_auditors, sheet = \"all_ipo_auditors\")\n\nTo better understand how one can use the data that have been placed in the all_ipo_auditors tab, I describe how I used the data myself. The first thing I did was to copy the data from the all_ipo_auditors sheet over to the bottom of the deal_matches sheet. Initially I retained the column names in the data that I copied over so that I could check that the columns in the pasted data line up with those in the deal_matches tab. I then deleted the unnecessary extra row of column names once I had confirmed this. The next step was to code all these additional rows as invalid (i.e., by putting FALSE in the valid column) because these should not be used until they are confirmed. I now describe how I confirmed the data for two example IPOs.\nThe first example IPO I considered was the NYSE IPO of Tamboran Resources Corp (dealID of 1295264-109998). I first observed that the company has an Australian address.35 Given that the expertName value is “Ernst & Young”, a name-based match will associate this IPO with the US operations of Ernst & Young. However, the Form AP filings have been made by the Australian affiliate of Ernst & Young (firm_id of 1435). So I put 1435 under firm_id and set valid to TRUE after doing so. As a result, this IPO will be correctly associated with Ernst & Young Australia in the final link table.\nAnother example is the 2015 IPO on Nasdaq of Collegium Pharmaceutical (dealID of 600781-78033), where “Grant Thornton LLP” is listed as the auditor on ipo_experts. But all rows in form_aps with issuer_cik == \"0001267565\" have the auditor as Deloitte & Touche LLP. Looking at the prospectus, we can see that Grant Thornton LLP of Boston, Massachusetts was indeed the auditor at the time of IPO and only later did Deloitte & Touche LLP take over. At this point, I have two options. One option would be to leave the valid column as FALSE for this match, so that a name-based match of “Grant Thornton LLP” to firm_id of 248 occurs. The other option is to encode what I have learnt and set firm_id in deal_matches to 248 and set the valid column as TRUE, so that a manual match occurs. The benefits of the latter option are that I would never need to revisit this case in the future and that I would recognize my match as being in the higher-quality “manual” category.\nAs I write this, most cases in the deal_matches tab are coded with valid set to FALSE. This means those rows are not used and matches for these deals are based on names. However, if we wanted to “perfect” our matches, we could investigate each row much as we did for the two examples above.\nAt this point, some readers might feel that I am being inconsistent regarding the reproducibility of the steps I am taking. How is manually copying data from from the all_ipo_auditors sheet to the deal_matches sheet and then manually moving data around consistent with the “every step should be done using code” mantra I am pushing? The answer is that there are portions of manual data collection that are inherently manual. Clicking links, reading prospectus documents, and typing firm_id values into rows are all inherently manual steps. Nonetheless, these manual steps should be kept to a minimum: anything automatic that can be achieved using code probably should be done in code to make the process as reproducible, transparent, and error-free as possible.\nBecause there are inevitably some manual steps, there is the inevitable possibility of bad data being introduced. For example, I could easily have entered “284” instead of “248” as the firm_id for the Collegium Pharmaceutical IPO above. I think there are are three primary safeguards to minimize the risk of this kind of bad data entering the process.\nFirst, be careful. After entering “284” I could easily search the name_matches tab for “284” to check that I have entered a valid number. I could also reduce the risk of errors by copying “248” from the name_matches tab rather than typing it from scratch.\nSecond, make it easy to check the hand-collected data. By documenting my steps with links to supporting information and notes explaining judgments, I make it easier for someone else (perhaps “future me”) to check my work.\nThird, look for ways to check the integrity of data and flag potential issues. For example, if I entered “284” instead of “248” as the firm_id for Grant Thornton LLP in name_matches, the process I described above would flag these cases in a version of no_form_ap_matches and I should quickly observe a lot of cases where “284” is the apparent firm_id while Form AP filings are made by Grant Thornton LLP with firm_id of 248. I might also compare values in firm_id in the matched table with valid values according to the PCAOB.\n\n\n3.3.6 Checking for gaps\nFinally, we look for any deals in ipo_auditors that are not matched in all_matches and add these to our Google Sheets file. One reason for these cases would be new IPO deals being added to the data with expertName values not handled already.\n\nipo_auditors |&gt; \n  anti_join(all_matches, by = \"dealID\", copy = TRUE) |&gt;\n  mutate(firm_id = NA, firm_name = NA, firm_country = NA) |&gt;\n  select(dealID, cik, CompanyAddress, firm_id, expertName,\n         firm_name, firm_country, pricedDate) |&gt;\n  collect() |&gt;\n  write_sheet(ss = gs_ipo_auditors, sheet = \"extra_deals\")\n\nThe cases in extra_deals sheet could be handled by adding additional name-based matches to name_matches or additional deal-level matches to deal_matches as seems most appropriate. For the most part I handled cases added to extra_deals by moving them to deal_matches as the first step and processing as I did for other cases above. One this has been done, we can run the “extra_deals” code again and, if all is well, there will be no additional deals in extra_deals after doing so.\n\n\n3.3.7 Persisting the link table\nAt that point, I can take the data in all_matches and—using the language of Figure 1—Persist it to disk. In my case, I add ipo_auditors.parquet to the nasdaq “schema” of my parquet-based data repository. More details on this data repository can be found in an appendix of Empirical Research in Accounting: Tools and Methods.\n\nall_matches |&gt;\n  mutate(firm_id = as.integer(firm_id)) |&gt;\n  collect() |&gt;\n  write_parquet(sink = file.path(Sys.getenv(\"DATA_DIR\"), \n                                 \"nasdaq\", \"ipo_auditors.parquet\"))"
  },
  {
    "objectID": "published/spreadsheets.html#no-empty-cells",
    "href": "published/spreadsheets.html#no-empty-cells",
    "title": "Data collection (with spreadsheets)",
    "section": "4.1 “No Empty Cells”",
    "text": "4.1 “No Empty Cells”\nIn the case study above, there are inevitably empty cells before data are collected. In some cases, there are empty cells even after data are collected. For example, the sec_filings column is only populated on an as-needed basis.\nWhat Broman and Woo (2018) says about empty cells is valid in general, so I think the recommendation should merely be refined a little. For example, “NA” should be used to flag cases where there is the decision is to code a cell as missing, but this is very different from making a cell blank prior to the RA evaluating the case. It is probably better to view “NA” as reflecting some conscious output of the data collection process and to recognize that sometimes not every value will be relevant (so there is no value in having an RA code a column as “NA” if it wasn’t even examined).\nOther recommendations about blank cells in Broman and Woo (2018) are completely valid. In general, blank cells should not be used to represent zeros and blank cells should not be implicitly “filled” in a way that is “obvious” if looking at the spreadsheet, but not at all obvious to the data analyst."
  },
  {
    "objectID": "published/spreadsheets.html#no-calculations-in-the-raw-data-files",
    "href": "published/spreadsheets.html#no-calculations-in-the-raw-data-files",
    "title": "Data collection (with spreadsheets)",
    "section": "4.2 “No Calculations in the Raw Data Files”",
    "text": "4.2 “No Calculations in the Raw Data Files”\nThe prescription here (Broman and Woo, 2018, p. 7) is that “your primary data files should contain just the data and nothing else: no calculations, no graphs.” I think I would relax this prescription a little in the context of sometimes-messy data collection like the ones I have used spreadsheets for.\nSometimes the data are the results of calculations and I think it would be fine to use the spreadsheet functionality to support data collection efforts. Suppose I were hand-collecting data on executive compensation and the value in the salary column is almost always found as a single value that can be typed in by the RA. But then one firm reports salary as 12 monthly payments of $132,236. In such a case, I think it would be perfectly appropriate for the RA to type = 12 * 132236 in the cell under salary rather than calculating by hand and typing the result in.\nOther situations might go the other way. For example, suppose that director_names contained a list of names of directors, such as ['Anne', 'Bob', 'Clara']. It might still make sense to have the RA enter a value under num_directors (3 in this case) even though this could be easily calculated, as this would provide an additional check. For example, if director_names is just ['Anne', 'Bob'] and num_directors is 3 then it’s probably worth double-checking the data.\nIn other cases, calculations can facilitate the collection of data in the first place. For example, the URL behind the hyperlink for sec_filings used in the case study above is calculated. In other cases, calculations might provide guidelines that help the RA collect data in other fields. For example, total_compensation might be collected by the RA and it might be helpful to calculate this value from components for comparison to allow discrepancies to be flagged and resolved as part of the data collection process."
  },
  {
    "objectID": "published/spreadsheets.html#do-not-use-font-color-or-highlighting-as-data",
    "href": "published/spreadsheets.html#do-not-use-font-color-or-highlighting-as-data",
    "title": "Data collection (with spreadsheets)",
    "section": "4.3 “Do Not Use Font Color or Highlighting as Data”",
    "text": "4.3 “Do Not Use Font Color or Highlighting as Data”\nI do not disagree with this one, but I would say that it can be fine to use font colour or highlighting to communicate with humans: “Please focus on the cells I have coloured green before moving on to the other cases.” The critical thing is that the colours are not being used as data per se."
  },
  {
    "objectID": "published/spreadsheets.html#make-backups",
    "href": "published/spreadsheets.html#make-backups",
    "title": "Data collection (with spreadsheets)",
    "section": "4.4 “Make Backups”",
    "text": "4.4 “Make Backups”\n“Make regular backups of your data. In multiple locations.” This recommendation (Broman and Woo, 2018, p. 8) has perhaps not aged well. For example, what counts as “multiple locations” in a world in which everything lives in some cloud service like Google Drive or Dropbox? Also, precisely what should be backed up if one is collecting data in Google Sheets? I would need to export data to a different format (e.g., Microsoft Excel) and back that up if my concern is that Google might disappear and take my data with it.\nOne idea might be to export the underlying data to CSV and back that up, but this would mean losing a lot of the cell notes and such like that were an integral part of the data collection process.\nAlso Broman and Woo (2018, p. 8) suggests “keep all versions of the data files” without indicating what counts as a version of a file. If an RA is between entering data in one column and entering data in another column, we probably don’t want to keep that version. Nor is it clear that a backup between each row is necessary. If you’re a researcher working on papers, the sensible view might be that some backup be retained for each version of a paper that is posted or shared with others. In such a case, one could restore the spreadsheet data applicable to a particular version of your paper. But this can be unhelpful in practice if you are using data from other sources and isn’t doing some kind of version control for those other sources.\nAdditionally, my experience is that “regular backups … in multiple locations” can be a recipe for complexity and confusion. If I need to go back to an earlier version, how do I decide whether to use the copy of glucose_6_weeks stored on a thumb drive in my drawer or the copy of glucose_6_weeks in Dropbox? Perhaps I would need to add the date on which I made the backup (e.g., glucose_6_weeks_2024-12-31) to allow me to distinguish them. But maybe I don’t always want the latest version of the data (if I did, why keep anything but the last copy?). If the reason for restoring the data is some issue that came up in the last week of 2024 then I’d probably prefer the glucose_6_weeks_2024-12-21 version. Of course, I may not even be sure when the issue arose, so versions named just for dates may not be particularly helpful.\nMy sense is that if you are collecting data in spreadsheets, you are probably not the lynchpin of some multi-billion dollar Stage III drug trial aimed at getting FDA approval. So accepting the “Google ceases to exist” risk does not seem unreasonable. Even “Google Sheets ceases to exist” seems tolerable (or at least not very different from “I can’t open Microsoft Excel files from 2024” risk) for almost any use case where storing data in a spreadsheet makes sense.\nMy recommendation is to rely on version control features embedded in Google Sheets for organization of data somewhat less critical than the codes for the US nuclear arsenal. For example, “name current version” allows you to label a version as something like “qje_submission” and this is likely to suffice for the relatively low stakes of most data collection efforts conducted with spreadsheets.36"
  },
  {
    "objectID": "published/spreadsheets.html#save-the-data-in-plain-text-files",
    "href": "published/spreadsheets.html#save-the-data-in-plain-text-files",
    "title": "Data collection (with spreadsheets)",
    "section": "4.5 “Save the Data in Plain Text Files”",
    "text": "4.5 “Save the Data in Plain Text Files”\nThis one is a little difficult to evaluate, as its rationale is not entirely clear. One reason is perhaps given by the statement that “this sort of nonproprietary file format [CSV] does not and never will require any sort of special software” (Broman and Woo, 2018, p. 8). But this claim arguably applies to the ipo_auditors.parquet file that I created above.37\nThe “CSV files are easier to handle in code” is arguably false because parquet files include information about data types and the like that are missing from CSV files.\nIn short, I am not sure that I fundamentally disagree with this recommendation if it simply means that there should be a “persist” step along the lines of that depicted in Figure 1 and the persistent format is something “nonproprietary” like CSV or parquet. An implicit assumption in Figure 1 is that the data are being persisted as part of the Curate process rather than the Curate and Understand processes being tangled messes where it is unclear where one begins and the other ends. I would argue that the spreadsheet plus code should be viewed together as a unit that fosters reproducibility."
  },
  {
    "objectID": "published/spreadsheets.html#create-a-data-dictionary",
    "href": "published/spreadsheets.html#create-a-data-dictionary",
    "title": "Data collection (with spreadsheets)",
    "section": "4.6 “Create a Data Dictionary”",
    "text": "4.6 “Create a Data Dictionary”\nAt some level it is difficult to quibble much with this one. However, it has an air of “eat less; do more exercise” (as advice for losing weight) to it. I would guess that most spreadsheets used for data collection do not come with the kind of data dictionary envisaged by Broman and Woo (2018, p. 8). For example, there is no data dictionary in the Google Sheets document I created for linking IPOs with auditor IDs. That said, it is probably a good idea to make one. In my case, the critical variables are deal_ID and firm_id, which are both defined by other entities (Nasdaq and PCAOB, respectively). Nonetheless, some discussion of the attributes of those variables likely makes sense, as does some discussion of the purpose of linking the tables so that a user of the data can understand the rationale for any decisions made when the links are not entirely clear.38\nMore importantly, I think that the data dictionary is less important than other documentation that can be associated with a spreadsheet, such as discussion of the goals of data collection and scripts used to test features of the data and to create final data sets that are passed along to the Understand process as depicted in Figure 1.\nIt may be that the best documentation is something like the document you are reading right now, with discussion of data sources, issues to be addressed, and illustrative code and all stored somewhere like GitHub."
  },
  {
    "objectID": "published/spreadsheets.html#audit-fee-data",
    "href": "published/spreadsheets.html#audit-fee-data",
    "title": "Data collection (with spreadsheets)",
    "section": "5.1 Audit fee data",
    "text": "5.1 Audit fee data\nIn September 2020, Edmund Tadros of the Australian Financial Review published an article pointing out that the Big Four audit firms were really more a “Big Three” in Australia. This article was based on research by me and Tom Barry where we found that Deloitte had a much smaller share of the top end of the corporate auditing market in Australia than its Big Four rivals.\nWhile the primary data source for audit fees for Australian companies is Securities Industry Research Centre of Asia-Pacific (SIRCA), which in turn appears to get the data from Morningstar, these data are riddled with errors.\nWe flagged suspicious values using a number of criteria. For example, audit fees greater than 60 million Australian dollars or less than zero. Observations with changes in audit fees of between more than 50%. Flagged cases were added to a Google Sheets sheet (corrections) including the necessary identifiers and filings with the Australian Stock Exchange (ASX) were used to determine the correct values, which are then entered into the corrections sheet. Links to the filings from which the correct information were obtained are retained so that the data collection process is a transparent as possible.\nNote that the corrections data is combined with the original data much as the deal-level data are combined with the name-based matches for the IPO auditor data above. That is, the observations in the corrections sheet are given priority over the information in the SIRCA data (again an anti-join is used to combine the data).\nBecause only values from the original SIRCA data that have been flagged and corrected are used, the majority of the rows of data are still coming from the SIRCA data. This means that the data process is robust to the continual updates that are made as more firms release annual reports with new data on audit fees. However any red flags raised by the new data should be checked much as the older values were."
  },
  {
    "objectID": "published/spreadsheets.html#activist-directors",
    "href": "published/spreadsheets.html#activist-directors",
    "title": "Data collection (with spreadsheets)",
    "section": "5.2 Activist directors",
    "text": "5.2 Activist directors\nGoogle Sheets were used to collect data on activist directors for Gow et al. (2024).39 A challenge with that paper is that, because not all activist directors in our sample appear in the Equilar database we used as a main source of data on directors, we could not simply rely on an existing identifer to distinguish one director from another.\nWe also hand-collected data on key events associated with activism events and data to match CUSIPs in our activism data with PERMNOs.\nBecause data collection for this project began when were were fairly unfamiliar with best practices for such data collection, our Google Sheets are effectively organized in sheets that represent waves of data collection. We used scripts (such as the script to create the key activist_directors data frame) to combine the sheets into single tables for analysis."
  },
  {
    "objectID": "published/spreadsheets.html#director-ethnicities",
    "href": "published/spreadsheets.html#director-ethnicities",
    "title": "Data collection (with spreadsheets)",
    "section": "5.3 Director ethnicities",
    "text": "5.3 Director ethnicities\nAn extensive data set on ethnicities of directors was constructed for Gow et al. (2023). This was a complex endeavour involving multiple steps, so I describe each step in turn.\n\n5.3.1 Database of director photos\nWe started with a repository of photos of directors from SEC filings on EDGAR; we later added photos from Equilar. The first step with regard to EDGAR photos was to link these with the director identifier in the Equilar database (executive_id) and for each filing a page was a created to match photos to each executive_id, either by selecting a name from a dropdown list or by writing in a name when the name was not found in the dropdown list. The hand-collection was performed by RAs found through Fiverr and generally only one RA was used at a time.\nGoogle Sheets were used in two ways for this activity. First, a sheet was created with the list of filings to be processed and the URLs for the RA to click to complete the task. Before clicking on a link, the RA would select her name from the dropdown list in the spreadsheet, then click the link to the data entry site.40 Once the director photos were mapped to names, the RA would return to the Google Sheets document to flag the status of the row as “DONE” before moving onto the next row.\nSecond, the output of the RAs data collection activity was stored in the another sheet of the Google Sheets document. In effect, Google Sheets served as a simple alternative to posting the data in some kind of database.41\n\n\n5.3.2 Classifying director photos\nThe next phase was to present each photo to qualified Mechanical Turkers who would classify each photo based on sex and ethnicity. The results from this task were returned by Mechanical Turk as CSV files. These CSV files were saved to a GitHub respository as is and were never modified in any way.\nEach photo was classified by two Mechanical Turkers. When there was disagreement between the Mechanical Turkers, someone from the research team (either me or someone working at Stanford) would make a final classification in a Google Sheets document (hand_classification), using information from various database to do so. Hyperlinks to the sources used as well as notes on the reasons for each classificaton were stored in the Google Sheets document. In addition to the resolving disagreements between Mechanical Turkers, the hand_classification sheet was used to address potential issues flagged by inconsistencies such as that between a classification of a director in our data and that in other databases.\n\n\n5.3.3 Creating the final ethnicities data file\nThe final step was to combine the data above into a final classification of all directors in our sample. As we have seen before, we needed to prioritize the confirmed matches in hand_classification over the matches in the Mechanical Turk data and this was achieved with code that used an anti-join. The final data set mapped each director (executive_id) to an ethniciy and sex."
  },
  {
    "objectID": "published/spreadsheets.html#director-biographies",
    "href": "published/spreadsheets.html#director-biographies",
    "title": "Data collection (with spreadsheets)",
    "section": "5.4 Director biographies",
    "text": "5.4 Director biographies\nGoogle Sheets were used extensively to support data collection for Gow et al. (2018). Again the data collection process proceeded in steps.42 The first step was the collection of the relevant SEC filing for our sample. The second step involved tagging of the biographies for each director.43 The third step required the tagging of directorships within biographies.\nNote that none of the tagged data was stored in Google Sheets, as the web-based tagging tool directly populated tables in a PostgreSQL database. Instead the Google Sheets provided a flexible system for assigning filings to RAs and to allow RAs to flag issues with the tagging tool that needed resolution by adjustments to the tagging tool. Often the Google Sheets used to this end were populated by data created using SQL queries against a database."
  },
  {
    "objectID": "published/spreadsheets.html#matching-identifiers",
    "href": "published/spreadsheets.html#matching-identifiers",
    "title": "Data collection (with spreadsheets)",
    "section": "5.5 Matching identifiers",
    "text": "5.5 Matching identifiers\nWhile the idea of matching identifiers studied in Section 3 might seem fairly arcane, this is actually one of the most common use cases for data collection with Google Sheets. Whether it is matching CIKs with GVKEYs or matching StreetEvents conference call IDs with PERMNOs, Google Sheets implemented with the kinds of processes and best practices described above has been invaluable in my research efforts.44"
  },
  {
    "objectID": "published/spreadsheets.html#footnotes",
    "href": "published/spreadsheets.html#footnotes",
    "title": "Data collection (with spreadsheets)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMy aim with these notes is not to replace existing resources such as Wickham et al. (2023), but to build on them with applied examples using richer data sets.↩︎\nEmpirical Research in Accounting: Tools and Methods was published in print form by CRC Press in 2024 and remains free online at https://iangow.github.io/far_book.↩︎\nThis is not intended to be a criticism of Wickham et al. (2023). One needs to draw the line for a book somewhere. Also, based on my experience in writing this note, writing about data collection is surprisingly challenging.↩︎\nI capitalize Import and put it in italics to indicate a reference to Figure 1.↩︎\nIn Figure 1, I actually propose an extension of Figure 1-1 of Wickham et al. (2023) that includes a Collect process.↩︎\nSee issues #3 and #5 there. Hopefully, this note addresses those issues.↩︎\nMy aim with these notes is not to replace existing resources such as R for Data Science, but to build on them with applied examples using richer data sets.↩︎\nEmpirical Research in Accounting: Tools and Methods was published in print form by CRC Press in 2024 and remains free online at https://iangow.github.io/far_book.↩︎\nGuidance on downloading the Form AP data is provided in another earlier note.↩︎\nExecute install.packages(c(\"tidyverse\", \"DBI\", \"duckdb\", \"farr\", \"googlesheets4\", \"arrow\", \"dbplyr\") within R to install all the packages you need to run the code in this note.↩︎\nIn general, there are much better formats for exporting data than either of these approaches and I see no good reason to use Excel files as part of a data science workflow.↩︎\nTo be fair, earlier chapters of Wickham et al. (2023) do discuss issues regarding importing data and scripts, but I think the importance of reproducibility is high enough—and the tendency for many to neglect it in practice—that it really bears repetition and more detailed guidance in each context.↩︎\nFigure 1 is a slightly expected version of the “whole game” I discussed in an earlier note on data curation—itself an extension of the whole game depicted in Wickham et al. (2023, p. xii).↩︎\n“Performed better” here means not only that the scripts worked faster, but that the imported data were easier to work with.↩︎\nAdditional examples of what can go wrong if one opens a structured text data file in Excel and saves it are provided in my earlier note.↩︎\nThe original “whole game” in Wickham et al. (2023) does not even discuss Collect, but I think Broman and Woo (2018) shows how it is an essential part of the data science process.↩︎\nI believe it is possible to lock selected cells in a spreadsheet, but I suspect that few users avail themselves of this functionality.↩︎\nIf there is an alternative available today—or if such a product becomes available at a later date—hopefully much of what I say here will apply to that product.↩︎\nThat is, of course, you don’t have dimwitted people coming from a Microsoft Office background replicating their approach to version control by creating copies of Google Sheets documents!↩︎\nThat said, while Google Docs is much better than Microsoft Word for collaborative editing, I would not use it for this purpose myself because there are better ways if you have co-authors not trapped in the late 20th century.↩︎\nThis is not a purely hypothetical scenario!↩︎\nThese RAs were members of a pool and presumably these RAs found work, if they wanted it, on a new task for a different faculty member.↩︎\nSome firms have SEC filings long before their IPOs, so this is not a perfect approach, but it is good enough for our purposes here.↩︎\nIn some sense, I really talking about foreign keys here, but it is probably more important to be precise in thinking than it is to be precise in terminology.↩︎\nIt is easy to confirm that there is only one firm_country value associated with each firm_id, but I leave this as an exercise for the reader.↩︎\nFor one, the new name is more descriptive of how I will use the data it contains. Additionally, if I rerun the write_sheet() code above, it will not overwrite the sheet that I already have. Note that Google Sheets has excellent support for version history that makes it possible to restore an earlier version if you accidentally overwrite hand-collected data.↩︎\nNote that I later deleted entire rows for such clearly bad matches. While a useful maxim for data in 2025 might be never delete data (especially when the incremental storage cost is essentially zero), I view the candidate rows in top_matches as merely tentative data suggestions rather than “real” data.↩︎\nThis data collection was joint work by me and Stephen Walker.↩︎\nNote that further research was required to determine that the match of “Marcum Asia CPAs LLP” with “Friedman LLP” is valid; there was a name change that occurred in 2022.↩︎\nI can give effect to this by setting valid to FALSE for the other rows in name_matches (e.g., for matches to firm_id values of 1147 and 1208 for “Deloitte & Touche LLP”) and to TRUE only for the main match (e.g., for firm_id of 34 for “Deloitte & Touche LLP”).↩︎\nWhile this might appear to violate the “No Calculations in the Raw Data Files” dictum of Broman and Woo (2018), it reflects the different way that I am using the spreadsheet format. In effect, the Google Sheets file is a kind of notebook with supporting information for the data that will be used in analysis.↩︎\nFor example, the expertName for dealID of “749074-74508” is Deloitte & Touche LLP. But inspection of the associated SEC filings reveals that the audit report in the prospectus was signed off by the Canadian firm.↩︎\nIn PostgreSQL, lists are called arrays and we would use array_agg() to achieve the same result in PostgresSQL. Note that we could have used the alias array_agg() in place of list() in DuckDB.↩︎\nThe reason for specifying ORDER BY firm_id is to ensure that the values in firm_ids correspond with the respective values in firm_names, etc.↩︎\nIt was to facilitate this kind of observation that I included CompanyAddress when constructing no_form_ap_matches in the first place.↩︎\no be fair, the topic of version control of data is a knotty one that probably warrants serious discussion, but this note is not the place for it.↩︎\nOf course if you were transported back to 1995 with nothing but a single data file, you might prefer CSV to parquet, but it seems exceedingly unlikely that you will be in a situation where you cannot work with a parquet file in 2025.↩︎\nFor example, the goal might be to identify the auditor at the time of the IPO, even in cases where a change in auditor is in process at that time.↩︎\nCode related to this is made available on GitHub.↩︎\nAt the time of writing, you can see a page from this site at https://go.unimelb.edu.au/hiq8.↩︎\nThis approach was set up by Mason Jiang, a data scientist at Stanford.↩︎\nSee Appendix 1 of Gow et al. (2018, pp. 466–467) for more details on the data collection process.↩︎\nThe tagging tool was created by Andrew Marder, then a data scientist at Harvard Business School.↩︎\nTo learn about GVKEYs and PERMNOs, see Chapter 7 of Gow and Ding (2024b).↩︎"
  },
  {
    "objectID": "published/sirca_eod.html",
    "href": "published/sirca_eod.html",
    "title": "SIRCA ASX End of Day (EOD) collection",
    "section": "",
    "text": "SIRCA’s ASX EOD (end of day) collection provides daily prices for ASX-listed companies and facilitates reliable measurement of security returns, with all data being retained for delisted companies. This note, based on SIRCA’s own Guide to ASX End of Day Prices.pdf, provides an introduction to the SIRCA ASX EOD collection.\nThe SIRCA ASX EOD collection includes the tables listed in Table 1. More details can be found in my separate document on importing SIRCA ASX EOF data here.\n\n\n\nTable 1: SIRCA ASX EOD price collections\n\n\n\n\n\n\n\n\n\n\nTable\nDescription\nPrimary key\n\n\n\n\nsi_au_ref_names\nName and ticker histories for listed companies from January 2000\ngcode, securityticker, listdate\n\n\nsi_au_prc_daily\nComplete daily price, volume and value histories\ngcode, date, seniorsecurity\n\n\nsi_au_retn_mkt\nDaily value- and equal-weighted whole market returns\ndate\n\n\nsi_au_ref_trddays\nRecord of ASX trading dates since January 2000\ndate\n\n\n\n\n\n\nAll company names and ticker codes are recorded in a separate table—si_au_ref_names—which links names and tickers through time with a permanent “group code” identifier created by SIRCA named gcode. SIRCA designed gcode to allow users to build price and return series for series for a company’s shares over time even if its ticker code changes. The same gcodes are used across a number of SIRCA’s data sets.\nhe si_au_prc_daily table includes all end-of-day trade prices for the equity securities of all ASX companies starting from January 2000. This table also includes dividend and franking events; capital returns; adjustments for numerous corporate action events, such as splits, consolidation, bonus issues, renounceable and non-renounceable issues; and total daily traded volume and value; and, the number of issued shares.\nOther components of the SIRCA ASX EOD library are si_au_retn_mkt which provides value- and equal-weighted all-of-market daily returns, which SIRCA generates from all observable daily company returns.\nSIRCA provides si_au_ref_trdday, which identifies all ASX trading days since the start of January 2000 and can be used to identify gaps in company price series, from suspensions or thin trading.\nFinally SIRCA provides a detailed description of all tables and fields in a data dictionary.\nWhile this note is based on SIRCA’s own Guide to ASX End of Day Prices.pdf, it goes beyond that guide in a number of respects. First, I provide detailed instructions on preparing the SIRCA ASX EOD data for use and illustrate analysis using DuckDB and parquet files, a high-performance state-of-the-art approach to data analysis. The SIRCA guide assumes that the user has access to an SQL database containing the four tables listed in Table 1, but provides no guidance on creating that database.\nSecond, I provide output—both tables and graphs—for the example queries provided here. While the SIRCA guide describes observed patterns, it does not provide the output from its queries.\nThird, I expand on or refine the queries used in the SIRCA guide. For example, where the SIRCA guide suggests the use of dayssince variables to identify non-trading days, I propose a more robust approach.\nThe code in this note uses the packages listed below, plus the duckdb package.1 This note was written using Quarto and compiled with RStudio, an integrated development environment (IDE) for working with R. The source code for this note is available here and the latest PDF version is available here.\n\nlibrary(tidyverse)\nlibrary(DBI)\nlibrary(dbplyr, warn.conflicts = FALSE)\nlibrary(farr)"
  },
  {
    "objectID": "published/sirca_eod.html#finding-gcodes-from-company-names-or-ticker-codes",
    "href": "published/sirca_eod.html#finding-gcodes-from-company-names-or-ticker-codes",
    "title": "SIRCA ASX End of Day (EOD) collection",
    "section": "1. Finding gcodes from company names or ticker codes",
    "text": "1. Finding gcodes from company names or ticker codes\nOne way of searching for a gcode is to look up the company name. For example, we could search for every gcode with a company name including WESTPAC. Because we specified compute(name = \"si_au_ref_names\"), we can refer to that table in SQL like the following:\n\nSELECT gcode, seniorsecurity, securityticker, abbrevcompanyname\nFROM si_au_ref_names\nWHERE fullcompanyname LIKE '%WESTPAC%'\n\n\n\nTable 4: Securities matching WESTPAC: SQL\n\n\n\n\n\n\ngcode\nseniorsecurity\nsecurityticker\nabbrevcompanyname\n\n\n\n\nwbc1\n1\nWBC\nWESTPAC BANKING CORP\n\n\nwot1\n1\nWOTCA\nWESTPAC OFFICE TRUST\n\n\nwot1\n1\nWOT\nWESTPAC OFFICE TRUST\n\n\nwpt1\n1\nWPT\nWESTPAC PROP. TRUST\n\n\n\n\n\n\n\n\nHowever, the same query could be run using tidyverse code:\n\nsi_au_ref_names |&gt;\n  filter(str_like(fullcompanyname, '%WESTPAC%')) |&gt;\n  select(gcode, seniorsecurity, securityticker, abbrevcompanyname) |&gt;\n  collect()\n\n\n\nTable 5: Securities matching WESTPAC: Tidyverse\n\n\n\n\n\n\ngcode\nseniorsecurity\nsecurityticker\nabbrevcompanyname\n\n\n\n\nwbc1\n1\nWBC\nWESTPAC BANKING CORP\n\n\nwot1\n1\nWOTCA\nWESTPAC OFFICE TRUST\n\n\nwot1\n1\nWOT\nWESTPAC OFFICE TRUST\n\n\nwpt1\n1\nWPT\nWESTPAC PROP. TRUST\n\n\n\n\n\n\n\n\nBehind the scenes, the tidyverse package is translating our code into SQL:\n\nsi_au_ref_names |&gt;\n  filter(str_like(fullcompanyname, '%WESTPAC%')) |&gt;\n  select(gcode, seniorsecurity, securityticker, abbrevcompanyname) |&gt;\n  show_query()\n\n&lt;SQL&gt;\nSELECT gcode, seniorsecurity, securityticker, abbrevcompanyname\nFROM si_au_ref_names\nWHERE (fullcompanyname LIKE '%WESTPAC%')\n\n\nAlternatively, one can also search by ticker code, as seen in Table 6 using the securityticker of ANZ.\n\nSELECT gcode, securityticker, abbrevcompanyname\nFROM si_au_ref_names \nWHERE securityticker = 'ANZ'\n\n\n\nTable 6: Securities with ticker ANZ: SQL\n\n\n\n\n\n\ngcode\nsecurityticker\nabbrevcompanyname\n\n\n\n\nanz1\nANZ\nAUSTRALIA AND NZ\n\n\nanz1\nANZ\nANZ GROUP HOLDINGS\n\n\n\n\n\n\n\n\nAgain the same query could be run using tidyverse code, with results show in Table 7. Because it is so straightforward to run SQL queries using R (tidyverse) code, we will just provide R code going forward.4 Note that using R code greatly facilitates bringing the data into R for analysis or (as we will do here) data visualization.\n\nsi_au_ref_names |&gt;\n  filter(securityticker == 'ANZ') |&gt;\n  select(gcode, seniorsecurity, securityticker, abbrevcompanyname) |&gt;\n  collect()\n\n\n\nTable 7: Securities with ticker ANZ: Tidyverse\n\n\n\n\n\n\ngcode\nseniorsecurity\nsecurityticker\nabbrevcompanyname\n\n\n\n\nanz1\n1\nANZ\nAUSTRALIA AND NZ\n\n\nanz1\n1\nANZ\nANZ GROUP HOLDINGS\n\n\n\n\n\n\n\n\nAs another example, suppose one is interested in Arena REIT which has a companyticker of ARF. Searching for this ticker code reveals the gcode of Arena REIT is arf2. This gcode can then be used to search the si_au_prc_daily table for information about the securities of Arena REIT. Results of this search are seen in Table 8.\n\nsi_au_ref_names |&gt;\n  filter(companyticker == 'ARF', seniorsecurity == 1L) |&gt;\n  select(gcode, securityticker, abbrevcompanyname, \n         listdate, delistdate) |&gt;\n  arrange(listdate) |&gt;\n  collect()\n\n\n\nTable 8: Securities with ticker ARF\n\n\n\n\n\n\ngcode\nsecurityticker\nabbrevcompanyname\nlistdate\ndelistdate\n\n\n\n\narf1\nARF\nARROWFIELD GROUP\n1987-11-12\n2000-09-20\n\n\narf2\nARF\nARENA REIT\n2013-06-13\n2013-12-10\n\n\narf2\nARFDA\nARENA REIT\n2013-12-11\n2013-12-19\n\n\narf2\nARF\nARENA GROUP\n2013-12-20\n2014-01-19\n\n\narf2\nARF\nARENA REIT\n2014-01-20\n2014-12-08\n\n\narf2\nARFDC\nARENA REIT\n2014-12-09\n2014-12-15\n\n\narf2\nARF\nARENA REIT\n2014-12-16\nNA\n\n\n\n\n\n\n\n\nThe previous search reveals that in 2000, the ticker code ARF was then associated with Arrowfield Group Limited. Arrowfield Group Limited is a different entity to Arena REIT, which was listed in 2013, so the two entities have separate gcodes.\nA search for companyticker of ARF also shows that in 2013, the securityticker of Arena REIT briefly changed from ARF to ARFDA, and then back to ARF, due to conversions to and from deferred units. If the holders of units in Arena REIT in 2013 participated in these conversions, then it makes sense to consider them as a single security, which is possible if we accumulate returns using the gcode of arf2, which remains unchanged throughout this period."
  },
  {
    "objectID": "published/sirca_eod.html#adjusting-for-the-effects-of-corporate-actions",
    "href": "published/sirca_eod.html#adjusting-for-the-effects-of-corporate-actions",
    "title": "SIRCA ASX End of Day (EOD) collection",
    "section": "2. Adjusting for the effects of corporate actions",
    "text": "2. Adjusting for the effects of corporate actions\nFigure 1 shows a large drop in the closing price of BHP in late June 2001.\n\nsi_au_prc_daily |&gt;\n  filter(gcode == 'bhp1', seniorsecurity == 1L,\n         between(date, '2001-01-01', '2001-12-31')) |&gt;\n  ggplot(aes(x = date, y = close)) +\n  geom_line()\n\n\n\n\n\n\n\nFigure 1: BHP stock price during 2001\n\n\n\n\n\nExamining the coraxdescription column, it seems likely that the change is due to a 1:0.94 bonus issue.\n\nsi_au_prc_daily |&gt;\n  filter(gcode == 'bhp1', seniorsecurity == 1L,\n         between(date, '2001-01-01', '2001-12-31')) |&gt;\n  filter(!is.na(coraxdescription)) |&gt;\n  select(gcode, date, close, coraxdescription) |&gt;\n  collect()\n\n\n\nTable 9: Corporate action events for BHP in 2001\n\n\n\n\n\n\ngcode\ndate\nclose\ncoraxdescription\n\n\n\n\nbhp1\n2001-06-29\n10.39\n1:0.94 bonus issue\n\n\n\n\n\n\n\n\nThe coraxdescription column provides details of corporate action (CORAX) events, when available. The numberofdilutionevents field will always show a value greater than 0 when a dilution event (CORAX or dividend) has occurred and numberofcoraxevents &gt; 0 indicates CORAX events, even if coraxdescription is not available. Likewise, numberofdividendevents &gt; 0 can be used to find all dividend events, even when data are not available in other descriptive fields .\n\nsi_au_prc_daily |&gt;\n  filter(gcode == 'bhp1', seniorsecurity == 1L,\n          between(date, '2001-06-27', '2001-07-02')) |&gt;\n  select(gcode, date, close, factor, numberofcoraxevents) |&gt;\n  collect()\n\n\n\nTable 10: Values of factor for BHP around 27 June 2001\n\n\n\n\n\n\ngcode\ndate\nclose\nfactor\nnumberofcoraxevents\n\n\n\n\nbhp1\n2001-06-27\n20.946\n1.000\n0\n\n\nbhp1\n2001-06-28\n21.420\n1.000\n0\n\n\nbhp1\n2001-06-29\n10.390\n2.065\n1\n\n\nbhp1\n2001-07-02\n10.480\n1.000\n0\n\n\n\n\n\n\n\n\nHow is factor calculated? As seen in Table 10, on most dates factor will be 1, but when a CORAX or dividend event occurs, factor will reflect the factor that allows the previous day’s price to be compared with the current one. With a 1:0.94 bonus issue, if I have \\(0.94\\) shares one day, I will have \\(1.94\\) shares the next, so factor equals \\(0.94 / 1.94 = 0.485\\). In other words, the share price of \\(21.420\\) on 28 June 2001 is equivalent to a share price of \\(21.420 \\times 0.485 = 10.379\\) on 29 June 2001.\nThe variable cumulativefactor is calculated from factor to facilitate adjustment of prices along the whole time series. In creating cum_factor_calcs, I replicate the calculation of cumulativefactor from factor. Note that the calculation of cumulativefactor moves from the end of the price series (implied by window_order(desc(date))) for each security (implied by group_by(gcode, seniorsecurity)) and accumulates the absolute value of factor in a multiplicative fashion.5 The calculation uses the lag() function because the first date we want to apply factor for 29 June 2001 to prices is the “next” date (in the reverse-ordered price series) or 28 June 2001.6\n\ncum_factor_calcs &lt;-\n  si_au_prc_daily |&gt;\n  group_by(gcode, seniorsecurity) |&gt;\n  window_order(desc(date)) |&gt;\n  mutate(cum_factor_calc = exp(cumsum(log(abs(factor))))) |&gt;\n  mutate(cum_factor_calc = lag(cum_factor_calc) * sign(lag(factor))) |&gt;\n  window_order() |&gt;\n  ungroup() |&gt;\n  select(gcode, seniorsecurity, date, close, factor, \n         cumulativefactor, cum_factor_calc)\n\nTable 11 presents cumulativefactor, as calculated by SIRCA, and cum_factor_calc, where I replicate the calculation of cumulativefactor from factor.\n\ncum_factor_calcs |&gt;\n  filter(gcode == 'bhp1', seniorsecurity == 1L,\n          between(date, '2001-06-27', '2001-07-02')) |&gt;\n  select(gcode, date, close, factor, cumulativefactor, cum_factor_calc) |&gt;\n  arrange(date) |&gt;\n  collect()\n\n\n\nTable 11: Calculated values of factor for BHP around 27 June 2001\n\n\n\n\n\n\ngcode\ndate\nclose\nfactor\ncumulativefactor\ncum_factor_calc\n\n\n\n\nbhp1\n2001-06-27\n20.946\n1.000\n1.053\n6.913\n\n\nbhp1\n2001-06-28\n21.420\n1.000\n1.053\n6.913\n\n\nbhp1\n2001-06-29\n10.390\n2.065\n2.175\n3.348\n\n\nbhp1\n2001-07-02\n10.480\n1.000\n2.175\n3.348\n\n\n\n\n\n\n\n\nThe cumulativefactor column can be used to adjust the closing price for the effects of corporate actions, such stock splits or entitlement offers, and dividends. Simply multiplying the close column by the cumulativefactor column will produce the adjusted price.\n\n si_au_prc_daily |&gt;\n  filter(gcode == 'bhp1', seniorsecurity == 1L,\n         between(date, '2001-01-01', '2001-12-31')) |&gt;\n  mutate(adjustedprice = close * cumulativefactor) |&gt;\n  pivot_longer(c(adjustedprice, close), \n               names_to = \"variable\", values_to = \"price\") |&gt;\n  ggplot(aes(x = date, y = price, color = variable)) +\n  geom_line()\n\n\n\n\n\n\n\nFigure 2: BHP adjusted stock price during 2001\n\n\n\n\n\nIn Figure 2, adjustedprice series is everywhere lower than close because cumulativefactor adjusts for all subsequent CORAX events and these tend to cause adjusted prices to be lower as one moves back through time (e.g., bonus issues or dividends). The important thing is that the resulting adjustedprice series is consistent over its entire history and can be used to reliably measure returns for bhp1 between any two trading dates.\nExactly the same process for cumulativefactor applies for dividends as well as corporate actions. AAA (gcode: aaa2) is an exchange-traded fund that deposits money in accounts with Australian banks and pays regular dividends. The effect of its dividends on its closing price can be observed in Figure 3.\n\nsi_au_prc_daily |&gt;\n  filter(gcode == 'aaa2', seniorsecurity == 1L,\n         between(date, '2017-01-01', '2018-12-31')) |&gt;\n  mutate(adjustedprice = close * cumulativefactor) |&gt;\n  select(date, close, adjustedprice) |&gt;\n  pivot_longer(-date, names_to = \"variable\", values_to = \"price\") |&gt;\n  ggplot(aes(x = date, y = price, color = variable)) +\n  geom_line()\n\n\n\n\n\n\n\nFigure 3: Adjusted and unadjusted closing prices for AAA"
  },
  {
    "objectID": "published/sirca_eod.html#plotting-a-distribution-of-price-relatives-for-a-security",
    "href": "published/sirca_eod.html#plotting-a-distribution-of-price-relatives-for-a-security",
    "title": "SIRCA ASX End of Day (EOD) collection",
    "section": "3. Plotting a distribution of price relatives for a security",
    "text": "3. Plotting a distribution of price relatives for a security\nThe following code calculates prel, the price relative or gross shareholder return, for securities on si_au_prc_daily. By using cumulativefactor, it adjusts for corporate actions and dividends.\n\nprels &lt;-\n  si_au_prc_daily |&gt;\n  mutate(adjustedprice = close * cumulativefactor) |&gt;\n  group_by(gcode, seniorsecurity) |&gt;\n  window_order(date) |&gt;\n  mutate(prel = adjustedprice / lag(adjustedprice)) |&gt;\n  ungroup() |&gt;\n  window_order()\n\nFigure 4 shows the distribution of returns from Commonwealth Bank.\n\nprels |&gt;\n  filter(!is.na(prel)) |&gt;\n  filter(gcode == 'cba1', seniorsecurity == 1L) |&gt;\n  ggplot(aes(x = prel)) +\n  geom_histogram(binwidth = 0.005)\n\n\n\n\n\n\n\nFigure 4: Distribution of daily returns for Commonwealth Bank"
  },
  {
    "objectID": "published/sirca_eod.html#dayssince-column",
    "href": "published/sirca_eod.html#dayssince-column",
    "title": "SIRCA ASX End of Day (EOD) collection",
    "section": "4. dayssince column",
    "text": "4. dayssince column\nIt is important to note that price relatives calculated in the previous section may not always relate to consecutive trading days. The following code calculates the number of days between consecutive observations for a given security on si_au_prc_daily.\n\nelapsed_days &lt;-\n  si_au_prc_daily |&gt;\n  group_by(gcode, seniorsecurity) |&gt;\n  window_order(date) |&gt;\n  mutate(days_elapsed = dayssince - lag(dayssince)) |&gt;\n  ungroup() |&gt;\n  window_order()\n\nFigure 5 shows the distribution of days_elapsed, the number of elapsed days between trading dates calculated using using the datesince column, for Commonwealth Bank. Although CBA is a stock that is consistently traded, a less-liquid security may show large gaps in trading activity, leading to price relatives that span longer time periods.\n\nelapsed_days |&gt;\n  filter(gcode == 'cba1', seniorsecurity == 1L) |&gt;\n  count(days_elapsed, sort = TRUE) |&gt;\n  filter(!is.na(days_elapsed)) |&gt;\n  ggplot(aes(x = days_elapsed, y = n)) +\n  geom_col() +\n  scale_x_continuous(breaks = 1:5)\n\n\n\n\n\n\n\nFigure 5: Distribution of days between trading dates for Commonwealth Bank\n\n\n\n\n\nAs a measure of the liquidity of a security, days_elapsed is problematic because it does not distinguish between days on which the market is open and those on which it is closed. We can improve on this measure using data from si_au_ref_trddays, a sample of which is shown in Table 12.\n\nsi_au_ref_trddays |&gt; collect(n = 10)\n\n\n\nTable 12: Sample rows from si_au_ref_trddays\n\n\n\n\n\n\ndate\ndayssince\nweekday\ncount\n\n\n\n\n2000-01-04\n36529\n3\n936\n\n\n2000-01-05\n36530\n4\n958\n\n\n2000-01-06\n36531\n5\n949\n\n\n2000-01-07\n36532\n6\n927\n\n\n2000-01-10\n36535\n2\n961\n\n\n2000-01-11\n36536\n3\n984\n\n\n2000-01-12\n36537\n4\n968\n\n\n2000-01-13\n36538\n5\n971\n\n\n2000-01-14\n36539\n6\n966\n\n\n2000-01-17\n36542\n2\n976\n\n\n\n\n\n\n\n\nUsing an approach described in more detail here, in place of dayssince, we can create a variable td to represent the “trading date” for each date on si_au_ref_trddays where td equals 1 on the first trading date, 2 the second trading date, and so on.\n\ntrading_days &lt;- \n  si_au_ref_trddays |&gt;\n  window_order(date) |&gt;\n  mutate(td = row_number()) |&gt;\n  distinct(date, td) |&gt;\n  arrange(date) |&gt;\n  compute()\n\nWith trading_days in hand, we can calculate tdays_elapsed as the number of trading dates between the current date and the previous date on si_au_prc_daily for each security and date.\n\ntdays_elapsed_df &lt;-\n  si_au_prc_daily |&gt;\n  inner_join(trading_days, by = \"date\") |&gt;\n  group_by(gcode, seniorsecurity) |&gt;\n  window_order(date) |&gt;\n  mutate(tdays_elapsed = td - lag(td),\n         lag_date = lag(date)) |&gt;\n  select(gcode, seniorsecurity, date, lag_date, tdays_elapsed) |&gt;\n  ungroup() |&gt;\n  window_order()\n\nTable 13 provides data on our improved measure of trading days between trading dates for both Commonwealth Bank (cba1) and a less liquid security (1st1). In Table 13, it can be seen that there are very few cases in which the trading days between dates is more than one for cba1, but quite a few such cases for 1st1.\n\ntdays_elapsed_df |&gt;\n  filter(gcode %in% c('cba1', '1st1'), seniorsecurity == 1L,\n         !is.na(tdays_elapsed)) |&gt;\n  count(gcode, tdays_elapsed) |&gt;\n  pivot_wider(names_from = \"gcode\", values_from = \"n\", values_fill = 0) |&gt;\n  arrange(tdays_elapsed) |&gt;\n  collect()\n\n\n\nTable 13: Trading days between trading dates: cba1 and 1st1\n\n\n\n\n\n\ntdays_elapsed\n1st1\ncba1\n\n\n\n\n1\n1298\n6314\n\n\n2\n176\n2\n\n\n3\n79\n1\n\n\n4\n33\n1\n\n\n5\n21\n0\n\n\n6\n6\n0\n\n\n7\n4\n0\n\n\n8\n6\n0\n\n\n9\n2\n0\n\n\n10\n1\n0\n\n\n11\n2\n0\n\n\n134\n1\n0\n\n\n\n\n\n\n\n\nTable 14 provides additional information on the apparent gaps in trading for Commonwealth Bank. We can use these data to investigate the cause of these gaps. Looking at the longest gap, it turns out there was a trading halt placed on 12 August 2015.\n\ntdays_elapsed_df |&gt;\n  filter(gcode == 'cba1', seniorsecurity == 1L, tdays_elapsed &gt; 1) |&gt;\n  collect()\n\n\n\nTable 14: Gaps in trading for Commonwealth Bank\n\n\n\n\n\n\ngcode\nseniorsecurity\ndate\nlag_date\ntdays_elapsed\n\n\n\n\ncba1\n1\n2008-10-09\n2008-10-07\n2\n\n\ncba1\n1\n2015-08-17\n2015-08-11\n4\n\n\ncba1\n1\n2015-09-14\n2015-09-10\n2\n\n\ncba1\n1\n2000-03-10\n2000-03-07\n3\n\n\n\n\n\n\n\n\nA natural question might be whether there are dates on si_au_prc_daily not found on si_au_ref_trddays. Table 15 shows that there are, but a small number of securities (in most cases, just one) have data on si_au_prc_daily on those days. I leave it as an exercise for the reader to understand what’s going on in these cases.\n\nsi_au_prc_daily |&gt;\n  distinct(date) |&gt;\n  anti_join(si_au_ref_trddays, by = \"date\") |&gt;\n  inner_join(si_au_prc_daily, by = \"date\") |&gt;\n  count(date) |&gt;\n  mutate(wday = wday(date, label = TRUE)) |&gt;\n  arrange(desc(n)) |&gt;\n  collect(n = 10)\n\n\n\nTable 15: Observations on si_au_prc_daily on non-trading days\n\n\n\n\n\n\ndate\nn\nwday\n\n\n\n\n2014-01-01\n3\nWed\n\n\n2000-01-01\n3\nSat\n\n\n2014-06-01\n2\nSun\n\n\n2009-01-01\n2\nThu\n\n\n2014-11-01\n1\nSat\n\n\n2014-03-15\n1\nSat\n\n\n2011-09-10\n1\nSat\n\n\n2015-01-01\n1\nThu\n\n\n2009-06-08\n1\nMon\n\n\n2011-12-17\n1\nSat"
  },
  {
    "objectID": "published/sirca_eod.html#using-the-seniorsecurity-column",
    "href": "published/sirca_eod.html#using-the-seniorsecurity-column",
    "title": "SIRCA ASX End of Day (EOD) collection",
    "section": "5. Using the seniorsecurity column",
    "text": "5. Using the seniorsecurity column\nAt times, some gcodes have multiple securities trading simultaneously and SIRCA provides the seniorsecurity field to distinguish different securities for a given firm. In Table 16, two classes of security are shown to be simultaneously trading for Telstra Corporation Ltd, whose gcode is tls1. These are evident from the different securityticker values: TLS and TLSCA.\n\nsi_au_prc_daily |&gt;\n  filter(gcode == 'tls1',\n         between(date, \"2008-05-01\", \"2008-05-07\")) |&gt;\n  select(gcode, seniorsecurity, date, securityticker) |&gt;\n  arrange(date) |&gt;\n  collect()\n\n\n\nTable 16: Sample of securityticker values for Telstra (tls1)\n\n\n\n\n\n\ngcode\nseniorsecurity\ndate\nsecurityticker\n\n\n\n\ntls1\n0\n2008-05-01\nTLSCA\n\n\ntls1\n1\n2008-05-01\nTLS\n\n\ntls1\n0\n2008-05-02\nTLSCA\n\n\ntls1\n1\n2008-05-02\nTLS\n\n\ntls1\n0\n2008-05-05\nTLSCA\n\n\ntls1\n1\n2008-05-05\nTLS\n\n\ntls1\n0\n2008-05-06\nTLSCA\n\n\ntls1\n1\n2008-05-06\nTLS\n\n\ntls1\n0\n2008-05-07\nTLSCA\n\n\ntls1\n1\n2008-05-07\nTLS"
  },
  {
    "objectID": "published/sirca_eod.html#sec-neg-fct",
    "href": "published/sirca_eod.html#sec-neg-fct",
    "title": "SIRCA ASX End of Day (EOD) collection",
    "section": "6. Negative factor values and zero volumeonmkt values",
    "text": "6. Negative factor values and zero volumeonmkt values\nWhen there is either no observed trade price before an event or no price after the event, a factor of -1 is assigned to that event. This can occur both in the beginning and the end of the lifetime of the security. Table 17 shows that relatively few observations have negative factor values.\n\nsi_au_prc_daily |&gt;\n  mutate(neg_factor = factor &lt; 0) |&gt;\n  count(neg_factor)\n\n\n\nTable 17: Number of observations by negative factors\n\n\n\n\n\n\nneg_factor\nn\n\n\n\n\nFALSE\n9050456\n\n\nTRUE\n234\n\n\n\n\n\n\n\n\nThe calculation of cumulativefactor when factor is negative seems to follow the calculation of cum_factor_calc provided above. That is the absolute value is accumulated and multiplied by the sign of the applicable factor value. Table 18 shows the alignment of cumulativefactor and cum_factor_calc calculated in this way for a stock with a negative value of factor.\n\ncum_factor_calcs |&gt;\n  filter(gcode == \"par1\", seniorsecurity == 1L,\n         between(date, \"2018-12-03\", \"2019-02-18\")) |&gt;\n  select(-seniorsecurity) |&gt;\n  arrange(date)\n\n\n\nTable 18: Calculating cumulativefactor with negative factor values\n\n\n\n\n\n\ngcode\ndate\nclose\nfactor\ncumulativefactor\ncum_factor_calc\n\n\n\n\npar1\n2018-12-11\n0.003\n1.000\n0.004\n1.308\n\n\npar1\n2018-12-12\n0.003\n1.000\n0.004\n1.308\n\n\npar1\n2018-12-14\n0.004\n1.000\n0.004\n-1.308\n\n\npar1\n2019-01-08\nNA\n-1.308\n-0.005\n1.000\n\n\npar1\n2019-01-24\n0.005\n1.000\n-0.005\n1.000\n\n\npar1\n2019-02-18\n0.009\n1.000\n-0.005\n1.000\n\n\n\n\n\n\n\n\nTable 19 shows that the calculation used to produce cum_factor_calc does not always match the valye in cumulativefactor. In this case, it seems that cumulativefactor is mysteriously “reset” to \\(1\\) on 27 September 2018. Further research would be needed to determine\n\ncum_factor_calcs |&gt;\n  filter(gcode == \"gcm2\", seniorsecurity == 1,\n         factor != 1) |&gt;\n  select(-seniorsecurity) |&gt;\n  arrange(date)\n\n\n\nTable 19: Case of mysterious factor values\n\n\n\n\n\n\ngcode\ndate\nclose\nfactor\ncumulativefactor\ncum_factor_calc\n\n\n\n\ngcm2\n2016-09-29\nNA\n1.010\n1.010\n1.155\n\n\ngcm2\n2016-12-29\n0.95\n1.010\n1.020\n1.143\n\n\ngcm2\n2017-03-30\nNA\n1.010\n1.031\n1.132\n\n\ngcm2\n2017-06-29\nNA\n1.012\n1.043\n1.119\n\n\ngcm2\n2017-09-28\nNA\n1.011\n1.054\n1.107\n\n\ngcm2\n2017-12-28\nNA\n1.009\n1.064\n1.096\n\n\ngcm2\n2018-03-28\nNA\n1.009\n1.074\n1.086\n\n\ngcm2\n2018-06-28\nNA\n1.018\n1.092\n1.068\n\n\ngcm2\n2018-09-27\nNA\n1.007\n1.101\n1.060\n\n\ngcm2\n2018-12-28\nNA\n1.015\n1.117\n1.044\n\n\ngcm2\n2019-03-28\nNA\n1.008\n1.126\n1.036\n\n\ngcm2\n2019-06-06\nNA\n1.023\n1.152\n1.013\n\n\ngcm2\n2019-06-27\n1.79\n1.007\n1.160\n1.006\n\n\ngcm2\n2019-09-27\nNA\n1.006\n1.166\n1.000\n\n\ngcm2\n2019-11-20\nNA\n-1.000\n0.000\nNA\n\n\n\n\n\n\n\n\nTable 20 flags other difficult-to-explain cumulativefactor values (excluding those where there are sign differences between cum_factor_calc and cumulativefactor). While further research would be needed to understand these, these are fortunately quite rare.\n\ncum_factor_calcs |&gt;\n  filter(gcode != \"gcm2\") |&gt;\n  filter(abs(abs(cumulativefactor) - abs(cum_factor_calc)) &gt; 0.001) |&gt;\n  distinct(gcode, cumulativefactor, cum_factor_calc) |&gt;\n  arrange(gcode, cumulativefactor) |&gt;\n  collect(n = 20)\n\n\n\nTable 20: A sample of other difficult-to-explain factor values\n\n\n\n\n\n\ngcode\ncumulativefactor\ncum_factor_calc\n\n\n\n\n14d1\n1.000\n1.027\n\n\n14d1\n1.021\n1.006\n\n\n14d1\n1.027\n1.000\n\n\n1ad1\n1.000\n1.030\n\n\n1ad1\n1.006\n1.024\n\n\n1ad1\n1.024\n1.006\n\n\n1ad1\n1.030\n1.000\n\n\n1al1\n1.000\n1.269\n\n\n1al1\n1.040\n1.221\n\n\n1al1\n1.066\n1.191\n\n\n1al1\n1.104\n1.150\n\n\n1al1\n1.134\n1.119\n\n\n1al1\n1.175\n1.081\n\n\n1al1\n1.199\n1.058\n\n\n1al1\n1.246\n1.019\n\n\n1al1\n1.269\n1.000\n\n\n1gov1\n1.000\n1.031\n\n\n1gov1\n1.002\n1.029\n\n\n1gov1\n1.004\n1.027\n\n\n1gov1\n1.006\n1.025\n\n\n\n\n\n\n\n\nThe example provided in Table 21 shows dividends between 2000-03-06 and 2001-09-28 without any trading. As no trading was observed prior to these dividend events, the factor and dividendfactor fields contain a value of -1. This makes sense, as one could not meaningfully push the sequence of stock returns back to dates before 2002-03-15, as there are no traded prices. There is a non-negative factor value for 2002-03-18, presumably because there are prices reported after 2002-03-18.\n\nsi_au_prc_daily |&gt; \n  filter(gcode == 'npx1', date &lt;= '2002-09-23', seniorsecurity == 1) |&gt;\n  select(gcode, date, close, dividend, factor, dividendfactor, volumeonmkt) |&gt;\n  collect()\n\n\n\nTable 21: Negative factor values: npx1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngcode\ndate\nclose\ndividend\nfactor\ndividendfactor\nvolumeonmkt\n\n\n\n\nnpx1\n2000-03-06\nNA\n0.065\n-1.000\n-1.000\n0\n\n\nnpx1\n2000-09-29\nNA\n0.054\n-1.000\n-1.000\n0\n\n\nnpx1\n2001-03-19\nNA\n0.068\n-1.000\n-1.000\n0\n\n\nnpx1\n2001-09-28\nNA\n0.059\n-1.000\n-1.000\n0\n\n\nnpx1\n2002-03-15\n3.34\nNA\n1.000\n1.000\n500\n\n\nnpx1\n2002-03-18\nNA\n0.067\n1.023\n1.023\n0\n\n\nnpx1\n2002-07-17\n2.95\nNA\n1.000\n1.000\n200\n\n\nnpx1\n2002-09-23\nNA\n0.078\n1.024\n1.024\n0\n\n\n\n\n\n\n\n\nTable 22 shows another example with a dividend on 2004-07-05. However, no price was observed after the event, and hence the factor and dividendfactor fields contain a value of -1. Note that there is a price in the close field on 2004-07-05 but it was not observed that day, after the dividend event. This is evident from the 0 value for VolumeOnMkt, and confirmed by NA or 0 values for open, high, low. This price is simply the previous observed trade price carried forward. This makes sense, as one could not meaningfully push the sequence of stock returns forward to dates after 2004-07-02, as there are no traded prices.\n\nsi_au_prc_daily |&gt; \n filter(gcode == 'wsf1', date &gt;= '2004-07-01', seniorsecurity == 1) |&gt;\n  select(gcode, date, close, dividend, factor, dividendfactor, volumeonmkt) |&gt;\n  collect()\n\n\n\nTable 22: Dividend example: npx1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ngcode\ndate\nclose\ndividend\nfactor\ndividendfactor\nvolumeonmkt\n\n\n\n\nwsf1\n2004-07-01\n15.48\nNA\n1\n1\n8968726\n\n\nwsf1\n2004-07-02\n15.60\nNA\n1\n1\n16453850\n\n\nwsf1\n2004-07-05\n15.60\n0.136\n-1\n-1\n0\n\n\n\n\n\n\n\n\n\nsi_au_prc_daily |&gt;\n  filter(adjustmentfactor &lt; 0) |&gt;\n  count(adjustmentfactor)\n\n\n\nTable 23: Frequency of negative adjustmentfactor values\n\n\n\n\n\n\nadjustmentfactor\nn\n\n\n\n\n-1.308\n1\n\n\n-0.025\n1\n\n\n-1.000\n69"
  },
  {
    "objectID": "published/sirca_eod.html#calculating-a-cumulative-factor-excluding-dividends",
    "href": "published/sirca_eod.html#calculating-a-cumulative-factor-excluding-dividends",
    "title": "SIRCA ASX End of Day (EOD) collection",
    "section": "7. Calculating a cumulative factor excluding dividends",
    "text": "7. Calculating a cumulative factor excluding dividends\nThe provided cumulativefactor field of si_au_prc_daily is calculated by cumulating the factor column, whcih adjusts for both corporate actions and dividends. The following example shows how to calculate an adjustment excluding dividends, It uses the adjustmentfactor field, which provides dilution factors for just the CORAX events (when followed at some time by a valid close price).\nUse the adjustmentfactor field, which does not account for dividends. Visualise the new adjustment and compare to the adjustment from the example in part 1 Note that the CorpAdjustedPrice, which is calculated without including dividends, looks identical to the close price as no corporate actions have occurred within this time frame.\nThe following shows the effect that dividends can have on the adjusted price series. The AdjustedPrice series incorporates both CORAX factors and dividend factors, whereas the CorpAdjustedPrice series incorporates only CORAX adjustments and ignores dividends. The CORAX-only price series shows a visible fall at the time when the dividend occurs as the value of the dividend is not accounted for.\n\nadj_rets &lt;-\n  si_au_prc_daily |&gt;\n  group_by(gcode, seniorsecurity) |&gt;\n  window_order(desc(date)) |&gt;\n  mutate(corporatefactor = exp(cumsum(log(abs(adjustmentfactor))))) |&gt;\n  mutate(corporatefactor = lag(corporatefactor) *\n           sign(lag(adjustmentfactor))) |&gt;\n  mutate(CorpAdjustedPrice = corporatefactor * close,\n         AdjustedPrice = close * cumulativefactor) |&gt;\n  window_order() |&gt;\n  ungroup()\n\nTo plot some date, we first construct anz_cum, which is a version of adj_rets focused on ANZ’s stock price.\n\nanz_cum &lt;-\n  adj_rets |&gt;\n  filter(gcode == \"anz1\", seniorsecurity == 1L) |&gt;\n  select(gcode, date, CorpAdjustedPrice, AdjustedPrice, close) |&gt;\n  compute()\n\nFigure 6 provides a plot of CorpAdjustedPrice (no adjustment for dividends) and AdjustedPrice (adjusted for dividends) for ANZ. One thing that makes this plot difficult to interpret is that the adjustment factors are calculated retrospectively. So going back in time the plots “start” at the same point and “end” at different prices.\n\nanz_cum |&gt;\n  pivot_longer(cols = ends_with(\"Price\"), \n               names_to = \"series\", values_to = \"price\") |&gt;\n  filter(!is.na(price)) |&gt;\n  ggplot(aes(x = date, y = price, color = series)) +\n  geom_line() +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\nFigure 6: ANZ: CorpAdjustedPrice and AdjustedPrice\n\n\n\n\n\nFigure 7 provides a more intuitive presentation. Rather than calculating returns using the adjusted prices directly, an alternative measure of returns is constructed by calculating a price relative using adjusted prices and then accumulating those returns. As can be seen in Figure 7, the two price series start at the same point (no scare quotes because this plot is going forward in time) and end at different points. As would be expected the cumulative returns without dividends are significantly lower by the end of the price series.\n\nanz_cum |&gt;\n  group_by(gcode) |&gt;\n  window_order(date) |&gt;\n  mutate(across(c(AdjustedPrice, CorpAdjustedPrice),\n                \\(x) coalesce(x / lag(x), 1)),\n         across(c(AdjustedPrice, CorpAdjustedPrice),\n                \\(x) exp(cumsum(log(x))))) |&gt;\n  window_order() |&gt;\n  pivot_longer(cols = ends_with(\"Price\"), \n               names_to = \"series\", values_to = \"price\") |&gt;\n  filter(!is.na(price)) |&gt;\n  ggplot(aes(x = date, y = price, color = series)) +\n  geom_line() +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\nFigure 7: ANZ: Fixed plot of CorpAdjustedPrice and AdjustedPrice\n\n\n\n\n\nTable 24 presents an example of an even larger difference between the different forms of adjusted price. The dividend of 0.4497 on 2012-07-09 precedes a fall in close price from 0.575 to 0.019. Adjusting only for CORAX events clearly leads to significantly different measures of share price performance when dividends are also present. If done with case, CORAX adjustments might also be used to standardise earnings information through time.\n\nadj_rets |&gt; \n filter(gcode == \"dmg1\", \n        between(date, \"2012-07-01\", \"2012-07-13\"), \n        seniorsecurity == 1) |&gt;\n  select(gcode, date, close, dividend, CorpAdjustedPrice, AdjustedPrice) |&gt;\n  arrange(date) |&gt;\n  collect() \n\n\n\nTable 24: Something about dmg1\n\n\n\n\n\n\ngcode\ndate\nclose\ndividend\nCorpAdjustedPrice\nAdjustedPrice\n\n\n\n\ndmg1\n2012-07-02\n0.570\nNA\n4.577\n0.570\n\n\ndmg1\n2012-07-03\n0.570\nNA\n4.577\n0.570\n\n\ndmg1\n2012-07-04\n0.575\nNA\n4.617\n0.575\n\n\ndmg1\n2012-07-05\n0.575\nNA\n4.617\n0.575\n\n\ndmg1\n2012-07-06\n0.575\nNA\n4.617\n0.575\n\n\ndmg1\n2012-07-09\n0.019\n0.45\n0.019\n0.602\n\n\ndmg1\n2012-07-10\n0.020\nNA\n0.020\n0.634\n\n\ndmg1\n2012-07-11\n0.019\nNA\n0.019\n0.602\n\n\ndmg1\n2012-07-12\n0.018\nNA\n0.018\n0.571\n\n\ndmg1\n2012-07-13\n0.018\nNA\n0.018\n0.571"
  },
  {
    "objectID": "published/sirca_eod.html#segmentation-by-trade-type",
    "href": "published/sirca_eod.html#segmentation-by-trade-type",
    "title": "SIRCA ASX End of Day (EOD) collection",
    "section": "8. Segmentation by trade type",
    "text": "8. Segmentation by trade type\nThe si_au_prc_daily table also contains information on the count, volume, and value of trades by various categories. This section provides examples of aggregating trading activity by different trade types:\n\nTrading activity across the whole market\nSegmentation by on- versus off-market trades\nProportion of on-market non-crossing trades that are carried out through ASX Centre Point\nComparison of lit-pool and dark-pool trading\nProportion of dark market trades that are carried out through ASX Centre Point\n\nFigure 8 shows the value of trading activity across the year plotted against time. Trading activity can vary significantly from month to month.\n\nsi_au_prc_daily |&gt;\n  mutate(month = floor_date(date, \"month\")) |&gt;\n  group_by(month) |&gt;\n  summarize(ValueWholeMkt = sum(valueonmkt + valueoffmkt, na.rm = TRUE)) |&gt;\n  ggplot(aes(x = month, y = ValueWholeMkt)) +\n  geom_line()\n\n\n\n\n\n\n\nFigure 8: Trading activity\n\n\n\n\n\n\nsi_au_prc_daily |&gt;\n  mutate(month = floor_date(date, \"month\")) |&gt;\n  group_by(month) |&gt;\n  summarize(across(c(valueonmkt, valueoffmkt), \\(x) sum(x, na.rm = TRUE))) |&gt;\n  pivot_longer(-month, names_to = \"location\", values_to = \"value\") |&gt;\n  ggplot(aes(x = month, y = value, color = location)) +\n  geom_line() +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\nFigure 9: Trading activity: On- versus off-market\n\n\n\n\n\nAs mentioned above, it is possible to segment the market by the visibility of trades. In the lit market, the order book is public and all orders (bid and offer) are visible to all participants. In contrast, in the dark market, the order book is not visible until trades are executed. The dark pool consists of both on-market and off-market crossing trades, as well as any Centre Point trades. This following section shows the distribution of activity across the lit and dark markets over time. Note: As our Centre Point trade measures include crossing trades, Centre Point crossing trade volumes need to be subtracted to avoid double-counting these trades in the calculation of the dark pool trading.\n\nsi_au_prc_daily |&gt;\n  mutate(month = floor_date(date, \"month\")) |&gt;\n  filter(volumeonmkt &gt; 0) |&gt;\n  group_by(month) |&gt;\n  summarize(Dark = sum(volumeoffmktcross + volumeonmktcross +\n                         volumecentrept - volumecentreptcross, na.rm = TRUE),\n            Lit = sum(volumeonmkt + volumeoffmkt - \n                        (volumeoffmktcross + volumeonmktcross +\n                           volumecentrept - volumecentreptcross), na.rm = TRUE)) |&gt;\n  pivot_longer(cols = -month, names_to = \"market\", values_to = \"volume\") |&gt;\n  ggplot(aes(x = month, y = volume, color = market)) +\n  geom_line() +\n  theme(legend.position = \"top\")\n\n\n\n\n\n\n\nFigure 10: Trading activity: Dark versus lit market\n\n\n\n\n\nThe ASX Centre Point matching system provides a market for dark pool liquidity. As such, Centre Point trades are a subset of on-market trades. More information on ASX Centre Point can be found on the ASX website. The composition of each market segment is displayed at the top of the si_au_prc_daily tab in our data dictionary for this service. Figure 11 shows the average proportion of on-market non-crossing trades that are directed through ASX Centre Point over time.\n\nsi_au_prc_daily |&gt;\n  filter(valueonmkt &gt; 0, valuecentrept &gt; 0) |&gt;\n  mutate(month = floor_date(date, \"month\")) |&gt;\n  group_by(month) |&gt;\n  summarize(AvgPropCentrePtNonCross = \n              sum(valuecentrept - valuecentreptcross, na.rm = TRUE) /\n                     sum(valueonmkt - valueonmktcross, na.rm = TRUE)) |&gt;\n  ggplot(aes(x = month, y = AvgPropCentrePtNonCross)) +\n  geom_line()\n\n\n\n\n\n\n\nFigure 11: Centre Point market share\n\n\n\n\n\nIt is a simple matter to focus on particular market segments. For example, the previous query can be targeted on companies whose market capitalisation is less than $50 million, with results depicted in Figure 12.\n\nsi_au_prc_daily |&gt;\n  filter(close * shares &lt; 50000000,\n         valueonmkt &gt; 0, valuecentrept &gt; 0) |&gt;\n  mutate(month = floor_date(date, \"month\")) |&gt;\n  group_by(month) |&gt;\n  summarize(AvgPropCentrePtNonCross = \n              sum(valuecentrept - valuecentreptcross, na.rm = TRUE) /\n                     sum(valueonmkt - valueonmktcross, na.rm = TRUE)) |&gt;\n  ggplot(aes(x = month, y = AvgPropCentrePtNonCross)) +\n  geom_line()\n\n\n\n\n\n\n\nFigure 12: Centre Point market share: Small caps\n\n\n\n\n\nFinally, Figure 13 shows the share of the dark market volumes traded on Centre Point.\n\nsi_au_prc_daily |&gt;\n  filter(valueonmkt &gt; 0, valuecentrept &gt; 0) |&gt;\n  mutate(Dark = volumeoffmktcross + volumeonmktcross +\n                         volumecentrept - volumecentreptcross,\n         CentrePt = volumecentrept - volumecentreptcross) |&gt;\n  mutate(month = floor_date(date, \"month\")) |&gt;\n  group_by(month) |&gt;\n  summarize(AvgPropCentrePtDark = mean(CentrePt / Dark, na.rm = TRUE)) |&gt;\n  ggplot(aes(x = month, y = AvgPropCentrePtDark)) +\n  geom_line()\n\n\n\n\n\n\n\nFigure 13: Centre Point share of dark market\n\n\n\n\n\n\ndbDisconnect(db)"
  },
  {
    "objectID": "published/sirca_eod.html#footnotes",
    "href": "published/sirca_eod.html#footnotes",
    "title": "SIRCA ASX End of Day (EOD) collection",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nExecute install.packages(c(\"tidyverse\", \"DBI\", \"duckdb\", \"arrow\", \"farr\", \"dbplyr\")) within R to install all the packages you need to run the code in this note. While duckdb and arrow are not listed below, they are needed to run the download script and to create the database we will use.↩︎\nSee SIRCA’s documentation for details on getting the data.↩︎\nWhile we access the data in a database throughout, most of the SQL is generated from tidyverse (R) code rather than being written by us directly.↩︎\nUsers with a background in SQL may find the SQL primer I wrote [here] to be a useful introduction to the dplyr package (this is the component of the tidyverse package that provides the relevant functions).↩︎\nThe reason for taking absolute values and accounting for the sign of factor is discussed below in Section 3.6.↩︎\nNote that the calculation of cum_factor_calc occurs on two separate lines, as DuckDB does not allow nesting of window functions, such as cumsum() and lag(). For more on window functions, see here.↩︎"
  },
  {
    "objectID": "published/sas_to_pd.html",
    "href": "published/sas_to_pd.html",
    "title": "Using SAS to create pandas data",
    "section": "",
    "text": "A strong point of pandas is its expressiveness. Its API allows users to explore data using succinct and (generally) intuitive code. However, some of this expressiveness relies on data being in forms (for example, with dates ready to serve as an index) that often differ from the data we have, and pandas can struggle to manipulate the data into those forms, especially with larger data sets.\nSAS might be another approach to manipulating data for pandas. My Python package wrds2pg offers a sas_to_pandas() function that can run code on the WRDS server and return the results as a pandas dataframe. While not quite as fast as using Ibis with the PostgreSQL server, SAS performs pretty well with this task.\n\n\n\n\n\n\nTip\n\n\n\nThe following command (run in the terminal on your computer) installs the packages you need.\n\npip install wrds2pg --upgrade\npip install pandas\n\nThe code assumes you have set the environment variable WRDS_ID to your WRDS ID.\nThis note was written using Quarto. The source code for this note is available here and the latest version of this PDF is here."
  },
  {
    "objectID": "published/repro_data.html",
    "href": "published/repro_data.html",
    "title": "Reproducible data collection",
    "section": "",
    "text": "An exercise I have assigned to students in the past is to go to the online supplements and datasheets page for the Journal of Accounting Research, pick an issue of the journal and evaluate whether one could reproduce the analysis in the associated paper using the materials made available there. Generally, the answer has been negative.1 That said, it seems that the Journal of Accounting Research is still the (relative) leader among accounting-focused academic journals with regard to requiring authors to supply materials.\n\n\n\n\n\n\nTip\n\n\n\nIn writing this note, I use several packages including those listed below.2 This note was written using Quarto and compiled with RStudio, an integrated development environment (IDE) for working with R. The source code for this note is available here and the latest version of this PDF is here.\nNote that the duckdb_to_parquet() function used below is currently only available in the development version of farr. Use remotes::install_github(\"iangow/farr\") to install this version.\n\nlibrary(tidyverse)\nlibrary(DBI)\nlibrary(farr)\n\n\n\nMy reason for visiting the datasheets page this week was to get a sense for how people are doing data analysis these days. Are people moving from R to Python, as the latter gets stronger for data science? Or are they preferring the domain-specific strengths of R? The answer is: neither. Based on the six papers in Volume 63, Issue 1 that are not listed as “datasheet and code forthcoming” and that provide something in terms of data, all used some combination of SAS and Stata. This is probably not much different from what you would have seen around fifteen years ago. The persistence of SAS and Stata surprises me given how easy modern tools make a lot of data analysis steps.3\nBut looking at the first paper in Volume 63, Issue 2, I see a mix of Stata and R. So, moving to the assignment I have given to students in the past, how reproducible is the analysis contained therein? I made some progress on this, but I didn’t get far (to be fair, I didn’t try very hard)."
  },
  {
    "objectID": "published/repro_data.html#getting-the-raw-data",
    "href": "published/repro_data.html#getting-the-raw-data",
    "title": "Reproducible data collection",
    "section": "2.1 Getting the raw data",
    "text": "2.1 Getting the raw data\nBased on the file names listed in the code (e.g., 2001_annual_singlefile.zip), ChatGPT suggested that I should go to the BLS’s QCEW data files page, where QCEW stands for “Quarterly Census of Employment and Wages” (not sure what happened to “investment” … I guess it’s a typo of sorts). If the authors had provided that link, it would’ve been very helpful for anyone trying to reproduce their analysis.\nGetting to the QCEW site, I guess the authors clicked the link for each year’s data—the paper used data from 2001 to 2019—and saved the .zip data file somewhere on one of their computers. Again, one can do better. A small function can get a single year’s data and lapply() can do this for all years. I tend to use an environment variable (RAW_DATA_DIR) to indicate where I put “raw” data files like these and in this case, I put them in a directory named qcew under that directory. Using environment variables in this way yields code that is much more reproducible than things like D:\\Users\\me\\my_projects\\this_project that seem very common on the JAR datasheets site. Note that the code below does not download a file if I already have it:\n\nyears &lt;- 2001:2019L\n\n\nget_qcew_zip_data &lt;- function(year, raw_data_dir = NULL,\n                              schema = \"qcew\") {\n\n  if (is.null(raw_data_dir)) {\n    raw_data_dir &lt;- file.path(Sys.getenv(\"RAW_DATA_DIR\"), schema)\n  } \n  \n  if (!dir.exists(raw_data_dir)) dir.create(raw_data_dir, recursive = TRUE)\n\n  url &lt;- stringr::str_glue(\"https://data.bls.gov/cew/data/files/{year}\",\n                           \"/csv/{year}_annual_singlefile.zip\")\n  t &lt;- file.path(raw_data_dir, basename(url))\n  if (!file.exists(t)) download.file(url, t)\n  invisible(t)\n}\n\nres &lt;- lapply(years, get_qcew_zip_data)"
  },
  {
    "objectID": "published/repro_data.html#processing-the-data-the-authors-approach",
    "href": "published/repro_data.html#processing-the-data-the-authors-approach",
    "title": "Reproducible data collection",
    "section": "2.2 Processing the data: The authors’ approach",
    "text": "2.2 Processing the data: The authors’ approach\nThe original R code supplied by the authors had code more or less like this (I omitted the code for bls03 through bls17 for reasons of space):\n\nbls01_in &lt;- read.csv(unzip(\"Data/BLS/2001_annual_singlefile.zip\")); \n  bls01 &lt;- bls_process(bls01_in); rm(bls01_in)\nbls02_in &lt;- read.csv(unzip(\"Data/BLS/2002_annual_singlefile.zip\")); \n  bls02 &lt;- bls_process(bls02_in); rm(bls02_in)\n...\nbls18_in &lt;- read.csv(unzip(\"Data/BLS/2018_annual_singlefile.zip\")); \n  bls18 &lt;- bls_process(bls18_in); rm(bls18_in)\nbls19_in &lt;- read.csv(unzip(\"Data/BLS/2019_annual_singlefile.zip\")); \n  bls19 &lt;- bls_process(bls19_in); rm(bls19_in)\n\nbls_all &lt;- rbind(bls01, bls02, bls03, bls04, bls05, bls06, bls07, bls08,\n                 bls09, bls10, bls11, bls12, bls13,\n                 bls14, bls15, bls16, bls17, bls18, bls19)\n\nI tweaked this code a little bit without changing its basic function. First, I replaced Data/BLS/ with something based on where I downloaded the files ({raw_data_dir}). Second, I put the unzip() and read.csv() calls inside the bls_process() function. Third, I replaced hard-coded file names such as 2001_annual_singlefile.zip with something like {year}_annual_singlefile.zip. Finally, I replaced the code creating bls_all with bls_all &lt;- rbind(lapply(2001:2019L, bls_process)).\nI put this code in a small R file and I can call this code using source(). The modified “original” code I used is available here.\n\nsystem.time(source(\"get_bls_data_orig.R\"))\n\n   user  system elapsed \n323.801  18.505 343.777 \n\n\nSo this takes about 6 minutes. This is not a big deal for the researchers, who probably worked on this project for years. But it’s an additional cost for anyone attempting to replicate the analysis in the paper.\nNote that the original “original” code left a lot of unzipped text files lying about on my hard drive; my modified code eliminates these (using unlink()). Also, quite a lot of memory is used up by having all the dataframes in memory before calling rbind()."
  },
  {
    "objectID": "published/repro_data.html#processing-the-data-my-first-attempt",
    "href": "published/repro_data.html#processing-the-data-my-first-attempt",
    "title": "Reproducible data collection",
    "section": "2.3 Processing the data: My first attempt",
    "text": "2.3 Processing the data: My first attempt\nCan we do better? I think one approach is to put the processed data into a parquet data repository of the kind I describe here. I use the environment variable DATA_DIR to keep track of the location of my repository (note that this could be different for different projects) and put data files in different “schemas” within DATA_DIR. In this case, I will put the data under qcew.\nFor my first attempt, I will use read_csv() from the readr package to read in the unzipped files and then write_parquet() from the arrow package to save the data to a parquet file in my data repository.\n\nget_qcew_data &lt;- function(year, schema = \"qcew\", data_dir = NULL,\n                          raw_data_dir = NULL) {\n\n  if (is.null(raw_data_dir)) {\n    raw_data_dir &lt;- file.path(Sys.getenv(\"RAW_DATA_DIR\"), schema)\n  } \n  \n  if (is.null(data_dir)) data_dir &lt;- Sys.getenv(\"DATA_DIR\")\n  data_dir &lt;- file.path(data_dir, schema)\n  if (!dir.exists(data_dir)) dir.create(data_dir, recursive = TRUE)\n\n  filename &lt;- stringr::str_glue(\"{year}_annual_singlefile.zip\")\n  t &lt;- path.expand(file.path(raw_data_dir, filename))\n\n  pq_path &lt;- stringr::str_c(\"annual_\", year, \".parquet\")\n  readr::read_csv(t, show_col_types = FALSE, guess_max = 100000) |&gt;\n    arrow::write_parquet(sink = file.path(data_dir, pq_path))\n  return(TRUE)\n}\n\nRunning this code, I see a bit of a performance pick-up, though it’s not (yet) an apples-to-apples comparison because my code is simply transforming the zipped CSV files into parquet files, while the “original” code was creating CSV files based on subsets of the data. While it will turn out that this is still a fair comparison, I wonder if I can do better. For one thing, there is still about 3GB of RAM used in the process of loading data into R and then saving to parquet files one at a time.\n\nsystem.time(lapply(years, get_qcew_data))\n\n   user  system elapsed \n265.143  21.882 145.036"
  },
  {
    "objectID": "published/repro_data.html#processing-the-data-my-second-attempt",
    "href": "published/repro_data.html#processing-the-data-my-second-attempt",
    "title": "Reproducible data collection",
    "section": "2.4 Processing the data: My second attempt",
    "text": "2.4 Processing the data: My second attempt\nFor my second approach, I will do all the data processing using DuckDB. I unzip the data, then use DuckDB’s read_csv() piped directly into parquet files, again saved in the same location.4\n\nget_qcew_data_duckdb &lt;- function(year, schema = \"qcew\", \n                                 data_dir = NULL,\n                                 raw_data_dir = NULL) {\n\n  if (is.null(raw_data_dir)) {\n    raw_data_dir &lt;- file.path(Sys.getenv(\"RAW_DATA_DIR\"), schema)\n  } \n  \n  if (is.null(data_dir)) data_dir &lt;- Sys.getenv(\"DATA_DIR\")\n  data_dir &lt;- file.path(data_dir, schema)\n  if (!dir.exists(data_dir)) dir.create(data_dir, recursive = TRUE)\n\n  filename &lt;- stringr::str_glue(\"{year}_annual_singlefile.zip\")\n  t &lt;- path.expand(file.path(raw_data_dir, filename))\n  csv_file &lt;- unzip(t, exdir = tempdir())\n\n  pq_file &lt;- stringr::str_glue(\"annual_{year}.parquet\")\n  pq_path &lt;- path.expand(file.path(data_dir, pq_file))\n  db &lt;- DBI::dbConnect(duckdb::duckdb())\n\n  args &lt;- \", types = {'year': 'INTEGER'}\"\n\n  sql &lt;- stringr::str_glue(\"COPY (SELECT * FROM read_csv('{csv_file}'{args})) \",\n                           \"TO '{pq_path}' (FORMAT parquet)\")\n\n  res &lt;- DBI::dbExecute(db, sql)\n  DBI::dbDisconnect(db)\n  unlink(csv_file)\n  res\n}\n\nHow does this code do? Well, much faster! Also, much less RAM used (more like hundreds of megabytes than gigabytes).\n\nsystem.time(lapply(2001:2019L, get_qcew_data_duckdb))\n\n   user  system elapsed \n130.561  17.008  36.656 \n\n\n\n2.4.1 Creating a code repository for the raw data\nGiven the general-purpose nature of the QCEW data, it seems to be a good candidate for creating a public repository that facilitates sharing of the data. I made such a repository here.\n\n\n2.4.2 Performing the final steps\nI said before that the comparison was a bit apples-to-oranges because I’ve skipped some steps from the “original” code. First, the original code applied some filters (e.g., industry_code &lt; 100 and own_code %in% c(0, 5)). I apply these filters in the code below.\nFirst, I connect to a fresh DuckDB database (a single line of code). Second, I use load_parquet() from the farr package with a wildcard (\"annual_*\") to load all the QCEW data into a single table. I then apply the filters. As you can see, this step is fast. One reason it is so fast is because the dbplyr package I am using to connect to DuckDB uses lazy evaluation, so it doesn’t actually realize any data in this step.\n\ndb &lt;- DBI::dbConnect(duckdb::duckdb())\n\nqcew_data &lt;-\n  load_parquet(db, \"annual_*\", schema = \"qcew\") |&gt;\n  filter(year %in% years) |&gt;\n  mutate(industry_code =\n           case_when(industry_code == \"31-33\" ~ \"31\",\n                     industry_code == \"44-45\" ~ \"44\",\n                     industry_code == \"48-49\" ~ \"48\",\n                     .default = industry_code),\n         industry_code = as.integer(industry_code)) |&gt;\n  filter(industry_code &lt; 100) |&gt;\n  # and for now just keep the ownership codes for\n  # - (a) total employment levels -- own_code = 0; and\n  # - (b) total private sector employment -- own_code = 5\n  filter(own_code %in% c(0, 5)) |&gt;\n  system_time()\n\n   user  system elapsed \n  0.064   0.003   0.068 \n\n\nI can take a quick peek at a sample from this data set:\n\nqcew_data\n\n# A query:  ?? x 38\n# Database: DuckDB 1.4.4 [igow@Darwin 25.4.0:R 4.5.2/:memory:]\n   area_fips own_code industry_code agglvl_code size_code  year qtr  \n   &lt;chr&gt;        &lt;dbl&gt;         &lt;int&gt;       &lt;dbl&gt;     &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;\n 1 01000            0            10          50         0  2001 A    \n 2 01000            5            10          51         0  2001 A    \n 3 01000            5            11          54         0  2001 A    \n 4 01000            5            21          54         0  2001 A    \n 5 01000            5            22          54         0  2001 A    \n 6 01000            5            23          54         0  2001 A    \n 7 01000            5            31          54         0  2001 A    \n 8 01000            5            42          54         0  2001 A    \n 9 01000            5            44          54         0  2001 A    \n10 01000            5            48          54         0  2001 A    \n# ℹ more rows\n# ℹ 31 more variables: disclosure_code &lt;chr&gt;, annual_avg_estabs &lt;dbl&gt;,\n#   annual_avg_emplvl &lt;dbl&gt;, total_annual_wages &lt;dbl&gt;,\n#   taxable_annual_wages &lt;dbl&gt;, annual_contributions &lt;dbl&gt;,\n#   annual_avg_wkly_wage &lt;dbl&gt;, avg_annual_pay &lt;dbl&gt;, lq_disclosure_code &lt;chr&gt;,\n#   lq_annual_avg_estabs &lt;dbl&gt;, lq_annual_avg_emplvl &lt;dbl&gt;,\n#   lq_total_annual_wages &lt;dbl&gt;, lq_taxable_annual_wages &lt;dbl&gt;, …\n\n\nThe original code created three data sets (all saved as CSV files): bls_state, bls_county, and bls_national.5 The following code chunks creates each of these in turn and saves them in the dlr schema (this might be considered to be the “project directory” for the paper, with qcew being a schema shared across projects). Note that the duckdb_to_parquet() function returns a DuckDB table based on the created parquet file, so it can be examined and used in subsequent analysis. As can be seen, none of these steps takes much time. Hence the apples-to-apples aspect of the performance comparison can be restored by adding up the time taken for all the steps (under a minute given that the zipped CSV files are already on my hard drive).\nNote that the underlying CSV files represented about 10 gigabytes of data, so the fact that creating data sets from the parquet versions of these can take a second or less is quite impressive.\n\nbls_county &lt;-\n  qcew_data |&gt;\n  filter(agglvl_code &gt; 69, agglvl_code &lt; 80) |&gt;\n  duckdb_to_parquet(name = \"bls_county\", schema = \"dlr\") |&gt;\n  system_time()\n\n   user  system elapsed \n 10.485   1.200   1.542 \n\nbls_county\n\n# A query:  ?? x 38\n# Database: DuckDB 1.4.4 [igow@Darwin 25.4.0:R 4.5.2/:memory:]\n   area_fips own_code industry_code agglvl_code size_code  year qtr  \n   &lt;chr&gt;        &lt;dbl&gt;         &lt;int&gt;       &lt;dbl&gt;     &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;\n 1 01001            0            10          70         0  2001 A    \n 2 01001            5            10          71         0  2001 A    \n 3 01001            5            11          74         0  2001 A    \n 4 01001            5            21          74         0  2001 A    \n 5 01001            5            22          74         0  2001 A    \n 6 01001            5            23          74         0  2001 A    \n 7 01001            5            31          74         0  2001 A    \n 8 01001            5            42          74         0  2001 A    \n 9 01001            5            44          74         0  2001 A    \n10 01001            5            48          74         0  2001 A    \n# ℹ more rows\n# ℹ 31 more variables: disclosure_code &lt;chr&gt;, annual_avg_estabs &lt;dbl&gt;,\n#   annual_avg_emplvl &lt;dbl&gt;, total_annual_wages &lt;dbl&gt;,\n#   taxable_annual_wages &lt;dbl&gt;, annual_contributions &lt;dbl&gt;,\n#   annual_avg_wkly_wage &lt;dbl&gt;, avg_annual_pay &lt;dbl&gt;, lq_disclosure_code &lt;chr&gt;,\n#   lq_annual_avg_estabs &lt;dbl&gt;, lq_annual_avg_emplvl &lt;dbl&gt;,\n#   lq_total_annual_wages &lt;dbl&gt;, lq_taxable_annual_wages &lt;dbl&gt;, …\n\n\n\nbls_state &lt;-\n  qcew_data |&gt;\n  filter(agglvl_code &gt; 49, agglvl_code &lt; 60) |&gt;\n  duckdb_to_parquet(name = \"bls_state\", schema = \"dlr\") |&gt;\n  system_time()\n\n   user  system elapsed \n  4.007   0.681   0.735 \n\n\n\nbls_national &lt;-\n  qcew_data |&gt;\n  filter(agglvl_code == 10) |&gt;\n  duckdb_to_parquet(name = \"bls_national\", schema = \"dlr\") |&gt;\n  system_time()\n\n   user  system elapsed \n  1.746   0.078   0.245"
  },
  {
    "objectID": "published/repro_data.html#footnotes",
    "href": "published/repro_data.html#footnotes",
    "title": "Reproducible data collection",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNot always, as several of the papers covered in Empirical Research in Accounting: Tools and Methods are replicated there using code from the Journal of Accounting Research page.↩︎\nExecute install.packages(c(\"tidyverse\", \"DBI\", \"duckdb\", \"remotes\", \"arrow\", \"dbplyr\")) within R to install all the packages you need to run the code in this note.↩︎\nIt seems that the typical SAS user dumps data to Excel to make plots and the typical Stata user exports tables to Excel for copy-pasting to Word. Does the typical Python or R user do that too?↩︎\nSo the files created in the previous step will be overwritten here.↩︎\nI use the same names for these files as were used by the original authors.↩︎\nNote that this is as far as I got in looking at the reproducibility of the steps in the original paper.↩︎"
  },
  {
    "objectID": "published/import_sirca.html",
    "href": "published/import_sirca.html",
    "title": "Data curation and the data science workflow",
    "section": "",
    "text": "This note proposes and illustrates an extended version of the data science “whole game” offered by Wickham, Çetinkaya-Rundel, and Grolemund (2023).1 The extended version divides the data science whole game into two processes: Curate and Understand. Using the case of Australian end-of-day stock price data, I explain what the Curate process is and how it covers important elements of the data science whole game that are neglected in the original version. One feature of Curate is that it requires a set of specialist skills often not possessed by practitioners expert in the Understand phase. As such, I argue that a more effective division of labour might result from better delineating the two data science processes.\nThe goal of this note is to outline an extended version of the data science model “whole game” proposed in R for Data Science (Wickham, Çetinkaya-Rundel, and Grolemund 2023). The original “whole game” comprises three steps. It starts with an Import-and-tidy process (this comprises import and tidy steps), then an Understand process (this involves iteration between transform, visualize, and model steps), followed by a Communicate process.2\nMy extension of the “whole game”—depicted in Figure 1 below—gives the name Curate to the original Import-and-tidy process and adds a persist step to it. As a complement to the new persist step, I also add a load step to the Understand process. As will we see, this load step will not generally be an elaborate one, but its inclusion serves to better delineate the boundary between the Curate and Understand processes.\nFigure 1: A representation of the data science workflow\nIn this note, I focus on the data curation (Curate) process. My rationale for separating Curate from Understand is that I believe it clarifies certain best practices in the curation of data. In particular, I see a lot of merit in applying the notion of a service-level agreement to delineating roles and responsibilities in the preparation and analysis of data. As discussed below, my conception of Curate encompasses some tasks that are included in the transform step (part of the Understand process) in R for Data Science. The current version of this note uses daily data on Australian stock prices as an application.\nWhile even a sole analyst who performs all three processes can benefit from thinking about Curate as a separate process from Understand, it is perhaps easiest to conceive of Curate and Understand as involving different individuals or organizational units of the “whole game” of a data analysis workflow.3\nInevitably, the distinction between tidy and transform can be difficult to draw. Nonetheless, I think it is useful to think of some “transform” steps as part of the process of data curation.4 For example, if the raw data express dates as strings (e.g., \"25/12/2023\"), there is merit in transforming these into parsed dates as part of the tidy step rather than confronting the issues associated with date formatting for each analysis conducted as part of the Understand process.\nThis note was written using Quarto and compiled with RStudio, an integrated development environment (IDE) for working with R. The source code for this note is available here and the latest PDF version is available here."
  },
  {
    "objectID": "published/import_sirca.html#storage-format",
    "href": "published/import_sirca.html#storage-format",
    "title": "Data curation and the data science workflow",
    "section": "2.1 Storage format",
    "text": "2.1 Storage format\nIn principle, the storage format should be a fairly minor detail determined by the needs of the Understand team. For example, if the Understand team works in Stata or Excel, then perhaps they will want the data in some kind of Stata format or as Excel files. However, I think it can be appropriate to push back on notions that data will be delivered in form that involves downgrading the data or otherwise compromises the process in a way that may ultimately add to the cost and complexity of the task for the Curate team. For example, “please send the final data as an Excel file attachment as a reply email” might be a request to be resisted because the process of converting to Excel can entail the degradation of data (e.g., time stamps or encoding of text).8 Instead it may be better to choose a more robust storage format and supply a script for turning that into a preferred format.\nOne storage format that I have used in the past would deliver data as tables in a (PostgreSQL) database. The Understand team could be given access data from a particular source organized as a schema in a database. Accessing the data in this form is easy for any modern software package. One virtue of this approach is that the data might be curated using, say, Python even though the client will analyse it using, say, Stata.9"
  },
  {
    "objectID": "published/import_sirca.html#good-database-principles",
    "href": "published/import_sirca.html#good-database-principles",
    "title": "Data curation and the data science workflow",
    "section": "2.2 Good database principles",
    "text": "2.2 Good database principles\nI included the word “pragmatic” because I think it’s not necessary in most cases to get particularly fussy about database normalization. That said, it’s probably bad practice to succumb to requests for One Big Table that the Understand team might make. It is reasonable to impose some obligation to merge naturally distinct tables on the client Understand team."
  },
  {
    "objectID": "published/import_sirca.html#primary-keys",
    "href": "published/import_sirca.html#primary-keys",
    "title": "Data curation and the data science workflow",
    "section": "2.3 Primary keys",
    "text": "2.3 Primary keys\nThe Curate team should communicate the primary key of each table to the Understand team.10 A table’s primary key is a set of variables that can be used to uniquely identify each row in that table. In general a primary key will have no missing values. Part of data curation will be confirming that a proposed primary key is in fact a valid primary key."
  },
  {
    "objectID": "published/import_sirca.html#data-types",
    "href": "published/import_sirca.html#data-types",
    "title": "Data curation and the data science workflow",
    "section": "2.4 Data types",
    "text": "2.4 Data types\nEach variable of each table should be of the correct type. For example, dates should be of type DATE, variables that only take integer values should be of INTEGER type.11 Date-times should generally be given with TIMESTAMP WITH TIME ZONE type. Logical columns should be supplied with type BOOLEAN.\nNote that there is an interaction between this element of the service-level agreement and the storage format. If the data are supplied in a PostgreSQL database or as parquet files, then it is quite feasible to prescribe the data types of each variable. But if the storage format is Excel files (not recommended!) or CSV files, then it is difficult for the data curator to control how each variable is understood by the Understand team.12\nIn some cases, it may seem unduly prescriptive to specify the types in a particular way. For example, a logical variable can easily be represented as INTEGER type (0 for FALSE, 1 for TRUE). Even in such cases, I think there is merit in choosing the most logical type (no pun intended) because of the additional information it conveys about the data. For example, a logical type should be checked to ensure that it only takes two values (TRUE or FALSE) plus perhaps NULL and that this checking has occurred is conveyed by the encoding of that variable as BOOLEAN."
  },
  {
    "objectID": "published/import_sirca.html#no-manual-steps",
    "href": "published/import_sirca.html#no-manual-steps",
    "title": "Data curation and the data science workflow",
    "section": "2.5 No manual steps",
    "text": "2.5 No manual steps\nWhen data vendors are providing well-curated datasets, much about the curation process will be obscure to the user. This makes some sense, as the data curation process has elements of trade secrets. But often data will be supplied by vendors in an imperfect state and significant data curation will be performed by the Curate team working for or within the same organization as the Understand team.\nFocusing on the case where the data curation process transforms an existing dataset—say, one purchased from an outside vendor—into a curated dataset in sense used here, there are a few ground rules regarding manual steps.\nFirst, the original data files should not be modified in any way. If data are supplied as CSV files, then merely opening them in Excel and saving them can mutilate the original data.13 I have encountered people whose idea of data curation extended to opening the original files, saving them as Excel files, and then proceeding to manually edit those files. This approach leads to a completely unreproducible set of data files, which is problematic not only in a world in which reproducibility is starting to be expected, but also when a new version of the data will be supplied by the vendor in the future.\nSecond, any manual steps should be extensively documented and applied in a transparent automated fashion. For example, if a dataset on financial statement items of US firms contains errors that can be corrected by reviewing original SEC filings, then any corrections should be clearly documented in separate files with links to the original filings and explanations. And the corrections should be implemented through code, not manual steps.14 For example, there should be code that imports the original data and the corrections and applies the latter to the former to create the final dataset."
  },
  {
    "objectID": "published/import_sirca.html#documentation",
    "href": "published/import_sirca.html#documentation",
    "title": "Data curation and the data science workflow",
    "section": "2.6 Documentation",
    "text": "2.6 Documentation\nThe process of curating the data should be documented sufficiently well that someone else could perform the curation steps should the need arise. Often that need will arise when the vendor provides an updated dataset. Perhaps the best way to understand what I have in mind here is through a case study and I provide one in Section 3."
  },
  {
    "objectID": "published/import_sirca.html#update-process",
    "href": "published/import_sirca.html#update-process",
    "title": "Data curation and the data science workflow",
    "section": "2.7 Update process",
    "text": "2.7 Update process\nPart of the rationale for having a well-documented process with no manual steps is that it greatly facilitates updating the data when the data vendor or other data source provides updated data. In some cases, updating the data will entail little more than downloading the new raw data and running a pre-existing script on those data. In other cases, the data may change in significant ways, such as addition of new variables, renaming of existing ones, or reorganization of data into different tables.\nAs future changes may be difficult to predict, the analyst might be able to do little more than describe the anticipated update process if no major changes occur. If major changes do subsequently occur, it likely makes sense for the analyst handling the update to extensively document the changes needed to process the new data, especially if earlier versions of the data remain relevant (e.g., they have been used in published research)."
  },
  {
    "objectID": "published/import_sirca.html#data-version-control",
    "href": "published/import_sirca.html#data-version-control",
    "title": "Data curation and the data science workflow",
    "section": "2.8 Data version control",
    "text": "2.8 Data version control\nWelch (2019) argues that, to ensure that results can be reproduced, “the author should keep a private copy of the full data set with which the results were obtained.” This imposes a significant cost on the Understand team to maintain archives of datasets that may run to several gigabytes or more and it would seem much more efficient for these obligations to reside with the parties with the relevant expertise.\nUnfortunately, even when data vendors provide curated datasets, they generally provide little in the way of version control. For example, there is no evidence that Wharton Research Data Services (WRDS), perhaps the largest data vendor in academic business research, provides any version control for its datasets, even though it should have much greater expertise for doing this than the users of its services.\nNonetheless, some notion of version control of data probably has a place in data curation, even if this is little more than archiving of various versions of data supplied to research teams."
  },
  {
    "objectID": "published/import_sirca.html#importing-si_au_ref_names",
    "href": "published/import_sirca.html#importing-si_au_ref_names",
    "title": "Data curation and the data science workflow",
    "section": "3.1 Importing si_au_ref_names",
    "text": "3.1 Importing si_au_ref_names\nAs discussed above, we start with si_au_ref_names. We first specify the name of the CSV file si_au_ref_names_csv, then quickly move on to reading the data using the read_csv() function. The displayed output from invoking read_csv() provides a good starting point for the next steps.\n\n3.1.1 Setting data types\nAs can be seen, si_au_ref_names contains 20 columns that read_csv() parses as character columns and 9 columns that read_csv() parses as numeric columns.\n\nsi_au_ref_names_csv &lt;- file.path(csv_dir, \"si_au_ref_names.csv.gz\")\nsi_au_ref_names &lt;- read_csv(si_au_ref_names_csv, guess_max = 10000)\n\nRows: 12025 Columns: 29\n── Column specification ───────────────────────────────────────────────────\nDelimiter: \",\"\nchr (20): Gcode, CompanyTicker, SecurityTicker, SecurityType, Abreviate...\ndbl  (9): SeniorSecurity, ListDate_YMD, DelistDate_YMD, ListDate_DaysSi...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThe next step we take is to inspect the columns to determine whether refinement of types makes sense. In practice, we can infer appropriate types by looking at the data.\nWe start with three of the numeric columns. The first three appear to be integers, either based on casual inspection of the values displayed or inferences from the variable names (e.g., “days since” seems likely to be an integer).\n\nsi_au_ref_names |&gt; \n  select_if(is.numeric) |&gt;\n  select(1, 4:5)\n\n# A tibble: 12,025 × 3\n  SeniorSecurity ListDate_DaysSince DelistDate_DaysSince\n           &lt;dbl&gt;              &lt;dbl&gt;                &lt;dbl&gt;\n1              1              43355                   NA\n2              1              42604                   NA\n3              1              44699                   NA\n4              1              42341                44060\n5              1              45197                   NA\n# ℹ 12,020 more rows\n\n\nWe can check that converting these variables to integers using as.integer() does not change any of their values.\n\nsi_au_ref_names |&gt; \n  select_if(is.numeric) |&gt;\n  select(1, 4:5) |&gt;\n  summarize(across(everything(), \\(x) all(x == as.integer(x), na.rm = TRUE)))\n\n# A tibble: 1 × 3\n  SeniorSecurity ListDate_DaysSince DelistDate_DaysSince\n  &lt;lgl&gt;          &lt;lgl&gt;              &lt;lgl&gt;               \n1 TRUE           TRUE               TRUE                \n\n\nWe can do the same for four of the remaining numeric columns.\n\nsi_au_ref_names |&gt; \n  select_if(is.numeric) |&gt;\n  select(6:9) |&gt;\n  summarize(across(everything(), \\(x) all(x == as.integer(x), na.rm = TRUE)))\n\n# A tibble: 1 × 4\n  RecordCount GICSIndustry SIRCAIndustryClassCode SIRCASectorCode\n  &lt;lgl&gt;       &lt;lgl&gt;        &lt;lgl&gt;                  &lt;lgl&gt;          \n1 TRUE        TRUE         TRUE                   TRUE           \n\n\nThe remaining numeric variables appear to be dates in ymd form read by read_csv() as numeric variables.\n\nsi_au_ref_names |&gt; \n  select_if(is.numeric) |&gt;\n  select(2:3)\n\n# A tibble: 12,025 × 2\n  ListDate_YMD DelistDate_YMD\n         &lt;dbl&gt;          &lt;dbl&gt;\n1     20180912             NA\n2     20160822             NA\n3     20220518             NA\n4     20151203       20200817\n5     20230928             NA\n# ℹ 12,020 more rows\n\n\nWe can convert these columns to dates with the ymd() function. In the following code snippet, we convert the numeric variables to the types we determined to be appropriate through the analysis above. Here this code just tests that nothing untoward happens; we will actually implement these type conversions in code below.\n\nsi_au_ref_names |&gt; \n  select_if(is.numeric) |&gt;\n  mutate(across(c(SeniorSecurity, ListDate_DaysSince, DelistDate_DaysSince,\n                  RecordCount, GICSIndustry, SIRCAIndustryClassCode, \n                  SIRCASectorCode), as.integer),\n         across(ends_with(\"_YMD\"), ymd))\n\n# A tibble: 12,025 × 9\n  SeniorSecurity ListDate_YMD DelistDate_YMD ListDate_DaysSince\n           &lt;int&gt; &lt;date&gt;       &lt;date&gt;                      &lt;int&gt;\n1              1 2018-09-12   NA                          43355\n2              1 2016-08-22   NA                          42604\n3              1 2022-05-18   NA                          44699\n4              1 2015-12-03   2020-08-17                  42341\n5              1 2023-09-28   NA                          45197\n# ℹ 12,020 more rows\n# ℹ 5 more variables: DelistDate_DaysSince &lt;int&gt;, RecordCount &lt;int&gt;,\n#   GICSIndustry &lt;int&gt;, SIRCAIndustryClassCode &lt;int&gt;,\n#   SIRCASectorCode &lt;int&gt;\n\n\nWe can now move onto the 20 columns read as character vectors. The first five character vectors seem correctly identified as such.\n\nsi_au_ref_names |&gt;\n  select_if(is.character) |&gt;\n  select(1:5)\n\n# A tibble: 12,025 × 5\n  Gcode CompanyTicker SecurityTicker SecurityType AbreviatedSecurityDescr…¹\n  &lt;chr&gt; &lt;chr&gt;         &lt;chr&gt;          &lt;chr&gt;        &lt;chr&gt;                    \n1 14d1  14D           14D            01           ordinary                 \n2 1ad1  1AD           1AD            01           ordinary                 \n3 1ae1  1AE           1AE            01           ordinary                 \n4 1al1  1AL           1AL            01           ordinary                 \n5 1gov1 1GO           1GOV           07           etf units                \n# ℹ 12,020 more rows\n# ℹ abbreviated name: ¹​AbreviatedSecurityDescription\n\n\nThe same is true for character vectors 8 and 9 …\n\nsi_au_ref_names |&gt;\n  select_if(is.character) |&gt;\n  select(8:9) \n\n# A tibble: 12,025 × 2\n  FullCompanyName                                AbbrevCompanyName   \n  &lt;chr&gt;                                          &lt;chr&gt;               \n1 1414 DEGREES LIMITED                           1414 DEGREES LIMITED\n2 ADALTA LIMITED                                 ADALTA LIMITED      \n3 AURORA ENERGY METALS LIMITED                   AURORAENERGYMETALS  \n4 ONEALL INTERNATIONAL LIMITED                   ONEALL IN LIMITED   \n5 VANECK 1-5 YEAR AUSTRALIAN GOVERNMENT BOND ETF VANECK 1-5 YR GOV   \n# ℹ 12,020 more rows\n\n\n… and character vectors 14 through 15 …\n\nsi_au_ref_names |&gt;\n  select_if(is.character) |&gt;\n  select(14:15) |&gt;\n  filter(if_all(everything(), \\(x) !is.na(x)))\n\n# A tibble: 23 × 2\n  CompanyDelistReasonComment                                    AlteredLink\n  &lt;chr&gt;                                                         &lt;chr&gt;      \n1 converts to a trust by a one-for-one in specie issue in trus… [aqf] is o…\n2 pursuant to scheme of arrangement with arrow pharmaceuticals… {awp2}[awp…\n3 &lt;18/02/2000&gt;. Demerger. {bor1} (bor) boral limited split int… {bor1} (bo…\n4 seven group holdings limited offers 170 cents plus 0.1116 {s… {bor1} (bo…\n5 redomiciled to New Zealand after one for one share exchange … redomicile…\n# ℹ 18 more rows\n\n\n… and character vectors 16 through 20.\n\nsi_au_ref_names |&gt;\n  select_if(is.character) |&gt;\n  select(16:20) |&gt;\n  filter(if_all(everything(), \\(x) !is.na(x)))\n\n# A tibble: 1 × 5\n  MS_CompanyID MS_SecurityID MS_CompanyID2 MS_SecurityID2 MA_Identifier\n  &lt;chr&gt;        &lt;chr&gt;         &lt;chr&gt;         &lt;chr&gt;          &lt;chr&gt;        \n1 0C00000O7P   0P000188J3    0C0000B4KQ    0P0001887N     MHJ          \n\n\nThis leaves character columns 12 and 13. Focusing on the cases where neither is NA, we see that these columns appear to be lists of codes separated by semi-colons (;).\n\nsi_au_ref_names |&gt;\n  select_if(is.character) |&gt;\n  select(12:13) |&gt;\n  filter(if_all(everything(), \\(x) !is.na(x)))\n\n# A tibble: 494 × 2\n  CompanyDelistReasonCode CompanyRelatedGCode\n  &lt;chr&gt;                   &lt;chr&gt;              \n1 A                       ama2               \n2 S;R;M                   mlb2               \n3 A                       tai1               \n4 A                       wgr1               \n5 N                       aln2; agl1; agk1   \n# ℹ 489 more rows\n\n\n\nsi_au_ref_names |&gt;\n  mutate(CompanyRelatedGCodes = CompanyRelatedGCode) |&gt;\n  filter(CompanyRelatedGCode != \"\") |&gt;\n  mutate(related_gcode = str_split(CompanyRelatedGCode, \"[;\\\\s]+\")) |&gt;\n  select(Gcode, CompanyRelatedGCode, related_gcode) \n\n# A tibble: 497 × 3\n  Gcode CompanyRelatedGCode related_gcode\n  &lt;chr&gt; &lt;chr&gt;               &lt;list&gt;       \n1 4wd1  ama2                &lt;chr [1]&gt;    \n2 5gn1  mlb2                &lt;chr [1]&gt;    \n3 a1c1  tai1                &lt;chr [1]&gt;    \n4 aag2  wgr1                &lt;chr [1]&gt;    \n5 aan2  aln2; agl1; agk1    &lt;chr [3]&gt;    \n# ℹ 492 more rows\n\n\n\nsi_au_ref_names |&gt;\n  filter(CompanyRelatedGCode != \"\") |&gt;\n  mutate(related_gcode = str_split(CompanyRelatedGCode, \"[;\\\\s]+\")) |&gt;\n  unnest(related_gcode) |&gt;\n  select(Gcode, CompanyRelatedGCode, related_gcode) \n\n# A tibble: 531 × 3\n  Gcode CompanyRelatedGCode related_gcode\n  &lt;chr&gt; &lt;chr&gt;               &lt;chr&gt;        \n1 4wd1  ama2                ama2         \n2 5gn1  mlb2                mlb2         \n3 a1c1  tai1                tai1         \n4 aag2  wgr1                wgr1         \n5 aan2  aln2; agl1; agk1    aln2         \n# ℹ 526 more rows\n\n\n\n\n\n\n\n\nTip 1: Friends don’t let friends use Excel\n\n\n\nFrom casual observation, it appears that valid Gcode values contain only lower case characters ([a-z] in regular expressions) or numbers ([0-9] in regular expressions). Are there any CompanyRelatedGCode values that contain other characters? It turns out that that there are.\n\nsi_au_ref_names |&gt;\n  mutate(related_gcode = str_split(CompanyRelatedGCode, \"[;\\\\s]+\")) |&gt;\n  unnest(related_gcode) |&gt;\n  filter(str_detect(related_gcode, \"[^a-z0-9]\")) |&gt;\n  select(Gcode, related_gcode, CompanyDelistReasonComment) \n\n# A tibble: 2 × 3\n  Gcode related_gcode CompanyDelistReasonComment\n  &lt;chr&gt; &lt;chr&gt;         &lt;chr&gt;                     \n1 ahx1  May-01        mayne nickless ltd        \n2 fhf1  May-01        mayne nickless limited    \n\n\nWhat’s happened here? May-01 looks more like a date than a Gcode. This has all the hallmarks of someone having imported data into Microsoft Excel as part of their process. Microsoft Excel has a well-known tendency to mangle values that it aggressively interprets as dates. It seems likely that the Gcode for Mayne Nickless was may1 and Excel read this as May-01 (a date).17 Is it true that Gcode values contain only lower-case characters and numbers?\n\nweird_gcodes &lt;-\n  si_au_ref_names |&gt;\n  filter(str_detect(Gcode, \"[^a-z0-9]\")) |&gt;\n  distinct(Gcode) \n\nIt seems not; some Gcodes have underscores (_):\n\nstr_flatten(pull(weird_gcodes), \", \")\n\n[1] \"92e_1, apr_1, aug_1, aug_3, mar_2, may_1, nov_1, oct_1\"\n\n\nTo see why underscores are used, we can remove the underscore and save the Gcodes in a CSV file.18\n\nweird_gcodes |&gt;\n  mutate(Gcode = str_remove(Gcode, \"_\")) |&gt;\n  write_csv(\"weird_gcodes.csv\")\n\nTry opening weird_gcodes.csv in Excel. What do you see? (It may help to open weird_gcodes.csv in a text editor to see the original values.) To be frank, I struggle to see any reason why Excel should have any part in the data science workflow.19\n\n\nWe can examine CompanyDelistReasonCode in much the same way we did CompanyRelatedGCode. For reasons of brevity, I spare you the coding details and focus on the processed data, information about which is shown in Table 3.\nOne problem is evident from Table 3 and that is the presence of what appears to be junk in the CompanyDelistReasonCode field (e.g., 18 or R-apx). Another problem is evident only after looking that the documentation for si_au_ref_names and that is that even when the codes appear well-formed (e.g., N or C), we have no information about what these codes mean.\n\n\n\n\nTable 3: Delisting reason codes on si_au_ref_names\n\n\n\n\n\n\ndelist_code\nn\n\ndelist_code\nn\n\n\n\n\nN\n3933\n\nG\n16\n\n\nC\n3646\n\nX\n13\n\n\nR\n1047\n\nI\n11\n\n\nA\n736\n\nT\n7\n\n\nS\n598\n\nZ\n6\n\n\nM\n412\n\n18\n5\n\n\nF\n294\n\n2\n5\n\n\nE\n291\n\n9\n5\n\n\nY\n187\n\nD\n5\n\n\nW\n49\n\nP\n5\n\n\nL\n32\n\nB\n4\n\n\nH\n27\n\np\n2\n\n\nU-x\n27\n\n0\n1\n\n\nO\n21\n\nE-x\n1\n\n\n\n\n\n\n\n\nGiven the issues apparent in both CompanyRelatedGCode and CompanyDelistReasonCode, I have elected to collect those, but keep them as simple character columns.\nFor those keeping track, we have four character columns left. It turns out that the name for each of these ends with Date. In the following, I focus on the observations with non-NA values in all of these columns.\n\nsi_au_ref_names |&gt;\n  select_if(is.character) |&gt;\n  select(ends_with(\"Date\")) |&gt;\n  filter(if_all(everything(), \\(x) !is.na(x)))\n\n# A tibble: 5,400 × 4\n  ListDate   DelistDate EarliestListDate LatestDelistDate\n  &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;            &lt;chr&gt;           \n1 03/12/2015 17/08/2020 03/12/2015       17/08/2020      \n2 14/06/2019 11/04/2023 14/06/2019       11/04/2023      \n3 02/03/2021 16/05/2023 02/03/2021       29/08/2023      \n4 17/05/2023 13/06/2023 02/03/2021       29/08/2023      \n5 14/06/2023 29/08/2023 02/03/2021       29/08/2023      \n# ℹ 5,395 more rows\n\n\nFrom the above, it seems clear that we have dates in dmy form. It turns out that a couple of observations have the value \"0/01/1900\", which is not a valid date and I convert these to missing values using the code below.\n\nsi_au_ref_names |&gt;\n  select(ends_with(\"Date\")) |&gt;\n  mutate(across(ends_with(\"Date\"), \n                \\(x) dmy(if_else(x == \"0/01/1900\", NA, x))))\n\n# A tibble: 12,025 × 4\n  ListDate   DelistDate EarliestListDate LatestDelistDate\n  &lt;date&gt;     &lt;date&gt;     &lt;date&gt;           &lt;date&gt;          \n1 2018-09-12 NA         2018-09-12       NA              \n2 2016-08-22 NA         2016-08-22       NA              \n3 2022-05-18 NA         2022-05-18       NA              \n4 2015-12-03 2020-08-17 2015-12-03       2020-08-17      \n5 2023-09-28 NA         2023-09-28       NA              \n# ℹ 12,020 more rows\n\n\nAt this point, we have two versions of the variables related to listing dates (ListDate_YMD and ListDate) and to delisting dates (DelistDate_YMD and DelistDate) and perhaps it makes sense to keep just one of each. If the values in each of the pair is the same as the other, then there’s no reason to keep both.\nLooking at ListDate_YMD and ListDate, we see that they are always equal and we could drop either one and keep the other.\n\nsi_au_ref_names |&gt;\n  select(matches(\"ListDate\")) |&gt;\n  mutate(across(ends_with(\"Date\"), \n                \\(x) dmy(if_else(x == \"0/01/1900\", NA, x))),\n         across(ends_with(\"_YMD\"), ymd)) |&gt;\n  filter(ListDate_YMD != ListDate)\n\n# A tibble: 0 × 8\n# ℹ 8 variables: ListDate_YMD &lt;date&gt;, DelistDate_YMD &lt;date&gt;,\n#   ListDate_DaysSince &lt;dbl&gt;, DelistDate_DaysSince &lt;dbl&gt;, ListDate &lt;date&gt;,\n#   DelistDate &lt;date&gt;, EarliestListDate &lt;date&gt;, LatestDelistDate &lt;date&gt;\n\n\nBut there is one instance where DelistDate_YMD and DelistDate differ.\n\nsi_au_ref_names |&gt;\n  select(matches(\"^DelistDate\")) |&gt;\n  mutate(across(ends_with(\"Date\"), \n                \\(x) dmy(if_else(x == \"0/01/1900\", NA, x))),\n         across(ends_with(\"_YMD\"), ymd)) |&gt;\n  filter(DelistDate_YMD != DelistDate)\n\n# A tibble: 1 × 3\n  DelistDate_YMD DelistDate_DaysSince DelistDate\n  &lt;date&gt;                        &lt;dbl&gt; &lt;date&gt;    \n1 2013-06-30                    41455 2016-06-30\n\n\nWhich one to choose? One approach would be to look to external sources to verify which date is correct. But for present purposes we will choose the one that keeps our data internally consistent. Specifically, we should choose whichever of DelistDate_YMD and DelistDate that is consistent with DelistDate_DaysSince.\nLooking for other rows where DelistDate_DaysSince == 45051, we see that that value is elsewhere consistent with the value in DelistDate, so here I choose to drop the _YMD variables.\n\nsi_au_ref_names |&gt;\n  select(Gcode, starts_with(\"DelistDate\")) |&gt;\n  filter(DelistDate_DaysSince == 45051)\n\n# A tibble: 1 × 4\n  Gcode DelistDate_YMD DelistDate_DaysSince DelistDate\n  &lt;chr&gt;          &lt;dbl&gt;                &lt;dbl&gt; &lt;chr&gt;     \n1 iesg1       20230505                45051 05/05/2023\n\n\nPutting all the pieces above we have the following:\n\nsi_au_ref_names &lt;-\n  read_csv(si_au_ref_names_csv, guess_max = Inf,\n           show_col_types = FALSE) |&gt;\n  mutate(across(c(SeniorSecurity, ListDate_DaysSince, DelistDate_DaysSince,\n                  RecordCount, GICSIndustry, SIRCAIndustryClassCode, \n                  SIRCASectorCode), as.integer),\n         across(ends_with(\"Date\"), \n                \\(x) dmy(if_else(x == \"0/01/1900\", NA, x)))) |&gt;\n  select(-ends_with(\"_YMD\"))\n\n\n\n3.1.2 Identifying the primary key\nBefore considering possible primary keys, we first determine if there are any duplicate rows. When there are duplicate rows, no possible combination of columns will work as a primary key.\nThe following function returns any rows that are duplicated in a dataset.\n\nget_dupes &lt;- function(df, count_var = \"count\") {\n  df |&gt; \n    count(pick(everything()), name = count_var) |&gt;\n    filter(.data[[count_var]] &gt; 1)\n}\n\nApplying this function to si_au_ref_names, we see that we have one row that appears twice in the dataset.\n\nsi_au_ref_names |&gt;\n  get_dupes() |&gt;\n  select(Gcode, SecurityTicker, ListDate, count)\n\n# A tibble: 0 × 4\n# ℹ 4 variables: Gcode &lt;chr&gt;, SecurityTicker &lt;chr&gt;, ListDate &lt;date&gt;,\n#   count &lt;int&gt;\n\n\nTo address this, we will simply use the distinct() function.\nMoving on to consider potential primary keys, we see immediately that (Gcode, SecurityTicker) is not a valid primary key. As seen in the output below, a given (Gcode, SecurityTicker) combination can appear as many as seven times in the data.\n\nsi_au_ref_names |&gt;\n  distinct() |&gt;\n  count(Gcode, SecurityTicker, name = \"num_rows\") |&gt;\n  count(num_rows)\n\n# A tibble: 7 × 2\n  num_rows     n\n     &lt;int&gt; &lt;int&gt;\n1        1  6345\n2        2  1719\n3        3   469\n4        4   137\n5        5    44\n6        6    10\n7        7     1\n\n\nLooking across the columns, we see that (Gcode, SecurityTicker, ListDate) almost works, as we have just one case where (Gcode, SecurityTicker, ListDate) fails to identify a single row. In this particular case, it seems that we have differences only in GICSIndustry and SIRCAIndustryClassCode. In one row, these variables are missing; in the other there are values.\n\nsi_au_ref_names |&gt;\n  distinct() |&gt;\n  group_by(Gcode, SecurityTicker, ListDate) |&gt;\n  filter(n() &gt; 1) |&gt;\n  ungroup() |&gt;\n  arrange(Gcode, SecurityTicker, ListDate) |&gt;\n  select(Gcode, SecurityTicker, ListDate, GICSIndustry, SIRCAIndustryClassCode)\n\n# A tibble: 0 × 5\n# ℹ 5 variables: Gcode &lt;chr&gt;, SecurityTicker &lt;chr&gt;, ListDate &lt;date&gt;,\n#   GICSIndustry &lt;int&gt;, SIRCAIndustryClassCode &lt;int&gt;\n\n\nIf we take the row with non-NA values for GICSIndustry and SIRCAIndustryClassCode to be the correct one, then we should delete the other row.\n\nsi_au_ref_names |&gt;\n  filter(Gcode == \"rgwb1\") |&gt;\n  select(Gcode, GICSIndustry)\n\n# A tibble: 1 × 2\n  Gcode GICSIndustry\n  &lt;chr&gt;        &lt;int&gt;\n1 rgwb1     99999999\n\n\nIt turns out that these are the only two rows where Gcode == \"rgwb1\", so if we eliminate the row with NA value in GICSIndustry we should have it that (Gcode, SecurityTicker, ListDate) uniquely identifies each row.\n\nsi_au_ref_names |&gt;\n  distinct() |&gt;\n  filter(!(Gcode == \"rgwb1\" & is.na(GICSIndustry))) |&gt;\n  count(Gcode, SecurityTicker, ListDate, name = \"num_rows\") |&gt;\n  count(num_rows)\n\n# A tibble: 1 × 2\n  num_rows     n\n     &lt;int&gt; &lt;int&gt;\n1        1 12025\n\n\nTo confirm that (Gcode, SecurityTicker, ListDate) is a valid primary key for our filtered si_au_ref_names, we also need to check that there are no NA values in any of these fields, which the following code confirms.\n\nsi_au_ref_names |&gt;\n  distinct() |&gt;\n  filter(!(Gcode == \"rgwb1\" & is.na(GICSIndustry))) |&gt;\n  summarize(across(c(Gcode, SecurityTicker, ListDate),\n                   \\(x) all(!is.na(x))))\n\n# A tibble: 1 × 3\n  Gcode SecurityTicker ListDate\n  &lt;lgl&gt; &lt;lgl&gt;          &lt;lgl&gt;   \n1 TRUE  TRUE           TRUE    \n\n\n\n\n3.1.3 Writing the parquet file\nSo, we can put the reading of raw data, the conversion of data types, and the filters needed to have a valid primary key together. But we have one final adjustment to make and that is to convert all variable names to lower case, as we will see later that the variable names embedded in si_au_prc_daily.csv.gz are all lower case (e.g., gcode), so we probably make our lives easier my converting our variables here to lower case (e.g., so we can join on gcode without worrying about slight differences in variable names).\nWith that final adjustment, we can then write to a parquet file, as we do here. We will use the environment variable DATA_DIR that you set above to specify the location.\n\npq_dir &lt;- file.path(Sys.getenv(\"DATA_DIR\"), \"sirca\")\nif (!dir.exists(pq_dir)) dir.create(pq_dir)\n\nsi_au_ref_names &lt;-\n  read_csv(si_au_ref_names_csv, show_col_types = FALSE) |&gt;\n  mutate(across(c(SeniorSecurity, ListDate_DaysSince, DelistDate_DaysSince,\n                  RecordCount, GICSIndustry, SIRCAIndustryClassCode, \n                  SIRCASectorCode), as.integer),\n         across(ends_with(\"Date\"), \n                \\(x) dmy(if_else(x == \"0/01/1900\", NA, x)))) |&gt;\n  select(-ends_with(\"_YMD\")) |&gt;\n  distinct() |&gt;\n  filter(!(Gcode == \"rgwb1\" & is.na(GICSIndustry))) |&gt;\n  rename_with(str_to_lower) |&gt;\n  write_parquet(sink = file.path(pq_dir, \"si_au_ref_names.parquet\")) |&gt;\n  system_time()\n\nWarning: One or more parsing issues, call `problems()` on your data frame for\ndetails, e.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\n   user  system elapsed \n  0.064   0.012   0.064"
  },
  {
    "objectID": "published/import_sirca.html#importing-si_au_ref_trddays",
    "href": "published/import_sirca.html#importing-si_au_ref_trddays",
    "title": "Data curation and the data science workflow",
    "section": "3.2 Importing si_au_ref_trddays",
    "text": "3.2 Importing si_au_ref_trddays\nA similar process to that used for si_au_ref_names can be applied to si_au_ref_trddays. However, si_au_ref_trddays is a much simpler file and we conclude that the types of the five columns can be specified using col_types = \"ciDii\", where c means character, i means integer, and D means date.20\n\nsi_au_ref_trddays_csv &lt;- file.path(csv_dir, \"si_au_ref_trddays.csv.gz\")\n\nsi_au_ref_trddays &lt;-\n  read_csv(si_au_ref_trddays_csv, col_types = \"ciDii\") |&gt;\n  mutate(dateymd = ymd(dateymd))\n\nWe can easily confirm that date is a valid primary key:\n\nsi_au_ref_trddays |&gt; \n  count(date, name = \"num_rows\") |&gt;\n  count(num_rows)\n\n# A tibble: 1 × 2\n  num_rows     n\n     &lt;int&gt; &lt;int&gt;\n1        1  6326\n\nsi_au_ref_trddays |&gt; \n  summarize(across(date, \\(x) all(!is.na(x))))\n\n# A tibble: 1 × 1\n  date \n  &lt;lgl&gt;\n1 TRUE \n\n\nWe can also confirm that we don’t need dateymd, as it contains the same information as date.\n\nsi_au_ref_trddays |&gt;\n  filter(dateymd != date) |&gt;\n  count() |&gt;\n  pull()\n\n[1] 0\n\n\nWe can specify - in col_types to omit dateymd when we read the data. Since date will be our primary key, we put that column first using the relocate() function.\n\nsi_au_ref_trddays &lt;- \n  read_csv(si_au_ref_trddays_csv, col_types = \"-iDii\") |&gt;\n  relocate(date)\n\nWe also confirm that dayssince simply represents the number of dates since 1899-12-30.\n\nsi_au_ref_trddays |&gt;\n  mutate(some_date = date - dayssince) |&gt; \n  count(some_date)\n\n# A tibble: 1 × 2\n  some_date      n\n  &lt;date&gt;     &lt;int&gt;\n1 1899-12-30  6326\n\n\nWe can also confirm that weekday represents the day of the week in the US system that starts the week on Sunday.21\n\nsi_au_ref_trddays |&gt;\n  mutate(\n    weekday_calc = wday(date),\n    wday = wday(date, label = TRUE)\n  ) |&gt;\n  count(weekday, weekday_calc, wday)\n\n# A tibble: 5 × 4\n  weekday weekday_calc wday      n\n    &lt;int&gt;        &lt;dbl&gt; &lt;ord&gt; &lt;int&gt;\n1       2            2 Mon    1215\n2       3            3 Tue    1278\n3       4            4 Wed    1285\n4       5            5 Thu    1285\n5       6            6 Fri    1263\n\n\n\nsi_au_ref_trddays &lt;-\n  read_csv(si_au_ref_trddays_csv,\n           col_types = \"-iDii\") |&gt;\n  relocate(date) |&gt;\n  write_parquet(sink = file.path(pq_dir, \"si_au_ref_trddays.parquet\")) |&gt;\n  system_time()\n\n   user  system elapsed \n  0.012   0.005   0.023"
  },
  {
    "objectID": "published/import_sirca.html#importing-si_au_retn_mkt",
    "href": "published/import_sirca.html#importing-si_au_retn_mkt",
    "title": "Data curation and the data science workflow",
    "section": "3.3 Importing si_au_retn_mkt",
    "text": "3.3 Importing si_au_retn_mkt\nWhile I omit the details here, I did confirm that much of what we saw with si_au_ref_trddays applies to si_au_retn_mkt:\n\nDate is a valid primary key\nDateYMD is redundant\nDaysSince represents the number of days since 1899-12-30\n\nAgain I convert all column names to lower case so that date is a common field across si_au_ref_trddays, si_au_retn_mkt, and si_au_prc_daily.\n\nsi_au_retn_mkt_csv &lt;- file.path(csv_dir, \"si_au_retn_mkt.csv.gz\")\n\nsi_au_retn_mkt &lt;-\n  read_csv(si_au_retn_mkt_csv,\n           col_types = \"-iDdddddd\",\n           locale = locale(date_format = \"%d/%m/%Y\"),\n           name_repair = str_to_lower) |&gt;\n  relocate(date) |&gt;\n  write_parquet(sink = file.path(pq_dir, \"si_au_retn_mkt.parquet\")) |&gt;\n  system_time()\n\n   user  system elapsed \n  0.010   0.004   0.011"
  },
  {
    "objectID": "published/import_sirca.html#importing-si_au_prc_daily",
    "href": "published/import_sirca.html#importing-si_au_prc_daily",
    "title": "Data curation and the data science workflow",
    "section": "3.4 Importing si_au_prc_daily",
    "text": "3.4 Importing si_au_prc_daily\nBy this point, we should be getting the hang of the workflow. I now move on to the largest file in the set, si_au_prc_daily.csv.gz. I start by identifying the CSV source and the parquet destination.\n\nsi_au_prc_daily_csv &lt;- file.path(csv_dir, \"si_au_prc_daily.csv.gz\")\nsi_au_prc_daily_pq &lt;- file.path(pq_dir, \"si_au_prc_daily.parquet\")\n\nUsing a process similar to that above, one can identify those columns needing special handling in the import process. Note that I specify guess_max = 1e6 because the default value for guess_max reads too few rows to infer the types of some variables that are mostly NA.\n\nsi_au_prc_daily &lt;-\n  read_csv(si_au_prc_daily_csv,\n           guess_max = 1e6,\n           show_col_types = FALSE) |&gt;\n  mutate(dateymd = ymd(dateymd),\n         date = dmy(date),\n         weekday = as.integer(weekday),\n         monthend = as.logical(monthend),\n         seniorsecurity = as.integer(seniorsecurity)) |&gt;\n  system_time()\n\n   user  system elapsed \n 71.659   7.355  44.749 \n\n\nAgain we need to choose between date and dateymd, which are almost always equal.\n\nsi_au_prc_daily |&gt; \n  filter(date != dateymd) |&gt; \n  select(gcode, securityticker, date, dateymd, dayssince)\n\n# A tibble: 0 × 5\n# ℹ 5 variables: gcode &lt;chr&gt;, securityticker &lt;chr&gt;, date &lt;date&gt;,\n#   dateymd &lt;date&gt;, dayssince &lt;dbl&gt;\n\n\nAgain dateymd seems to be the one of the two that is consistent with dayssince.\n\nsi_au_prc_daily |&gt; \n  filter(dayssince == 40682) |&gt;\n  count(date)\n\n# A tibble: 1 × 2\n  date           n\n  &lt;date&gt;     &lt;int&gt;\n1 2011-05-19  1513\n\nsi_au_prc_daily |&gt; \n  filter(dayssince == 40682) |&gt; \n  count(dateymd)\n\n# A tibble: 1 × 2\n  dateymd        n\n  &lt;date&gt;     &lt;int&gt;\n1 2011-05-19  1513\n\n\nSo in saving to parquet, I keep dateymd, but rename it to date for consistency across datasets.\n\nsi_au_prc_daily |&gt;\n  select(-date) |&gt;\n  rename(date = dateymd) |&gt;\n  write_parquet(sink = si_au_prc_daily_pq) |&gt;\n  system.time()\n\nOne issue with the code above is that it is quite slow and requires the full dataset to be loaded in RAM. Given that si_au_prc_daily occupies 4.31 GB of RAM when loaded, this can be a problem if you have modest computing resources.\nAn alternative approach would be to use DuckDB’s facility for reading CSV files and writing to parquet files. The small export_parquet() function accepts a remote data frame in a DuckDB connection and writes it to parquet.\n\nexport_parquet &lt;- function(df, file) {\n  db &lt;- df[[\"src\"]][[\"con\"]]\n  df &lt;- dplyr::collapse(df)\n  sql &lt;- paste0(\"COPY (\", dbplyr::remote_query(df),\n                \") TO '\", file, \"'\")\n  DBI::dbExecute(db, sql)\n  invisible(df)\n}\n\nThe following code creates a DuckDB connection, then uses that connection to read the CSV file and then calls export_parquet() to write it the data to a parquet file. This is an order of magnitude faster than the read_csv() code above, yet seems to make no demands on RAM.\n\ndb &lt;- dbConnect(duckdb::duckdb())\n\nsi_au_prc_daily &lt;-\n  tbl(db, str_c(\"read_csv('\", si_au_prc_daily_csv, \"',\n                    DateFormat = '%Y%m%d',\n                    types = {'dateymd': 'DATE',\n                             'dayssince': 'INTEGER',\n                             'weekday': 'INTEGER',\n                             'monthend': 'BOOLEAN',\n                             'seniorsecurity': 'INTEGER'})\")) |&gt;\n  compute() |&gt;\n  select(-date) |&gt;\n  rename(date = dateymd) |&gt;\n  export_parquet(file = si_au_prc_daily_pq) |&gt;\n  system_time()\n\n   user  system elapsed \n 38.239  17.863  13.844 \n\n\n\n3.4.1 Identifying the primary key\nObviously gcode and date are going to be part of any primary key, but one can quickly deduce from the documentation supplied by SIRCA that a single gcode can be associated with multiple securities at one time and that seniorsecurity is used to distinguish these. This suggests (gcode, date, seniorsecurity) as a candidate primary key, so let’s check this.\nFirst, does each combination of (gcode, date, seniorsecurity) identify a single row?\n\nsi_au_prc_daily |&gt; \n  count(gcode, date, seniorsecurity, name = \"num_rows\") |&gt; \n  count(num_rows) |&gt;\n  collect()\n\n# A tibble: 1 × 2\n  num_rows       n\n     &lt;dbl&gt;   &lt;dbl&gt;\n1        1 9050690\n\n\nSecond, are there no NA values in the (gcode, date, seniorsecurity) combination?\n\nsi_au_prc_daily |&gt; \n  summarize(across(c(gcode, date, seniorsecurity),\n                   \\(x) all(!is.na(x), na.rm = TRUE))) |&gt;\n  collect()\n\n# A tibble: 1 × 3\n  gcode date  seniorsecurity\n  &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt;         \n1 TRUE  TRUE  TRUE          \n\n\nOne thing to note here is that I checked the primary key using the DuckDB version of the data rather than the dplyr data frame (or tibble). One reason for this is that the code was much faster using the DuckDB version.\nNow that I am done with the DuckDB connection, I can disconnect from it.\n\ndbDisconnect(db)"
  },
  {
    "objectID": "published/import_sirca.html#the-final-script",
    "href": "published/import_sirca.html#the-final-script",
    "title": "Data curation and the data science workflow",
    "section": "3.5 The final script",
    "text": "3.5 The final script\nI organized the code above (e.g., removed redundant elements) and placed it in a script here. With the raw data in RAW_DATA_DIR and the necessary packages installed, I can create parquet data files by simply running the following code:22\n\nsource_url &lt;- str_c(\"https://raw.githubusercontent.com/\",\n                    \"iangow/notes/main/import_sirca.R\")\n\nSys.setenv(RAW_DATA_DIR = \"~/Dropbox/raw_data\")\nt &lt;- tempdir()\nSys.setenv(DATA_DIR = t)\nsource(source_url) |&gt; system.time()\n\n   user  system elapsed \n 30.117   1.444   6.321"
  },
  {
    "objectID": "published/import_sirca.html#storage-format-1",
    "href": "published/import_sirca.html#storage-format-1",
    "title": "Data curation and the data science workflow",
    "section": "4.1 Storage format",
    "text": "4.1 Storage format\nWe have chosen to use parquet files for our output. Table 4 provides some data on the parquet files we have produced for our hypothetical client (the Understand team). Assuming that the client is a group of colleagues at an institution with access to SIRCA, we (the Curate team) might just send a link to the Dropbox folder where we have stored the parquet files.\n\n\n\n\nTable 4: Data on processed parquet files\n\n\n\n\n\n\nfile_name\nsize\n\n\n\n\nsi_au_prc_daily.parquet\n517.71 MB\n\n\nsi_au_ref_names.parquet\n838.82 kB\n\n\nsi_au_ref_trddays.parquet\n86.75 kB\n\n\nsi_au_retn_mkt.parquet\n439.38 kB"
  },
  {
    "objectID": "published/import_sirca.html#primary-keys-1",
    "href": "published/import_sirca.html#primary-keys-1",
    "title": "Data curation and the data science workflow",
    "section": "4.2 Primary keys",
    "text": "4.2 Primary keys\nTable 5 provides a summary of our analysis above of primary keys.\n\n\n\nTable 5: SIRCA ASX EOD price collection: Primary keys\n\n\n\n\n\nTable\nPrimary key\n\n\n\n\nsi_au_ref_names\ngcode, securityticker, listdate\n\n\nsi_au_prc_daily\ngcode, date, seniorsecurity\n\n\nsi_au_retn_mkt\ndate\n\n\nsi_au_ref_trddays\ndate"
  },
  {
    "objectID": "published/import_sirca.html#good-database-principles-1",
    "href": "published/import_sirca.html#good-database-principles-1",
    "title": "Data curation and the data science workflow",
    "section": "4.3 Good database principles",
    "text": "4.3 Good database principles\nIn general, I think one wants to be fairly conservative in considering database principles with a data library. If the data are workable and make sense in the form they come in, then it may make most sense to keep them in that form.\nThe SIRCA ASX EOD data are organized into four tables with easy-to-understand primary keys and a fairly natural structure. At some level, the two primary tables are si_au_ref_names and si_au_prc_daily.23 These two tables are naturally distinct, with one about companies and the other about daily security returns.\nWhile there might be merit in splitting si_au_prc_daily into separate tables to reduce its size, it is actually quite manageable in its current form."
  },
  {
    "objectID": "published/import_sirca.html#no-manual-steps-1",
    "href": "published/import_sirca.html#no-manual-steps-1",
    "title": "Data curation and the data science workflow",
    "section": "4.4 No manual steps",
    "text": "4.4 No manual steps\nThere are no manual steps in creating the parquet files except for the initial download of the CSV files from SIRCA. While some data vendors allow users to download files using scripts (e.g., the scripts I have here for WRDS), this does not appear to be an option for SIRCA. But once the data have been downloaded, the subsequent steps are automatic.\nWhile some of the checks and data-cleaning had manual elements (e.g., identifying the near-duplicate with Gcode==\"rgwb1\" in si_au_ref_names), the resulting code implements the fix in an automated fashion. So long as the SIRCA data remain unchanged, the fix will continue to work."
  },
  {
    "objectID": "published/import_sirca.html#documentation-1",
    "href": "published/import_sirca.html#documentation-1",
    "title": "Data curation and the data science workflow",
    "section": "4.5 Documentation",
    "text": "4.5 Documentation\nA important principle here is that the code for processing the data is documentation in its own right. Beyond that the document you are reading now is a form of documentation. If the goal of this document were to provide details explaining the process used to produce the final datasets, then it might make sense to edit this document to reflect that different purpose, but in many ways I hope this document already acts as good documentation."
  },
  {
    "objectID": "published/import_sirca.html#update-process-1",
    "href": "published/import_sirca.html#update-process-1",
    "title": "Data curation and the data science workflow",
    "section": "4.6 Update process",
    "text": "4.6 Update process\nIn some ways, the update process is straightforward: when new CSV files become available, download them into RAW_DATA_DIR and run the script. However, it would probably be necessary to retrace some of the steps above to ensure that no data issues have crept in (e.g., duplicated keys). It may make sense to document the update process as part of performing it the first time."
  },
  {
    "objectID": "published/import_sirca.html#data-version-control-1",
    "href": "published/import_sirca.html#data-version-control-1",
    "title": "Data curation and the data science workflow",
    "section": "4.7 Data version control",
    "text": "4.7 Data version control\nI achieve a modest level of data version control by using Dropbox, which offers the ability to restore previous versions of data files. As discussed earlier, version control of data is a knotty problem."
  },
  {
    "objectID": "published/import_sirca.html#footnotes",
    "href": "published/import_sirca.html#footnotes",
    "title": "Data curation and the data science workflow",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis is a slightly edited version of a document originally made available in September 2025.↩︎\nThe terms “process” and “step” are my own concoctions here and represent an attempt to group certain things together. I use capitalized verbs to describe what I am calling processes and lower-case verbs to denote steps. The original “whole game” model has just a single step after the Understand process and I upgrade that single step to a process.↩︎\nThe authors of R for Data Science call the “whole game” a process, but I’ve already used that term to describe the next level down. So I choose workflow to denote the whole shebang here.↩︎\nI think it is interesting that R for Data Science places its part on transform before its part on import.↩︎\nWithin business schools, my sense is that Chicago offers faculty the best data curation support. While much of this occurs in a fairly decentralized fashion through the employment of research professionals who work closely with one or two faculty members, Chicago Booth also provides excellent research computing infrastructure, such as provisioning PostgreSQL databases for faculty use.↩︎\nMore recently I have moved to also downloading CRSP data as parquet files using my db2pq Python package.↩︎\nBy “manufacture” I merely mean to connote some notion of a production process, not some idea of untoward processes for producing data.↩︎\nI discuss some of the issues with Excel as a storage format below.↩︎\nOne project I worked on involved Python code analysing text and putting results in a PostgreSQL database and a couple of lines of code were sufficient for a co-author in a different city to load these data into Stata.↩︎\nSometimes there will be more than one primary key for a table.↩︎\nHere I used PostgreSQL data types, but the equivalent types in other formats should be fairly clear.↩︎\nSAS and Stata are somewhat loose with their “data types”. In effect SAS has just two data types—fixed-width character and floating-point numeric—and the other types are just formatting overlays over these. These types can be easily upset depending on how the data are used.↩︎\nAn example where this could happen is provided in Tip 1.↩︎\nBest practices here could be the subject of a separate note, as there are a lot of subtleties and my experience is that people often have bad habits here.↩︎\nExecute install.packages(c(\"tidyverse\", \"DBI\", \"duckdb\", \"arrow\", \"farr\")) within R to install all the packages you need to run the code in this note.↩︎\nUsing environment variables to specify RAW_DATA_DIR and DATA_DIR may not have an obvious payoff in the context of this note. The benefit comes more from follow-on work using the data and also from applying the approach to managing raw data more broadly.↩︎\nWhoever did this also had their computer set to format dates in the US-style Mmm-dd format, rather than the dd-Mmm style I see on my computer.↩︎\nYou can download this CSV file here.↩︎\nSee Broman and Woo (2018) for further discussion of some of the issues with using Excel for data science.↩︎\nSee the help for read_csv() to learn more.↩︎\nThe ISO 8601 convention is more consistent with the idea that Sunday is at the end of the week—hence “weekend”—and starts the week on Monday. But these distinctions are not important here.↩︎\nNote that I set DATA_DIR to a different directory to avoid overwriting the files I just created and creating problems with Dropbox having to sync new files before it’s even uploaded old ones.↩︎\nIt seems possible that si_au_retn_mkt and si_au_ref_trddays are generated from si_au_prc_daily.↩︎"
  },
  {
    "objectID": "published/getting_dera_notes.html",
    "href": "published/getting_dera_notes.html",
    "title": "Getting SEC EDGAR XBRL data",
    "section": "",
    "text": "In a recent note, I used XBRL data to identify potentially missing Form AP filings. In writing that note, I used two data sources: SEC EDGAR for the XBRL data and the PCAOB website for the Form AP data. However, I provided no real information on how to get the XBRL data from SEC EDGAR. This note aims to provide this missing information.1\nThis note was written using Quarto and compiled with RStudio, an integrated development environment (IDE) for working with R. The source code for this note is available here and the latest version of this PDF is here.\nThis note uses the following R packages:2\nlibrary(tidyverse)\nlibrary(DBI)\nlibrary(farr)\nlibrary(httr2)  \nlibrary(rvest)\nlibrary(arrow)"
  },
  {
    "objectID": "published/getting_dera_notes.html#getting-financial-statement-and-notes-files",
    "href": "published/getting_dera_notes.html#getting-financial-statement-and-notes-files",
    "title": "Getting SEC EDGAR XBRL data",
    "section": "1 Getting Financial Statement and Notes files",
    "text": "1 Getting Financial Statement and Notes files\nThere are two XBRL bulk data sets made available on SEC EDGAR: the Financial Statements and Financial Statement and Notes data sets, with the latter being roughly ten times as large as the former. For the task considered in the note discussed above, we needed the Financial Statement and Notes data set, so I focus on that data set here.\n\n1.1 Structure of processed data\nThe Financial Statement and Notes data library comprises seven tables:\n\ntag contains all standard taxonomy tags (not just those appearing in submissions to date) and all custom taxonomy tags defined in the submissions. The standard tags are derived from taxonomies in the SEC’s standard taxonomies file as of the date of submission.\ndim contains all of the combinations of XBRL axis and member used to tag any submission.\nnum contains numeric data, one row per data point in the financial statements.\ntxt contains non-numeric data, one row per data point in the financial statements.\nren summarizes for each filing the data provided by filers about each presentation group as defined in EDGAR filer manual.\npre contains one row for each line of the financial statements tagged by the filer.\ncal contains one row for each calculation relationship (“arc”). Note that XBRL allows a parent element to have more than one distinct set of arcs for a given parent element, thus the rationale for distinct fields for the group and the arc.3\n\n\n\n1.2 Structure of unprocessed data\nIf you visit the Financial Statement and Notes site, you will see something like the table partially seen in Figure 1. This table provides links to many ZIP files. The last year or so of data are found in monthly data files and earlier periods are found in quarterly data files. Each data file is found using a link provided in the table.\n\n\n\n\n\n\nFigure 1: Financial Statement and Notes website\n\n\n\nI start with the 2024_10 file, the link to which points to a file named 2024_10_notes.zip. We can download that file and extract its contents, which are depicted in Figure 2. It seems that each of the data tables discussed above is found in an eponymous .tsv file.\n\n\n\n\n\n\nFigure 2: Contents of 2024_10_notes.zip\n\n\n\nI start with sub.tsv and I repeat the download steps for the .zip file programmatically. To programmatically download data from SEC EDGAR, you will need to set HTTPUserAgent to your email address by running code like the following in R.\n\noptions(HTTPUserAgent = \"your_name@email_provider.com\")\n\nWhile we are on the topic of setting variables that are user-specific, we will later store data in a subdirectory of a directory that is identified by the environment variable DATA_DIR. I set DATA_DIR to a folder named pq_data inside my Dropbox location. You should run the following code but with a destination that is convenient for you.\n\nSys.setenv(DATA_DIR = \"~/Dropbox/pq_data\")\n\nHaving set HTTPUserAgent, I begin by downloading the file for October 2024.\n\nfile &lt;- \"2024_10_notes.zip\"\nurl &lt;- str_c(\"https://www.sec.gov/files/dera/data/\",\n             \"financial-statement-notes-data-sets/\", file)\nt &lt;- \"../data/2024_10_notes.zip\"\ndownload.file(url, t)\n\nWe can start by simply applying read_tsv() to this file.4\n\nsub &lt;- read_tsv(unz(t, \"sub.tsv\"))\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nRows: 7117 Columns: 40\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr  (26): adsh, name, sic, countryba, stprba, cityba, zipba, bas1, bas2, ba...\ndbl  (12): cik, changed, wksi, period, fy, filed, prevrpt, detail, nciks, pu...\nlgl   (1): floataxis\ndttm  (1): accepted\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nAlas, we see problems. What’s the cause? Let’s follow the prompt and use problems() to investigate.\n\nproblems(sub)\n\n# A tibble: 1 × 5\n    row   col expected           actual       file \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;              &lt;chr&gt;        &lt;chr&gt;\n1  1620    39 1/0/T/F/TRUE/FALSE ClassOfStock \"\"   \n\n\nIt seems that read_tsv() guessed that column 39 is a logical variable (i.e., TRUE or FALSE), which is inconsistent with the value \"ClassOfStock\" observed in row 1620. Maybe setting guess_max to a higher value will help.\n\nsub &lt;- read_tsv(unz(t, \"sub.tsv\"), guess_max = 10000)\n\nRows: 7117 Columns: 40\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \"\\t\"\nchr  (27): adsh, name, sic, countryba, stprba, cityba, zipba, bas1, bas2, ba...\ndbl  (12): cik, changed, wksi, period, fy, filed, prevrpt, detail, nciks, pu...\ndttm  (1): accepted\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nOK, no problems now. What are the types of each column? Here I apply a small function first_class() to sub to find out.5\n\nfirst_class &lt;- function(x) {\n  class(x)[[1]]\n}\n\nunlist(map(sub, first_class))\n\n       adsh         cik        name         sic   countryba      stprba \n\"character\"   \"numeric\" \"character\" \"character\" \"character\" \"character\" \n     cityba       zipba        bas1        bas2        baph   countryma \n\"character\" \"character\" \"character\" \"character\" \"character\" \"character\" \n     stprma      cityma       zipma        mas1        mas2  countryinc \n\"character\" \"character\" \"character\" \"character\" \"character\" \"character\" \n    stprinc         ein      former     changed         afs        wksi \n\"character\" \"character\" \"character\"   \"numeric\" \"character\"   \"numeric\" \n        fye        form      period          fy          fp       filed \n\"character\" \"character\"   \"numeric\"   \"numeric\" \"character\"   \"numeric\" \n   accepted     prevrpt      detail    instance       nciks       aciks \n  \"POSIXct\"   \"numeric\"   \"numeric\" \"character\"   \"numeric\" \"character\" \npubfloatusd   floatdate   floataxis   floatmems \n  \"numeric\"   \"numeric\" \"character\"   \"numeric\" \n\ntable(unlist(map(sub, first_class)))\n\n\ncharacter   numeric   POSIXct \n       27        12         1 \n\n\nWhile most columns are either character or numeric, the accepted column is read as a date-time (POSIXct).\nThe read_tsv() function has a col_types argument that allows us to “use a compact string representation where each character represents one column” as follows:\n\nc = character\ni = integer\nn = number\nd = double\nl = logical\nf = factor\nD = date\nT = date time\nt = time\n? = guess\n_ or - = skip\n\nThe following get_coltypes_str() function creates a string that we can use to specify column types when calling read_tsv().6\n\nget_coltypes_str &lt;- function(df) {\n  type_to_str &lt;- function(col) {\n    case_when(col == \"character\" ~ \"c\",\n              col == \"logical\" ~ \"l\",\n              col == \"numeric\" ~ \"d\",\n              col == \"POSIXct\" ~ \"T\",\n              .default = \"c\")\n  }\n\n  res &lt;-\n    tibble(type = unlist(map(sub, first_class))) |&gt;\n    mutate(col_type = type_to_str(type))\n\n  paste(res$col_type, collapse = \"\")\n}\n\nget_coltypes_str(sub)\n\n[1] \"cdcccccccccccccccccccdcdccddcdTddcdcddcd\"\n\n\nEven though read_tsv() is able to guess most types, it is generally best to look at the data. In this case, we can see that four columns are actually dates coded as numbers of the form yyyymmdd.\n\nsub |&gt;\n  select(changed, filed, period, floatdate) |&gt;\n  arrange(floatdate)\n\n# A tibble: 7,117 × 4\n    changed    filed   period floatdate\n      &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1 20050502 20241004 20221231  20220630\n 2 20080808 20241002 20240531  20221130\n 3 20220308 20241028 20231231  20221231\n 4       NA 20241009 20240731  20230131\n 5 20101025 20241029 20230731  20230131\n 6       NA 20241029 20240731  20230131\n 7 20120910 20241029 20240731  20230131\n 8 20101025 20241031 20230731  20230131\n 9 20030416 20241029 20230930  20230331\n10 20001117 20241002 20231231  20230630\n# ℹ 7,107 more rows\n\n\nIn the following code, I use ymd() to convert these four variables into dates. I also read accepted initially as a character variable and use ymd_hms() from the lubridate package to convert it to a date-time.7\n\nsub &lt;- \n  read_tsv(unz(t, \"sub.tsv\"),\n           col_types = \"cdcccccccccccccccccccdcdccddcdcddcdcddcd\") |&gt;\n   mutate(across(c(changed, filed, period, floatdate), ymd),\n           across(accepted, ymd_hms))\n\nFinally I create a DuckDB instance and copy the data frame sub to DuckDB, giving it the name sub_notes.\n\ndb &lt;- dbConnect(duckdb::duckdb())\nsub |&gt;\n  copy_to(db, df = _, name = \"sub_notes\", overwrite = TRUE)\n\nFinally, I create a parquet file by exporting the data from the DuckDB table I just created. I then disconnect from the database, as I no longer need it.\n\nperiod &lt;- str_replace(basename(t), \"^(.*)_notes.*$\", \"\\\\1\")\npq_dir &lt;- file.path(Sys.getenv(\"DATA_DIR\"), \"dera_notes\")\npq_file &lt;- file.path(pq_dir, str_c(\"sub_notes_\", period, \".parquet\"))\ndbExecute(db, str_c(\"COPY sub_notes TO '\", pq_file, \"'\"))\n\n[1] 7117\n\ndbDisconnect(db)\n\nI then do similar work for the remaining tables (dim, num, txt, ren, pre, and cal). I then put all of this inside a function get_notes_data(file) that downloads a .zip file and creates parquet files for each table. I can load this function by running the following code:\n\nsource(str_c(\"https://raw.githubusercontent.com/iangow/\",\n             \"notes/refs/heads/main/published/get_dera_functions.R\"))\n\nThis code also loads the function get_zip_files_df() that can be used to get the list of .zip files shown on SEC website.\n\nzip_files &lt;- get_zip_files_df()\nzip_files\n\n# A tibble: 77 × 2\n   file              last_modified                \n   &lt;chr&gt;             &lt;chr&gt;                        \n 1 2026_01_notes.zip Wed, 04 Feb 2026 17:27:33 GMT\n 2 2025_12_notes.zip Wed, 14 Jan 2026 20:59:24 GMT\n 3 2025_11_notes.zip Thu, 15 Jan 2026 14:09:07 GMT\n 4 2025_10_notes.zip Mon, 01 Dec 2025 14:48:45 GMT\n 5 2025_09_notes.zip Tue, 18 Nov 2025 21:32:32 GMT\n 6 2025_08_notes.zip Wed, 03 Sep 2025 16:10:15 GMT\n 7 2025_07_notes.zip Tue, 05 Aug 2025 16:57:51 GMT\n 8 2025_06_notes.zip Thu, 03 Jul 2025 18:46:28 GMT\n 9 2025_05_notes.zip Tue, 03 Jun 2025 11:09:16 GMT\n10 2025_04_notes.zip Wed, 07 May 2025 17:47:37 GMT\n# ℹ 67 more rows\n\n\nNext, I can apply the function get_notes_data() to each file in zip_files using map():\n\nmap(zip_files$file, get_notes_data)\n\nDoing this takes me a bit under 38 minutes.8 The resulting files take up about 39 GB of space, likely representing about 10 times that in terms of raw data due to compression.\n\n\n1.3 Doing incremental updates\nWhile 38 minutes is a reasonable amount of time to download hundreds of gigabytes of data, it is not something that we would want to repeat on a regular basis. The astute reader will note that the last_modified field of zip_files contains information on the date on which the applicable file was modified. It seems we could use this information to limit ourselves to files that have been added or modified since we last updated the data.\nIn the past I have use three different approaches to this kind of problem:\n\nStoring last_modified data in the metadata of parquet files containing the data.\nModifying the file properties of the data file to match the last_modified data.\nSaving a table containing last_modified data that can be compared with the current data to identify files that need to be downloaded.\n\nOf these three approaches, the first is probably the most robust because the last_modified information is part of the parquet file itself. I use this first approach in wrds_update_pq() in two Python packages, wrds2pg and db2pq. The second approach also collocates the information with the file, but is perhaps a little less robust. I use this approach in wrds_update_csv() in wrds2pg because the output files are CSV files where there is no place to store metadata.\nHere I will use the third approach just because it is simpler. However it is a little less robust. For example, if the download process is interrupted or the data files are moved around, the value of a directory-level file with last_modified might be limited.\nI start by loading a file called last_modified.parquet in the parquet data directory if one exists. The first time you run the code, there will be no such file and I create an empty data frame last_modified in that case.\n\npq_dir &lt;- file.path(Sys.getenv(\"DATA_DIR\"), \"dera_notes\")\npq_path &lt;- file.path(pq_dir, \"last_modified.parquet\")\n\nif (file.exists(pq_path)) {\n  last_modified &lt;- arrow::read_parquet(pq_path)\n} else {\n  last_modified &lt;- tibble(file = NA, last_modified = NA)\n}\n\nI then compare zip_files with last_modified to identify files on SEC EDGAR with a different modification date from that recorded in last_modified. These are the files that we will want to download and we store the list of such files in the data frame to_update.\n\nto_update &lt;-\n  zip_files |&gt;\n  left_join(last_modified,\n            by = \"file\",\n            suffix = c(\"_new\", \"_old\")) |&gt;\n  filter(is.na(last_modified_old) |\n           last_modified_new != last_modified_old)\n\nNow I can apply get_notes_data() to the files in to_update.\n\nmap(to_update$file, get_notes_data)\n\nHaving updated the files, we now save the data in zip_files as the new copy of last_updated. This new last_updated.parquet will be used the next time we update the data.\n\nsave_parquet &lt;- function(df, name) {\n  file_path &lt;- file.path(pq_dir, paste0(name, \".parquet\"))\n  arrow::write_parquet(df, sink = file_path)\n}\n\nzip_files |&gt;\n  save_parquet(name = \"last_modified\")\n\nAccording to the SEC EDGAR website, “effective March 2024, monthly data sets will be consolidated into quarterly files after a year, so that only a year of monthly files will be available at a time.” This will mean that monthly files will become obsolete after about a year and presumably need to be deleted to avoid duplicating data in quarterly files. A subsequent update to this note will discuss how we can identify and delete obsolete files."
  },
  {
    "objectID": "published/getting_dera_notes.html#using-financial-statement-and-notes-data",
    "href": "published/getting_dera_notes.html#using-financial-statement-and-notes-data",
    "title": "Getting SEC EDGAR XBRL data",
    "section": "2 Using Financial Statement and Notes data",
    "text": "2 Using Financial Statement and Notes data\nNow that we have downloaded the data, we can access it quite easily using DuckDB and the load_parquet() function from the farr library.9 Note that while the tables are split across several files, these are easily combined using wildcards in DuckDB. For example, sub_notes_* can be used to refer to all files that make up the submission data (sub table). As can be seen, working with parquet files using DuckDB is generally very fast.\n\ndb &lt;- dbConnect(duckdb::duckdb())\n\nsub &lt;- load_parquet(db, \"sub_notes_*\", schema = \"dera_notes\")\n\nsub |&gt;\n  mutate(year = year(filed)) |&gt;\n  count(year) |&gt; \n  arrange(desc(year)) |&gt;\n  collect() |&gt;\n  system_time()\n\n   user  system elapsed \n  0.042   0.019   0.035 \n\n\n# A tibble: 18 × 2\n    year      n\n   &lt;dbl&gt;  &lt;dbl&gt;\n 1  2026   5428\n 2  2025  99208\n 3  2024 194501\n 4  2023 104015\n 5  2022  97814\n 6  2021  82740\n 7  2020  60923\n 8  2019  35040\n 9  2018  26396\n10  2017  26557\n11  2016  34431\n12  2015  29906\n13  2014  31219\n14  2013  31798\n15  2012  32755\n16  2011  18337\n17  2010   3914\n18  2009    951"
  },
  {
    "objectID": "published/getting_dera_notes.html#footnotes",
    "href": "published/getting_dera_notes.html#footnotes",
    "title": "Getting SEC EDGAR XBRL data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nGuidance on downloading the Form AP data is provided in an earlier note I wrote.↩︎\nTo install these packages, run install.packages(c(tidyverse, \"DBI\", \"farr\", \"httr2\", \"rvest\", \"arrow\") in the console of RStudio.↩︎\nRun source(\"https://raw.githubusercontent.com/iangow/notes/refs/heads/main/get_dera_notes.R\") to get these data.↩︎\nUsing unz(t, \"sub.tsv\") allows us to unzip just that one file in a way that does not leave detritus in our file system.↩︎\nI use first_class() to get just the first class for each column as one column has two classes associated with it. You can see this by running unlist(map, sub, class)) and comparing the output with that from the code I use below.↩︎\nThis function only handles a subset of the types that might be identified by read_tsv(), but it suffices for current purposes.↩︎\nI do not recall why I chose this option, but it may have been that the automatic type detection and conversion did not work with all files and setting it explicitly works best.↩︎\nObviously the time taken will depend on the speed of your internet connection and your “distance” from the SEC EDGAR server.↩︎\nThe farr package was originally created to supplement the book by me and Tony Ding, Empirical Research in Accounting: Tools and Methods.↩︎"
  },
  {
    "objectID": "published/delisting.html",
    "href": "published/delisting.html",
    "title": "Adding delisting returns to monthly data",
    "section": "",
    "text": "This short note demonstrates how to convert SAS code provided as delistings.sas by Richard Price here. This code is presumably closely related to code used in Beaver et al. (2007)."
  },
  {
    "objectID": "published/delisting.html#introduction",
    "href": "published/delisting.html#introduction",
    "title": "Adding delisting returns to monthly data",
    "section": "",
    "text": "This short note demonstrates how to convert SAS code provided as delistings.sas by Richard Price here. This code is presumably closely related to code used in Beaver et al. (2007)."
  },
  {
    "objectID": "published/delisting.html#setting-up-tables",
    "href": "published/delisting.html#setting-up-tables",
    "title": "Adding delisting returns to monthly data",
    "section": "2 Setting up tables",
    "text": "2 Setting up tables\nIn the R code that folows, we will use a number of packages, including pacakges to connect to a PostgreSQL database containing WRDS data. Instructions for this can be found here.1\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(broom)\nlibrary(DBI)\nlibrary(dbplyr)\nlibrary(purrr)\n\n\npg &lt;- dbConnect(RPostgres::Postgres(), bigint = \"integer\")\n\nWe will use three tables. The data on “regular” returns come from crsp.msf. The tables crsp.mse and crsp.dsedelist are used for delisting returns.\n\ncrsp.msf &lt;- tbl(pg, sql(\"SELECT * FROM crsp.msf\"))\ncrsp.mse &lt;- tbl(pg, sql(\"SELECT * FROM crsp.mse\"))\ncrsp.dsedelist &lt;- tbl(pg, sql(\"SELECT * FROM crsp.dsedelist\"))"
  },
  {
    "objectID": "published/delisting.html#the-dataset-with-monthly-return-data",
    "href": "published/delisting.html#the-dataset-with-monthly-return-data",
    "title": "Adding delisting returns to monthly data",
    "section": "3 The dataset with monthly return data",
    "text": "3 The dataset with monthly return data\nHere we get a table of “regular” returns, we will incorporate delisting returns in the returns from this table. The code includes “an arbitrary restriction of the sample for illustration purposes.” The data are restricted to 2003 and permno values less than 12000.\nproc sql;\n    create table monthlyreturns as\n    select permno, date, ret\n    from crsp.msf\n    where year(date)=2003 and permno&lt;12000;  \n\nmonthlyreturns &lt;-\n  crsp.msf %&gt;%\n  filter(year(date) == 2003, permno &lt; 12000) %&gt;%\n    select(permno, date, ret)"
  },
  {
    "objectID": "published/delisting.html#the-monthly-delisting-dataset",
    "href": "published/delisting.html#the-monthly-delisting-dataset",
    "title": "Adding delisting returns to monthly data",
    "section": "4 The monthly delisting dataset",
    "text": "4 The monthly delisting dataset\n\n4.1 Get the base data for delisting returns\nThe following code uses crsp.mse for the values of dlret. It’s not clear what this table provides that crsp.dsedelist does not, but using crsp.dsedelist here—renaming dlstdt as date to conform with later code—results in small differences in the results below.\nThe first portion of SAS code is easily translated to R.\ndata delist;\n  set crsp.mse;\n  where dlstcd &gt; 199;\n  keep permno date dlstcd dlpdt dlret;\nrun;\n\ndelist &lt;-\n  crsp.mse %&gt;%\n  filter(dlstcd &gt; 199) %&gt;%\n  select(permno, date, dlstcd, dlpdt, dlret)\n\n\n\n4.2 Calculate replacement values\nCompute replacement values for missing delisting returns using daily delisting returns. Richard says modify year range as needed, but I drop this filter in the R code, as it makes little difference to performance and (surprisingly) no difference to the results.\nproc sql;\n  create table rvtemp as\n    select * from crsp.dsedelist\n    where dlstcd &gt; 199 and 1960 le year(DATE) le 2020\n    order by dlstcd;\nI omit the step of creating rvtemp, as it’s easy enough to include a single line of code in creating rv below.\nThe following code calculates the mean values of dlret by dlstcd. Later will use these values to fill missing values of dlret. Richard says in a comment “could use median=median_dlret and probm=median_pvalue if you do not like mean delisting returns as the replacement value.”\nproc univariate data=rvtemp noprint;\n    var dlret;\n    output out=rv mean=mean_dlret probt=mean_pvalue;\n    by dlstcd;\nrun;\n\n* require replacement values to be statistically significant;\ndata rv;\n    set rv;\n      * adjust p-value as desired;\n    if mean_pvalue le 0.05 then rv = mean_dlret; \n    else rv = 0; * adjust as desired;\n    keep dlstcd rv;\nrun;\nThe SAS code uses PROC UNIVARIATE. We just need a small function to return the \\(p\\)-value, which I call prt() to match the name of a similar function in SAS. We could use the t.test() function, but it’s easy enough to calculate the two-sided \\(p\\)-value ourselves.2\n\nprt &lt;- function(x) {\n  p &lt;- pt(mean(x) * sqrt(length(x)) / sd(x), length(x))\n  2 * pmin(p, 1 - p)\n}\n\nAs disussed I use filter(dlstcd &gt; 199) without between(year(dlstdt), 1960, 2020) because it makes no noticeable difference to performance or results. I call the resulting table rvs (“replacement values”) to distinguish it from the variable rv it contains. This makes no functional difference, but (to my mind) makes the code a little easier to read. Note that we bring data from PostgreSQL into R to calculate p_val and then return it to PostgreSQL using copy_inline(). An alternative might be to calculate \\(t\\)-statistics in PostgreSQL and just bring summary values into R to calculate \\(p\\)-values, but this approach works fine.\n\nrvs &lt;-\n  crsp.dsedelist %&gt;%\n  filter(dlstcd &gt; 199) %&gt;%\n  select(dlstcd, dlret) %&gt;%\n  filter(!is.na(dlret)) %&gt;%\n  collect() %&gt;% \n  group_by(dlstcd) %&gt;%\n  summarize(mean = mean(dlret),\n            p_val = prt(dlret),\n            .groups = \"drop\") %&gt;%\n  mutate(rv = if_else(p_val &lt;= 0.05, mean, 0)) %&gt;%\n  select(dlstcd, rv) %&gt;%\n  copy_inline(pg, .)\n\n\n\n4.3 Merge replacement values with delisting returns\nAgain the R code is a simple translation of the SAS code. One difference here is that I use a new table name delist_rv, as I find re-using table names to be confusing when debugging code (though no real debugging was required here because I am mimicking someone else’s code). Note that I add month and year variables, as we will use these to merge below data sets below.\nproc sql;\n    create table delist as\n    select a.*, b.rv\n    from delist a left join rv b\n    on a.dlstcd = b.dlstcd;\n\ndelist_rv &lt;- \n  delist %&gt;%\n  left_join(rvs, by = \"dlstcd\") %&gt;%\n  mutate(month = month(date),\n         year = year(date)) %&gt;%\n  rename(dldate = date)\n\n\n\n4.4 Creating a function\nOf course, all of the above could be easily be put into a function. Making functions in R is much easier than making macros in SAS.3 Note that if using the function, we could omit the code creating crsp.mse and crsp.dsedelist above. Optionally, we could easily use dates taken from crsp.msi in the returned table that we could merge with monthly returns without any need to create month and year fields. Ideally, we would also incorporate the steps to correct dlret covered in the next section in the code here.\nIt would be quite straightforward to add this function to my farr package.4 The only thing needed to be provided to the function is the PostgreSQL database connection (conn).\n\nget_delist &lt;- function(conn) {\n  crsp.mse &lt;- tbl(conn, sql(\"SELECT * FROM crsp.mse\"))\n  crsp.dsedelist &lt;- tbl(conn, sql(\"SELECT * FROM crsp.dsedelist\"))\n  \n  delist &lt;-\n    crsp.mse %&gt;%\n    filter(dlstcd &gt; 199) %&gt;%\n    select(permno, date, dlstcd, dlpdt, dlret)\n\n  prt &lt;- function(x) {\n    p &lt;- pt(mean(x) * sqrt(length(x)) / sd(x), length(x))\n    2 * pmin(p, 1 - p)\n  }\n  \n  rvs &lt;-\n    crsp.dsedelist %&gt;%\n    filter(dlstcd &gt; 199) %&gt;%\n    select(dlstcd, dlret) %&gt;%\n    filter(!is.na(dlret)) %&gt;%\n    collect() %&gt;% \n    group_by(dlstcd) %&gt;%\n    summarize(mean = mean(dlret),\n              p_val = prt(dlret),\n              .groups = \"drop\") %&gt;%\n    mutate(rv = if_else(p_val &lt;= 0.05, mean, 0)) %&gt;%\n    select(dlstcd, rv) %&gt;%\n    copy_inline(conn, .)\n  \n  delist %&gt;%\n    left_join(rvs, by = \"dlstcd\") %&gt;%\n    mutate(month = month(date),\n           year = year(date)) %&gt;%\n    rename(dldate = date)\n}\n\nSo we can replace the delist_rv data table we created above with one produced by the get_delist() function.\n\ndelist_rv &lt;- get_delist(pg)"
  },
  {
    "objectID": "published/delisting.html#merge-monthly-returns-with-delisting-data",
    "href": "published/delisting.html#merge-monthly-returns-with-delisting-data",
    "title": "Adding delisting returns to monthly data",
    "section": "5 Merge monthly returns with delisting data",
    "text": "5 Merge monthly returns with delisting data\nTranslating the SAS code here is pretty straightforward. The SAS code involves PROC SQL followed by a data step, but I do it all in one series of pipes.\nproc sql;\n  create table monthlyreturns as\n      select a.*, b.dlret, b.dlstcd, b.rv, b.date as dldate, b.dlpdt\n      from monthlyreturns a left join delist b\n      on (a.permno = b.permno)\n      and (month(a.date)= month(b.date))\n      and (year(a.date) = year(b.date));\nquit;\n      \ndata monthlyreturns;\n    set monthlyreturns;\n    ret_orig = ret;\n  \n    if not missing(dlstcd) and missing(dlret) then dlret=rv;\n    else if not missing(dlstcd) and dlpdt le dldate and not missing(dlret)\n        then dlret=(1+dlret)*(1+rv)-1;\n\n    ** Then, incorporate delistings into monthly return measure;\n    if not missing(dlstcd) and missing(ret) then ret=dlret;\n    else if not missing(dlstcd) and not missing(ret) \n        then ret=(1+ret)*(1+dlret)-1;\nrun;\nThe code below first uses replacement values where necessary (is.na(dlret)). Note, this will happen when the delisting occurs on the last day of the month and ret is not missing, but the delisting return is unknown. If the delisting return is a partial month return, CRSP flags it by setting dlpdt to a date less than or equal to the delisting date. Richard says that one could use a single replacement value as in Shumway (1997) (\\(-0.35\\)) or Sloan (1996) (\\(-1.0\\)) and that he would only do single replacement value for a subset of delisting codes &gt; 499.\nAgain I use a new name (monthlyreturns_delist) for the resulting table, as I prefer not to reuse table names.\n\nmonthlyreturns_delist &lt;-\n  monthlyreturns %&gt;%\n  mutate(month = month(date),\n         year = year(date)) %&gt;%\n  left_join(delist_rv, join_by(permno, month, year)) %&gt;%\n  mutate(ret_orig = ret,\n         dlret = case_when(!is.na(dlstcd) & is.na(dlret) ~ rv,\n                           !is.na(dlstcd) & dlpdt &lt; dldate & !is.na(dlret) ~ \n                             (1 + dlret) * (1 + rv) - 1,\n                           .default = dlret),\n         ret = case_when(!is.na(dlstcd) & is.na(ret) ~ dlret,\n                         !is.na(dlstcd) & !is.na(ret) ~ \n                           (1 + ret)*(1 + dlret) - 1,\n                         .default = ret)) %&gt;%\n  collect()\n\n\n5.1 Comparison of output with that of SAS code\nRichard includes output from SAS’s PROC MEANS that we can use to compare our results with his. Comparing the numbers in Table 1 with Richard’s output confirms that our R code has done the same thing as his.\n\n\n\n\nTable 1: Summary statistics for monthly returns with delisting returns\n\n\n\n\n\n\nN\nMean\nStd Dev\nMinimum\nMaximum\n\n\n\n\n3859\n0.0418905\n0.1820073\n-0.9913043\n5.1785717\n\n\n3842\n0.0429354\n0.1800152\n-0.5890411\n5.1785717\n\n\n20\n-0.1684432\n0.3543438\n-0.9913043\n0.3333333\n\n\n\n\n\n\n\n\n\n\n5.2 Performance\nRunning the SAS code above takes between 17 and 20 seconds on the WRDS server. The R code takes 9 seconds using a database on my laptop, and about 20 seconds on the same laptop, but using the remote WRDS database.5\nI would call this performance comparison in R’s favour because the R code is also generating the PDF document you are reading, which takes a few seconds. If using the SAS code, you would likely also need to add time to retrieve the data from the WRDS server if you are running analysis on a local computer."
  },
  {
    "objectID": "published/delisting.html#footnotes",
    "href": "published/delisting.html#footnotes",
    "title": "Adding delisting returns to monthly data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYou don’t need to install all the packages listed there, just the ones listed below plus RPostgres.↩︎\nIt’s probably a good thing to do this “by hand” just so you remember your statistics. Of course I checked that I got the same answer as t.test().↩︎\nI don’t think you’d cover macros in a first SAS class, but I cover making functions here.↩︎\nOnce I understand this code better, I may do this, as it would be good to use returns with delisting returns in the chapter replicating Sloan [1996].↩︎\nRunning the code above actually takes 27 seconds, but that’s because I unnecessarily create delist_rv twice. This is the portion of code that takes longer using a remote database because of the need to pull data into R.↩︎"
  },
  {
    "objectID": "published/datetimes.html",
    "href": "published/datetimes.html",
    "title": "Working with date and times",
    "section": "",
    "text": "The purpose of this note is address the topic of temporal data (dates and times) in more detail than found in the “Dates and Times” chapter of R for Data Science (Wickham et al., 2023) and to provide a hands-on application of working with them. The application I work with is based on the SEC submissions data I discussed in another recent note. I study this application not only using R, but also DuckDB and PostgreSQL, as moving data between systems can be a pain point, especially with date-times.\nAs discussed in a recent post on LinkedIn, one goal of Empirical Research in Accounting: Tools and Methods (Gow and Ding, 2024) is to provide a pathway to mastery of the contents of Wickham et al. (2023).1 That said, I identified a few gaps, including spreadsheets, hierarchical data, and dates and times. One recent note covered hierarchical data and a forthcoming note will address spreadsheets.\nThe purpose of this note is address the gap regarding dates and times in more detail—including a hands-on application—than found in the “Dates and Times” chapter of Wickham et al. (2023). The application I work with is based on the SEC submissions data I discussed in another recent note. I study this application not only using R, but also DuckDB and PostgreSQL, as moving data between systems can be a pain point, especially with date-times.\nIn writing this note, I use the packages listed below.2 This note was written using Quarto and compiled with RStudio, an integrated development environment (IDE) for working with R. The source code for this note is available here.\nlibrary(tidyverse)\nlibrary(DBI)\nlibrary(farr)\nlibrary(scales)"
  },
  {
    "objectID": "published/datetimes.html#dates",
    "href": "published/datetimes.html#dates",
    "title": "Working with date and times",
    "section": "Dates",
    "text": "Dates\nIn some respects, dates are relatively straightforward. A constant bane of data analysts is the representation of dates in ambiguous formats. For example, in many parts of the world \"11/12/2023\" represents 11 December 2023; in other parts of the world (read “the United States of America”), \"11/12/2023\" represents November 12, 2023. Converting string representations of dates to dates is an important element of the data curation process.4 R for Data Science provides an excellent discussion of issues that arise in converting string data to dates (Wickham et al., 2023, pp. 296–302).\nWe might also encounter dates in data sets from other programs, such as SAS, Stata, or Excel. Functions such as read_sas(), read_stata(), and read_excel() will generally detect and convert dates. Otherwise exporting data from those programs to text with dates represented in an unambiguous text format is perhaps the best approach. For example, in my wrds2pg Python package, I apply SAS code that uses format=YYMMDD10. to represent dates using the ISO 8601 format discussed in R for Data Science before exporting to CSV using SAS’s PROC EXPORT command.5\nDates are generally transferred without any issues from databases such as DuckDB or PostgreSQL to R or vice versa. Once properly encoded as type Date, dates do not present particular difficulties in R in many contexts. The as.Date() function converts an ISO 8601 string to a date (i.e., an object with class class Date in R):\n\na_date &lt;- as.Date(\"2024-05-28\")\nclass(a_date)\n\n[1] \"Date\"\n\n\nFunctions provided by the lubridate package (part of the core Tidyverse) allow us to extract information about dates, such as the year …\n\nyear(a_date)\n\n[1] 2024\n\n\n… the month (as a number) …\n\nmonth(a_date)\n\n[1] 5\n\n\n… and the month (as a word in the relevant locale; I’m using English).\n\nmonth(a_date, label = TRUE)\n\n[1] May\n12 Levels: Jan &lt; Feb &lt; Mar &lt; Apr &lt; May &lt; Jun &lt; Jul &lt; Aug &lt; Sep &lt; ... &lt; Dec\n\n\nMuch of the complexity surrounding dates and times arises from the existence of time zones. It is important to note that one does not completely avoid this complexity with dates. For example, the date 2024-06-28 in the America/New_York time zone will include points of time that are associated with the date 2024-06-29 in the Australia/Sydney time zone. So if a date-time related to Australia is converted to a date based on UTC or America/New_York, then it may end up on a different date that would result if converted to a date using Australia/Melbourne.\nAnother aspect of dates that is easy to overlook is that some things that seem simple turn out to be complicated when examined more closely. For example, what date is one month after 2024-01-31? Also, what date is one year after 2024-02-29? Note that R will return NA as the answer to each of these questions.6"
  },
  {
    "objectID": "published/datetimes.html#date-times",
    "href": "published/datetimes.html#date-times",
    "title": "Working with date and times",
    "section": "Date-times",
    "text": "Date-times\nA date-time, also known as a timestamp, combines a date with a time and thus represents an instant in time. From this perspective, I would argue that a timestamp only has meaning if understood in the context of a time zone, as a date-time with a different time zone represents a different instant in time, as can be seen from the following examples.\n\nts &lt;- \"2008-06-30 16:52:26\"\nt1 &lt;- parse_date_time(ts, orders = \"ymdHMS\")\nt2 &lt;- parse_date_time(ts, tz = \"Australia/Melbourne\", orders = \"ymdHMS\")\nt3 &lt;- parse_date_time(ts, tz = \"America/New_York\", orders = \"ymdHMS\")\n\nIt turns out that t1 is a date-time representing 2008-12-31 16:52:26 UTC, which is a different point in time from both t2, which is the “same” time in the Australia/Melbourne time zone, and t3, which is the “same” time in the America/New_York time zone.\n\nt1 - t2\n\nTime difference of 10 hours\n\nt1 - t3\n\nTime difference of -4 hours\n\n\nTo determine the correct string to use for a given time zone, inspect the output of OlsonNames(). Many denizens of US time zone will indicate them using abbreviations such as EST for New York even at times of the year when that time zone does not apply (e.g., in June, when EDT is the applicable time zone). In this regard, using America/New_York obviates the risk of a mismatch such as that implied by 2008-06-30 16:52:26 EST."
  },
  {
    "objectID": "published/datetimes.html#duckdb-and-time-zones",
    "href": "published/datetimes.html#duckdb-and-time-zones",
    "title": "Working with date and times",
    "section": "DuckDB and time zones",
    "text": "DuckDB and time zones\nWe create a DuckDB instance by connecting to it as follows.\n\ndb &lt;- dbConnect(duckdb::duckdb(), timezone_out =\"America/New_York\")\n\nWe specify timezone_out =\"America/New_York\" so that data returned to R are in the time zone that makes most sense for plotting and such like. Note that this does not change the actual moment in time reflected in the data, just how it is displayed.\nBy default, DuckDB does not load information about time zones. These are found in the package icu, which can be installed and loaded using the commands below.7\n\ndbExecute(db, \"INSTALL icu\")\ndbExecute(db, \"LOAD icu\")\n\nWe will use functions like hour(), minute(), and second() to prepare data for plotting. To understand these functions, we create a table containing just one value.\n\na_datetime &lt;- \"2008-12-31 16:52:26 America/New_York\"\nsample &lt;- tbl(db, sql(str_c(\"SELECT '\", a_datetime, \"'::TIMESTAMPTZ AS a_datetime\")))\n\nIntuitively, we would expect hour(a_datetime) to return 16, minute(a_datetime) to return 52, and second() to return 26. However, these functions are interpreted with respect to the time zone setting of the database. The default setting for DuckDB is the local time, but we can set this to UTC using the following command.\n\ndbExecute(db, \"SET TIME ZONE 'UTC'\")\n\nThus we get the value 21 because UTC was five hours ahead of New York time on 31 December 2008.\n\nsample |&gt; mutate(hour = hour(a_datetime)) |&gt; collect()\n\n# A tibble: 1 × 2\n  a_datetime           hour\n  &lt;dttm&gt;              &lt;dbl&gt;\n1 2008-12-31 16:52:26    21\n\n\nSo, we set the DuckDB time zone to New York time …\n\ndbExecute(db, \"SET TIME ZONE 'America/New_York'\")\n\n… and try again.\n\nsample |&gt; mutate(hour = hour(a_datetime)) |&gt; collect()\n\n# A tibble: 1 × 2\n  a_datetime           hour\n  &lt;dttm&gt;              &lt;dbl&gt;\n1 2008-12-31 16:52:26    16"
  },
  {
    "objectID": "published/datetimes.html#working-with-sec-filings-data",
    "href": "published/datetimes.html#working-with-sec-filings-data",
    "title": "Working with date and times",
    "section": "Working with SEC filings data",
    "text": "Working with SEC filings data\nWe start by loading the filings data stored in a parquet file filings.parquet in the data subdirectory of our current project.8\n\nfilings &lt;- load_parquet(db, table = \"filings\", data_dir = \"data\")\n\nAn important thing to note about the creation of this file is that the original data were coded such that the time zone of the data was UTC. For example, the original text representation of acceptanceDateTime was of the form 2024-04-25T17:04:01.000Z. The Z is meant to indicate that the timestamp is expressed in UTC.9 However, I ignored that in importing the data and interpreted these timestamps as relating to America/New_York. As we will see shortly, this interpretation is correct based on other information.10\nTo understand the date-times on filings, I extract some components of the timestamp to create a variable acceptance_time that converts the date component of each acceptanceDateTime to the same date (2000-01-01) as this facilitates plotting the times ignoring the dates.11\n\nfiling_times &lt;- \n  filings |&gt;\n  mutate(year = year(acceptanceDateTime),\n         hour = hour(acceptanceDateTime),\n         minute = minute(acceptanceDateTime),\n         second = second(acceptanceDateTime),\n         acceptance_time = make_timestamptz(2000L, 01L, 01L, \n                                            hour, minute, second)) |&gt;\n  select(cik, year, accessionNumber, acceptanceDateTime, acceptance_time) |&gt;\n  compute()\n\nFigure 1 plots the acceptance times for all filings in our sample. One observation is that we have over a million filings right around midnight.\n\nfiling_times |&gt;\n  ggplot(aes(x = acceptance_time)) + \n  geom_histogram(binwidth = 5 * 60) +\n  scale_x_datetime(date_breaks = \"1 hour\",\n                   date_labels = \"%H:%M\") +\n  theme(axis.text.x = element_text(angle = 90))\n\n\n\n\n\n\n\nFigure 1: Distribution of acceptance time for all filings\n\n\n\n\n\nFigure 2 digs into these “midnight” filings based on the year of filing. It appears that the majority of these are from 2002 or earlier. A plausible explanation is that most filings during that earlier period only have filing dates and acceptanceDateTime is expressed as midnight on those filing dates.\nIn Figure 2, we also see evidence of a small number of filings for years 1980 and 1993. The former value probably relates to errors, but a small number of filings did occur in 1993.\n\nfiling_times |&gt;\n  mutate(midnight = acceptance_time == \"2000-01-01\") |&gt;\n  mutate(year = as.character(year)) |&gt;\n  ggplot(aes(x = year, fill = midnight)) + \n  geom_bar() +\n  theme(axis.text.x = element_text(angle = 90))\n\n\n\n\n\n\n\nFigure 2: Year of ‘midnight’ filings\n\n\n\n\n\nFigure 3 focuses on cases before 2002 with non-midnight acceptance times. While there are some, these appear to be very few in number, perhaps reflecting a pilot program of some sort.12 Note that—with one stray exception before 07:00—filings appear to occur between 08:00 and 22:00 during this period.\n\nfiling_times |&gt; \n  filter(year &lt; 2002,\n         acceptance_time != \"2000-01-01\") |&gt;\n  mutate(year = as.character(year)) |&gt;\n  ggplot(aes(x = acceptance_time, fill = year)) + \n  geom_histogram(binwidth = 5 * 60) +\n  scale_x_datetime(date_breaks = \"1 hour\",\n                   date_labels = \"%H:%M\") +\n  theme(axis.text.x = element_text(angle = 90))\n\n\n\n\n\n\n\nFigure 3: Distribution of acceptance time–pre-2002 with times\n\n\n\n\n\nFigure 4 focuses on non-midnight filings in 2002 or later, we see that the bulk of filings are between 06:00 and 22:00, consistent with the statement on the SEC website that “EDGAR is available to accept filings from 6 a.m. to 10 p.m. ET weekdays (except federal holidays).” In fact, it was this information that allowed me to conclude that the timestamps in the underlying JSON files are in New York times, as interpreting them as UTC times put many filings outside this window.\nHowever, some pre-08:00 filings appear in Figure 4.\n\nfiling_times |&gt;\n  filter(year &gt;= 2002,\n         acceptance_time &gt; \"2000-01-01\") |&gt;\n  ggplot(aes(x = acceptance_time)) + \n  geom_histogram(binwidth = 5 * 60) +\n  scale_x_datetime(date_breaks = \"1 hour\",\n                   date_labels = \"%H:%M\") +\n  theme(axis.text.x = element_text(angle = 90))\n\n\n\n\n\n\n\nFigure 4: Distribution of acceptance time–post-2002 with times\n\n\n\n\n\nFigure 5 examines the pre-08:00 filings that appear in Figure 4. It appears that these are oddly concentrated at around 00:15 and are perhaps data errors.\n\nfiling_times |&gt;\n  filter(between(acceptance_time, \"2000-01-01 00:00:01\", \"2000-01-01 01:00:00\")) |&gt;\n  ggplot(aes(x = acceptance_time)) + \n  geom_histogram(binwidth = 1 * 60) +\n  scale_x_datetime(date_breaks = \"5 min\",\n                   date_labels = \"%H:%M\") +\n  theme(axis.text.x = element_text(angle = 90))\n\n\n\n\n\n\n\nFigure 5: Distribution of acceptance time–early-morning outliers\n\n\n\n\n\nWhile not apparent in Figure 2, Figure 6 reveals that there are some post-2003 midnight filings. However, these are all concentrated in 2009 and are perhaps evidence of data issues.\n\nfiling_times |&gt;\n  mutate(year = year(acceptanceDateTime)) |&gt; \n  filter(year &gt; 2003,\n         acceptance_time == \"2000-01-01 00:00:00\") |&gt;\n  ggplot(aes(x = acceptanceDateTime)) + \n  geom_histogram(binwidth = 60 * 60 * 24 * 7) +\n  scale_x_datetime(date_breaks = \"1 month\", date_labels = \"%Y-%m-%d\") +\n  theme(axis.text.x = element_text(angle = 90))\n\n\n\n\n\n\n\nFigure 6: Distribution of dates–post-2003 midnight filers\n\n\n\n\n\nGiven the issues revealed above, Figure 7—our final plot of acceptance times—focuses on filings after 2003 and with filing times after 05:00 and before 23:00.\n\nfiling_times |&gt;\n  filter(year &gt; 2003,\n         between(acceptance_time,\n                 \"2000-01-01 05:00:00\",\n                 \"2000-01-01 23:00:00\")) |&gt;\n  ggplot(aes(x = acceptance_time)) + \n  geom_histogram(binwidth = 5 * 60) +\n  scale_x_datetime(date_breaks = \"1 hour\",\n                   date_labels = \"%H:%M\") +\n  theme(axis.text.x = element_text(angle = 90))\n\n\n\n\n\n\n\nFigure 7: Distribution of acceptance times with valid times\n\n\n\n\n\nThere are some interesting patterns to be observed in Figure 7.\n\nFor filings before 10:00, there are spikes in the number of filings on the hour. This may reflect filings that are pre-programmed at specific times in pre-trading hours.13\nThese spikes are not apparent during trading hours.\nThere is a big spike in the number of filings just after 16:00, consistent with many filings being delayed until the close of trading.\nPutting aside the pre-trading spikes, there is a steady rise int he number of filings until about 12:00, then a flattening out till about 13:30, when filings steadily rise until 16:00.\nThere is a sharp drop in the number of filings at about 17:30.\nAfter 17:30, the number of filings steadily decreases until 22:00.\nThere are tiny spike on the hour after 17:00."
  },
  {
    "objectID": "published/datetimes.html#appendix-postgresql",
    "href": "published/datetimes.html#appendix-postgresql",
    "title": "Working with date and times",
    "section": "Appendix: PostgreSQL",
    "text": "Appendix: PostgreSQL\nAt the outset I alluded to using PostgreSQL and DuckDB. In this appendix, I show how one can achieve the same result as shown in Figure 8 using PostgreSQL.\n\nGetting data into PostgreSQL\nIf we were starting with data already in PostgreSQL, we could skip this step. However, a big attraction of DuckDB is its ability to glue together data from many different sources and we can use DuckDB to populate PostgreSQL.\nWe first need to install and load the postgres extension for DuckDB, which is done with the following lines of code.\n\ndbExecute(db, \"INSTALL postgres\")\ndbExecute(db, \"LOAD postgres\")\n\nI have a PostgreSQL database running on my computer, so I can connect to it with an empty connection string. See the documentation for the postgres extension if you need to supply additional information to connect to a PostgreSQL database.18\n\ndbExecute(db, \"ATTACH 'user=igow' AS pg (TYPE POSTGRES, READ_WRITE)\")\n\nThe first step is to copy the data in filings.parquet to PostgreSQL. We need to go from the parquet file as we did not materialize a table with these data in DuckDB. This step takes some time, but would be a one-off step. I include IF NOT EXISTS to avoid overwriting any existing table you might have.\n\nsql &lt;- \"CREATE TABLE IF NOT EXISTS pg.filings AS\n        SELECT * FROM 'data/filings.parquet'\"\ndbExecute(db, sql) |&gt; system_time()\n\n   user  system elapsed \n  0.004   0.001   0.046 \n\n\n[1] 0\n\n\nWe did materialize named (temporary) tables for tickers and sample_calls, so we can copy directly from those to PostgreSQL. These small tables are populated very quickly.\n\ndbExecute(db, \"CREATE TABLE IF NOT EXISTS pg.tickers AS\n               SELECT * FROM tickers\")\ndbExecute(db, \"CREATE TABLE IF NOT EXISTS pg.sample_calls AS\n               SELECT * FROM sample_calls\")\n\n\n\nRe-running the analysis using PostgreSQL\nThe first step is to connect to PostgreSQL, again using the timezone argument for the same reasons we discussed above with DuckDB.\n\npg &lt;- dbConnect(RPostgres::Postgres(), timezone = \"America/New_York\")\n\nWe next create remote data tables for each of the three underlying data tables we used above.\n\nfilings &lt;- tbl(pg, \"filings\")\ntickers &lt;- tbl(pg, \"tickers\")\nsample_calls &lt;- tbl(pg, \"sample_calls\")\n\nNote that DuckDB creates tables with lower-case column names, so we need to adapt the code above to reflect this. This is not an inherent limitation of PostgreSQL, which can support case-sensitive column names. Apart from the lower-case column names, the following query converts hour and minute to integers, as the make_timestamptz() function in PostgreSQL expects integers in those slots. Note that I do not compute the table filing_times, as doing so is unnecessary and actually slows the code down.\n\nfiling_times &lt;- \n  filings |&gt;\n  mutate(year = year(acceptanceDateTime),\n         hour = as.integer(hour(acceptanceDateTime)),\n         minute = as.integer(minute(acceptanceDateTime)),\n         second = second(acceptanceDateTime),\n         acceptance_time = make_timestamptz(2000L, 1L, 1L, \n                                            hour, minute, second)) |&gt;\n  select(cik, year, accessionNumber, acceptanceDateTime, acceptance_time)\n\nThe code for creating earnings_filings is unchanged from above apart from using lower-case column names, as PostgreSQL offers the same regexp_split_to_table() function that we used in DuckDB.\n\nearnings_filings &lt;-\n  filings |&gt;\n  inner_join(tickers, by = \"cik\") |&gt;\n  mutate(item = regexp_split_to_table(items, \",\")) |&gt;\n  filter(form == \"8-K\", item == \"2.02\") |&gt;\n  select(cik, ticker, filingDate, acceptanceDateTime, item)\n\nThe code creating sample_merged differs slightly (apart from using lower-case column names) because start_date - acceptancedatetime results in a column of type interval in PostgreSQL that R is unable to interpret easily. To address this, I use the date_part('epoch', x) function to convert it to the number of seconds between the two timestamps.\n\nsample_merged &lt;-\n  earnings_filings |&gt;\n  inner_join(sample_calls, join_by(ticker, filingDate)) |&gt;\n  mutate(time_diff = date_part('epoch', start_date - acceptanceDateTime)) |&gt;\n  collect() |&gt;\n  system_time()\n\n   user  system elapsed \n  0.020   0.001   2.397 \n\n\nFinally, the code to produce Figure 9 is unchanged from the equivalent code above. Happily, Figure 9 looks identical to Figure 8.\n\nsample_merged |&gt;\n  mutate(time_diff = time_diff / 60) |&gt;\n  ggplot(aes(x = time_diff, fill = ticker)) +\n  geom_histogram(binwidth = 5)\n\n\n\n\n\n\n\nFigure 9: Distribution of times between 8-K filings and conference calls: PostgreSQL version\n\n\n\n\n\nMy experience is that DuckDB generally offers better performance than PostgreSQL, but in this case PostgreSQL seems quite performant relative to DuckDB once the data have been loaded into PostgreSQL. PostgreSQL does offer some benefits, including the ease of sharing data with others and avoidance of messy details of data files. For example, I could easily run the code to create data on my server in Massachusetts and access the data from my current location in Melbourne, Australia. In any case, the code above shows that the task can be accomplished in either DuckDB or PostgreSQL, so we are not forced to choose in this case."
  },
  {
    "objectID": "published/datetimes.html#footnotes",
    "href": "published/datetimes.html#footnotes",
    "title": "Working with date and times",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nGow and Ding (2024) was published in print form by CRC Press in December 2024 and remains free online.↩︎\nExecute install.packages(c(\"tidyverse\", \"DBI\", duckdb\", \"scales\", \"farr\")) within R to install all the packages you need to run the code in this note.↩︎\nIt is pointed out in that R does not have a native class for storing times (Wickham et al., 2023, p. 296).↩︎\nFor more on the data curation process as part of the data science workflow, see my note here.↩︎\nAfter rummaging through the depths of the SAS documentation, I landed on format=E8601DT19. as the way to represent date-times being exported by SAS.↩︎\nTo see this, enter as.Date(\"2024-01-31\") + months(1) and as.Date(\"2024-02-29\") + years(1), respectively.↩︎\nThe INSTALL icu command is only needed if you have not already installed it, but it’s harmless to run it if it’s already present.↩︎\nAdjust data_dir to match the location of the downloaded file on your compute. The process for creating this file is described here. You can obtain a copy of this file here. If you have set up a parquet data repository with this data in the submissions schema, then use filings &lt;- load_parquet(db, table = \"filings\", schema = \"submissions\") instead.↩︎\nSee the Wikipedia page for ISO 8601.↩︎\nNote that the information is stored in parquet files in UTC.↩︎\nAs mentioned above, R has no native type for times, but a set of date-times with the date fixed on a common date will provide us with what we need.↩︎\nThough we might expect a preponderance of these filings to occur in the years just before 2002 in this case, and the filings appear to be fairly spread out over the pre-2002 years.↩︎\nNYSE and NASDAQ trading hours are 09:30–16:00.↩︎\nSee here for information about each of the items.↩︎\nSee the note discussed earlier for details on how this file was created from data on SEC EDGAR.↩︎\nNote that we don’t strictly “create” this table in the sense of SQL’s CREATE TABLE using compute(), as we will only use a small portion of it and it would be expensive—about 10 seconds and 1.5GB of RAM—to compute() this whole table.↩︎\nBy default, the result is a difftime, which ggplot2 is unsure about.↩︎\nNote that you will need to have write access to this database.↩︎"
  },
  {
    "objectID": "published/data_mgt_r.html",
    "href": "published/data_mgt_r.html",
    "title": "Data management ideas for researchers (R version)",
    "section": "",
    "text": "My sense is that data management is a challenge for researchers. In an academic context, some fields may receive greater institutional support than others. My experience in business schools was that there was very little support for data curation and management. While many of the ideas I discuss here are general in nature, for concreteness, I focus on the special case of a WRDS user maintaining a local Parquet data library of the kind discussed in Appendix E of Empirical Research in Accounting: Tools and Methods and provide examples using my R package db2pq."
  },
  {
    "objectID": "published/data_mgt_r.html#caveats",
    "href": "published/data_mgt_r.html#caveats",
    "title": "Data management ideas for researchers (R version)",
    "section": "2.1 Caveats",
    "text": "2.1 Caveats\nAt the outset, I should note some limitations to my discussion here.\nFirst, this note does not data at the scale of multiple terabytes. Researchers working with data at the scale that one starts thinking about Spark clusters and immense cloud storage will not find any answers here. That said, I think the approaches I cover here scale up to a “low terabyte” scale, at least for aggregate data.\nSecond, with very few exceptions, I have not really dealt with data with confidentiality issues. Readers dealing with sensitive data would need to overlay the necessary safeguards and protocols associated with such data onto the discussion I provide here."
  },
  {
    "objectID": "published/data_mgt_r.html#some-concepts-in-data-management",
    "href": "published/data_mgt_r.html#some-concepts-in-data-management",
    "title": "Data management ideas for researchers (R version)",
    "section": "2.2 Some concepts in data management",
    "text": "2.2 Some concepts in data management\n\n2.2.1 Scope\nMany datasets are project-specific datasets, meaning that only have use within a single project (e.g., paper). Examples of project-specific would include experimental data generated in a particular study.\nOther datasets are general-purpose datasets, meaning that they contain data that might be relevant to many studies. Classic examples in a business-school context would be the US stock price files offered by the Center for Research in Security Prices, LLC (CRSP) or financial statement data provided by Compustat, or economic time-series data provided by various statistical offices around the world.\nOther datasets are project-level datasets, meaning that the particular data sets are somehow frozen for a particular project, even though the nature of the data otherwise puts them in the category of general-purpose datasets. For example, I might want to fix on a particular version of comp.g_funda, Compustat’s global dataset for annual financial statements for my project, even though this dataset has relevance beyond a specific project.1\nThere are two reasons for having project-level that I can think of. The first reason arises in the context of reproducibility. If I have published a paper, then the replication package for that paper should ideally contain the data used to produce the exact results in the paper. For this purpose, if the paper used comp.g_funda data, then the ideal replication package would include the precise project-level version of that data set used to produce the paper. Of course, in reality, one cannot simply post the project-level version of comp.g_funda as part of a public replication package. Nonetheless, the authors themselves should have a project-level version of the dataset that they retain. This much aligns with the views of Welch (2019), who suggests that “the author should keep a private copy of the full data set with which the results were obtained.”\nThe second reason is related to the first, but in some ways opposite in spirit. Some authors do not want their results to be upset by updates to any of datasets used to produce them. On one research project, I was responsible for supplying a curated data set of significant scale and complexity. Unfortunately, my understanding of variable scoping in Perl meant that about 2% of the data were simply corrupted and I felt I had to fix this. At my end, I wanted to manage the data as a general-purpose data set, but my co-author wanted to stick to the earlier project-level data.2\nAs far as WRDS data are concerned, my db2pq package aims to facilitate managing WRDS data either as general-purpose data or as project-level data.3 On my computers, I have the environment variable DATA_DIR set to a location inside Dropbox. So, by default, new WRDS data will go in the matching schema (i.e., subdirectory) of the directory indicated by DATA_DIR. In R, I can inspect the value in DATA_DIR:\n\nSys.getenv(\"DATA_DIR\")\n\n[1] \"/Users/igow/Dropbox/pq_data\"\n\n\nThe db2pq package is not yet on CRAN, so you need to install it from GitHub using pak:\npak::pak(\"iangow/db2pqr\")\nWithin R, you can check the version using the following.\n\npackageVersion(\"db2pq\")\n\n[1] '0.0.2'\n\n\n\nlibrary(arrow)\nlibrary(dplyr)\nlibrary(ffiec.pq)\nlibrary(db2pq)\nlibrary(wrds)\n\nThe db2pq package uses the wrds package to get a connection to the WRDS PostgreSQL database. So if you have not done so, you will need to set up your credentials using that package.\n\nwrds_set_credentials()\n\nThe core function of db2pq is wrds_update_pq(). If I ask wrds_update_pq() to update my general-purpose version of crsp.dsi, I can see that the latest data on WRDS are no more recent than what I already have, so no update occurs.\n\nwrds_update_pq(\"dsi\", \"crsp\")\n\ncrsp.dsi already up to date.\n\n\nBut, if I wanted a project-level version of crsp.dsi, I can specify the project-level data directory (\"data\") and WRDS will update the data there. As I don’t have any data in that folder to begin with, an “update” occurs.\n\nwrds_update_pq(\"dsi\", \"crsp\", data_dir = \"data\")\n\nUpdated crsp.dsi is available.\n\n\nBeginning file download at 2026-02-26 14:30:17 UTC.\n\n\nCompleted file download at 2026-02-26 14:30:17 UTC.\n\n\n\n\n2.2.2 Version control\nVersion control is a thorny issue with data. As far as I know there is no equivalent of Git for datasets.4 While I am sure that version control of data is a big issue in many contexts (e.g., data for regulated bodies), many data providers, even commercial vendors, often do a poor job of version control.\nMany data sources will provide the current version of any given dataset and nothing else. For example, there is no way to get the version of the data you downloaded from WRDS two years ago if you want to understand why results have changed. In practice, researchers need to do any archiving of WRDS data sets themselves.\nMy db2pq R package provides some functions that make it more convenient to maintain archives of previous versions of tables from WRDS. The core function for maintaining a local repository of Parquet files based on WRDS data is wrds_update_pq(). This function has an archive argument that, if set to TRUE, causes any existing data in the repository to be archived when an update is available and is applied:\n\nwrds_update_pq(\"company\", \"comp\", archive = TRUE)\n\nUpdated comp.company is available.\n\n\nBeginning file download at 2026-02-26 14:30:18 UTC.\n\n\nCompleted file download at 2026-02-26 14:30:19 UTC.\n\n\nFor most tables on the WRDS database, it appears that “last updated” metadata is included in table comments. The wrds_update_pq() function will, by default, extract that metadata and embed it as metadata in the Parquet files.\nThe pq_last_modified() function, if given a table_name argument, will by default return the metadata embedded in the Parquet file.\n\npq_last_modified(table_name = \"company\", schema = \"comp\")\n\n[1] \"Company (Updated 2026-02-26)\"\n\n\nBut if I specify archive = TRUE, then pq_last_modified() will instead return a tibble containing information about (possibly several) files matching the specified table_name in the archive.5 Here, we see that I have four previous versions of comp.company in my archive.\n\npq_df &lt;- pq_last_modified(table_name = \"company\", schema = \"comp\", archive = TRUE)\npq_df[c(\"file_name\", \"last_mod\")]\n\n# A tibble: 9 × 2\n  file_name                last_mod           \n  &lt;chr&gt;                    &lt;dttm&gt;             \n1 company_20240614T062835Z 2024-06-14 06:28:35\n2 company_20260105T070000Z 2026-01-05 07:00:00\n3 company_20260107T070000Z 2026-01-07 07:00:00\n4 company_20260209T070000Z 2026-02-09 07:00:00\n5 company_20260218T070000Z 2026-02-18 07:00:00\n6 company_20260224T070000Z 2026-02-24 07:00:00\n7 company_20260225T000000Z 2026-02-24 07:00:00\n8 company_20260225T070000Z 2026-02-25 07:00:00\n9 company_20260226T070000Z 2026-02-26 07:00:00\n\n\nI can use the function pq_restore() to make the one from 2024-06-14 the one that I am using for my data repository.\n\npq_restore(\"company_20240614T062835Z\", \"comp\")\n\nArchived to: /Users/igow/Dropbox/pq_data/comp/archive/company_20260226T070000Z.parquet\n\n\nRestored to: /Users/igow/Dropbox/pq_data/comp/company.parquet\n\n\nI now see that this is the version used when I look for company in the comp schema:\n\npq_last_modified(table_name = \"company\", schema = \"comp\")\n\n[1] \"Last modified: 06/14/2024 02:28:35\"\n\n\nOne thing you will notice is that the format of the “last modified” string has changed from the one above. This could be for one of three reasons:\n\nThe Parquet file that has been restored was created using my Python package wrds2pg, which extracts data from WRDS’s SAS data files. Naturally, it uses information returned by the SAS command PROC CONTENTS.\nThe Parquet file that has been restored was created using an earlier version of the Python version of db2pq. Because WRDS did not initially store “last modified” information with its PostgreSQL tables, earlier versions of the Python version of db2pq retrieved information from the matching SAS file on the assumption that the SAS and PostgreSQL data sets would generally be aligned.\nThe Parquet file that has been restored was created using a recent Python version of db2pq, but with use_sas=True. In this case, wrds_update_pq() will retrieve the “last modified” information from the SAS file.6\n\nNote that the use_sas argument has not yet been implemented in the R version of db2pq.\nBy default, the pq_restore() function has archive = TRUE, which means that any existing data file is archived.7 We can see the file that we created just moments ago using wrds_update_pq() is now in the archive:\n\npq_df &lt;- pq_last_modified(table_name = \"company\", schema = \"comp\", archive = TRUE)\npq_df[c(\"file_name\", \"last_mod\")]\n\n# A tibble: 8 × 2\n  file_name                last_mod           \n  &lt;chr&gt;                    &lt;dttm&gt;             \n1 company_20260105T070000Z 2026-01-05 07:00:00\n2 company_20260107T070000Z 2026-01-07 07:00:00\n3 company_20260209T070000Z 2026-02-09 07:00:00\n4 company_20260218T070000Z 2026-02-18 07:00:00\n5 company_20260224T070000Z 2026-02-24 07:00:00\n6 company_20260225T000000Z 2026-02-24 07:00:00\n7 company_20260225T070000Z 2026-02-25 07:00:00\n8 company_20260226T070000Z 2026-02-26 07:00:00\n\n\nIf we update again with archive = TRUE, we will effectively put the 2024-06-14 version back in the archive and replace it with the current version on WRDS.\n\nwrds_update_pq(\"company\", \"comp\", archive = TRUE)\n\nUpdated comp.company is available.\n\n\nBeginning file download at 2026-02-26 14:30:19 UTC.\n\n\nArchived existing file to: /Users/igow/Dropbox/pq_data/comp/archive/company_20240614T062835Z.parquet\n\n\nCompleted file download at 2026-02-26 14:30:20 UTC.\n\n\nSome WRDS PostgreSQL tables appear not (yet) to have “last modified” information. For example, some RavenPack data tables appear not to have this information. In the following, I set obs = 100 and data_dir = \"data\", as I am doing this “update” purely for the purposes of this note, so only get 100 observations to keep things fast.\n\nwrds_update_pq(\"rpa_entity_mappings\", \"ravenpack_common\", obs = 100, \n               data_dir = \"data\")\n\nNo comment found for ravenpack_common.rpa_entity_mappings.\n\n\nravenpack_common.rpa_entity_mappings already up to date.\n\n\nWe can confirm this using pq_last_modified():\n\npq_last_modified(table_name = \"rpa_entity_mappings\", schema = \"ravenpack_common\",\n                 data_dir = \"data\")\n\n[1] \"\"\n\n\nIn such cases, any subsequent call to wrds_update_pq() will not trigger an “update” because there is effectively nothing to allow it to confirm that the local data are not current.8\n\nwrds_update_pq(\"rpa_entity_mappings\", \"ravenpack_common\", obs = 100, \n               data_dir = \"data\")\n\nNo comment found for ravenpack_common.rpa_entity_mappings.\n\n\nravenpack_common.rpa_entity_mappings already up to date.\n\n\nIn such cases, it makes sense to use the SAS data to determine the vintage of the data. However, a wrinkle in this case is that there is no SAS library called ravenpack_common. Instead the data are stored in the SAS library named rpa. So we also need to tell wrds_update_pq() where to get the SAS data.\n\nwrds_update_pq(\"rpa_entity_mappings\", \"ravenpack_common\", obs = 100, \n               data_dir = \"data\", use_sas = TRUE, sas_schema = \"rpa\")\n\nUpdated ravenpack_common.rpa_entity_mappings is available.\n\n\nBeginning file download at 2026-02-26 14:30:29 UTC.\n\n\nCompleted file download at 2026-02-26 14:30:30 UTC.\n\n\nNow we have valid “last modified” data:\n\npq_last_modified(table_name = \"rpa_entity_mappings\", \"ravenpack_common\",\n                 data_dir=\"data\")\n\n[1] \"Last modified: 02/09/2026 16:11:10\"\n\n\nSo a subsequent call to wrds_update_pq() does not trigger an update, but for the correct reasons.\n\nwrds_update_pq(\"rpa_entity_mappings\", \"ravenpack_common\", obs = 100, \n               data_dir = \"data\", use_sas = TRUE, sas_schema = \"rpa\")\n\nravenpack_common.rpa_entity_mappings already up to date.\n\n\n\n\n2.2.3 Storage formats\nWhile there are many storage formats available for data, I think a strong case can be made for Parquet being the default storage format for many users. If you use R or Python, I think the case is easy to make. Many software packages can read Parquet data and some of them (e.g., the arrow package or DuckDB) will absolutely fly with Parquet data.\nI believe that recent editions of Stata can read Parquet files, though the way Stata operates means that Stata users are unlikely to see the performance benefits Parquet offers.9 SAS users might find the case for Parquet less compelling, though there are probably benefits in moving to a storage medium that is more compact, less proprietary, and more likely to be supported in a few years’ time.\nOf course, an alternative to using Parquet files would be using a database, such as PostgreSQL. I think such systems have a lot of merit (and I have used PostgreSQL to store WRDS data since 2011), but I think they are more complicated for most users’ needs and their benefits (e.g., shared access to data and rock-solid assurance) are less meaningful for most.\nAnother alternative is the CSV file, perhaps compressed. I think if one were sending data on the next Voyager mission, then CSV might be the chosen format.10 Or if you really, really wanted data novices to inspect your data in Excel or Word, then CSV might be the go-to option. Or perhaps you want to put your data in a written form in a book for users to type in. For any other purpose with serious data needs, I think Parquet dominates.\nOne issue with CSV is that one is always dealing with type inference (string, integer, timestamp) and I think that type inference is one of those problems you want to solve once for any given dataset. For the WRDS data that is the focus of this note, I think CSV is to be avoided.\n\n\n2.2.4 Timestamps\nSpeaking of type inference, one bane of the existence of any data analyst might be timestamps. The usual purpose of timestamps is to identify a moment in time. For example, I want to know the precise time at which an earnings conference call happened, so I can turn to a dataset with intra-day data on quotes and trades to see how the market reacted. If the data on earnings conference calls use UTC and the trade-and-quote data use US Eastern time and I ignore these differences, then I will be looking at times that are off by four or five hours (depending on the time of year).\nTo examine this issue, I’m going to revisit the Call Report data I wrote about recently. I have these data in my (general-purpose) data repository and I can use the ffiec_scan_pqs from my ffiec.pq package to read the data into R.\n\ndb &lt;- DBI::dbConnect(duckdb::duckdb())\n\nIn my earlier note, I discussed how I managed to infer that the timestamps on that dataset are in US Eastern time (America/New_York). We can inspect the data I have using the function above, focused on a single observation:\n\nffiec_scan_pqs(db, \"por\") |&gt;\n  select(IDRSSD, date, last_date_time_submission_updated_on) |&gt;\n  filter(IDRSSD == 37, date == as.Date(\"2023-12-31\")) |&gt;\n  collect()\n\n# A tibble: 1 × 3\n  IDRSSD date       last_date_time_submission_updated_on\n   &lt;int&gt; &lt;date&gt;     &lt;dttm&gt;                              \n1     37 2023-12-31 2024-01-10 18:43:43                 \n\n\nWRDS offers essentially the same data in its bank schema. We can use wrds_update_pq() to get a sample of these data.11\n\nwrds_update_pq(\"wrds_call_rcfa_1\", \"bank\", data_dir = \"data\", obs = 100)\n\nUpdated bank.wrds_call_rcfa_1 is available.\n\n\nBeginning file download at 2026-02-26 14:30:35 UTC.\n\n\nApplying tz='UTC' to 1 timestamp column(s): rssdsubmissiondate.\n\n\nCompleted file download at 2026-02-26 14:30:35 UTC.\n\n\nTo load the data into R, I will use the following load_parquet() helper function.\n\nload_parquet &lt;- function(table, schema, data_dir = Sys.getenv(\"DATA_DIR\", \".\")) {\n  path &lt;- file.path(data_dir, schema, paste0(table, \".parquet\"))\n  open_dataset(path)\n}\n\nAs can be seen from the output below, the timestamp is off by five hours. That is because wrds_update_pq() assumes that timestamps are in UTC, which is a correct assumption for some datasets on WRDS, but is incorrect in this case.\n\nrcfa_1 &lt;- load_parquet(\"wrds_call_rcfa_1\", \"bank\", data_dir = \"data\")\nrcfa_1 |&gt;\n  select(rssd9001, wrdsreportdate, rssdsubmissiondate) |&gt;\n  filter(rssd9001 == 37, wrdsreportdate == as.Date(\"2023-12-31\")) |&gt;\n  collect()\n\n# A tibble: 1 × 3\n  rssd9001 wrdsreportdate rssdsubmissiondate \n     &lt;int&gt; &lt;date&gt;         &lt;dttm&gt;             \n1       37 2023-12-31     2024-01-10 13:43:43\n\n\nWRDS generally stores timestamps in PostgreSQL with type TIMESTAMP WITHOUT TIME ZONE, which is equivalent to saying “you figure out the time zone, user.”12 Because we know that the timestamps provided by the FFIEC are expressed in US Eastern time, we can tell wrds_update_pq() this using the tz argument:\n\nwrds_update_pq(\"wrds_call_rcfa_1\", \"bank\", \n               data_dir = \"data\", obs = 100, \n               force = TRUE, tz = \"America/New_York\")\n\nForcing update based on user request.\n\n\nBeginning file download at 2026-02-26 14:30:36 UTC.\n\n\nApplying tz='America/New_York' to 1 timestamp column(s): rssdsubmissiondate.\n\n\nCompleted file download at 2026-02-26 14:30:36 UTC.\n\n\nNow, we see that the data are correct.\n\nrcfa_1 &lt;- load_parquet(\"wrds_call_rcfa_1\", \"bank\", data_dir = \"data\")\nrcfa_1 |&gt;\n  select(rssd9001, wrdsreportdate, rssdsubmissiondate) |&gt;\n  filter(rssd9001 == 37, wrdsreportdate == as.Date(\"2023-12-31\")) |&gt;\n  collect()\n\n# A tibble: 1 × 3\n  rssd9001 wrdsreportdate rssdsubmissiondate \n     &lt;int&gt; &lt;date&gt;         &lt;dttm&gt;             \n1       37 2023-12-31     2024-01-10 18:43:43\n\n\nIn other cases, WRDS doesn’t even bother to store the data as TIMESTAMP WITHOUT TIME ZONE, but instead the data are stored as strings. Here’s one example.13\n\nwrds_update_pq(\"feed03_audit_fees\", \"audit\", \n               keep = c(\"auditor_fkey\", \"file_accepted\"),\n               obs = 5, data_dir = \"data\")\n\nUpdated audit.feed03_audit_fees is available.\n\n\nBeginning file download at 2026-02-26 14:30:37 UTC.\n\n\nCompleted file download at 2026-02-26 14:30:37 UTC.\n\n\nBut here we see that file_accepted is stored as a string (and auditor_fkey is a floating-point value).\n\nload_parquet(\"feed03_audit_fees\", \"audit\", data_dir = \"data\") |&gt; collect()\n\n# A tibble: 5 × 2\n  auditor_fkey file_accepted      \n         &lt;dbl&gt; &lt;chr&gt;              \n1            5 2001-03-28 00:00:00\n2            5 2002-03-25 00:00:00\n3            4 2003-03-31 09:19:45\n4            6 2004-04-06 14:34:14\n5            6 2005-04-04 11:55:05\n\n\nFortunately, with wrds_update_pq(), I can specify the (Arrow) data types for selected columns and, in the case of timestamp, the time zone.\n\nwrds_update_pq(\"feed03_audit_fees\", \"audit\", \n               col_types = list(auditor_fkey = \"int32\",\n                                file_accepted = \"timestamp\"),\n               tz = \"America/New_York\", force = TRUE,\n               keep = c(\"auditor_fkey\", \"file_accepted\"),\n               data_dir = \"data\")\n\nForcing update based on user request.\n\n\nBeginning file download at 2026-02-26 14:30:38 UTC.\n\n\nApplying tz='America/New_York' to 1 timestamp column(s): file_accepted.\n\n\nCompleted file download at 2026-02-26 14:30:39 UTC.\n\n\nNow things look much better.\n\nload_parquet(\"feed03_audit_fees\", \"audit\", data_dir = \"data\") |&gt;\n  head() |&gt;\n  collect()\n\n# A tibble: 6 × 2\n  auditor_fkey file_accepted      \n         &lt;int&gt; &lt;dttm&gt;             \n1            5 2001-03-28 05:00:00\n2            5 2002-03-25 05:00:00\n3            4 2003-03-31 14:19:45\n4            6 2004-04-06 18:34:14\n5            6 2005-04-04 15:55:05\n6            6 2006-03-28 21:32:12"
  },
  {
    "objectID": "published/data_mgt_r.html#other-ideas",
    "href": "published/data_mgt_r.html#other-ideas",
    "title": "Data management ideas for researchers (R version)",
    "section": "2.3 Other ideas",
    "text": "2.3 Other ideas\nThere are several ideas not covered by this note currently, but that might be added later:\n\nBack up your data\nModification of raw data files\nThe WRDS web query\n\nIn the last case, don’t use it! (I will explain why, but one issue is reproducibility.)"
  },
  {
    "objectID": "published/data_mgt_r.html#footnotes",
    "href": "published/data_mgt_r.html#footnotes",
    "title": "Data management ideas for researchers (R version)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAs academic researchers generally get Compustat data through Wharton Research Data Services (WRDS), I refer to this dataset using the nomenclature used by WRDS. Here “global” means “not the United States (or Canada)”.↩︎\nThere can be reasonable explanations for my co-author’s stance. From some discussions I’ve had, it seems that many authors are worried about reviewers querying any change in any number in any table. While I do not understand why “because I updated Compustat” wouldn’t be an adequate response to “why did the coefficients change?” query, I guess many authors put a high priority on triggering as few questions as possible in a far-from-perfect review process. Another reason for this stance is that many researchers have a very manual research process, so changing an input data set means changing many other things, including re-typing the coefficients in the Word document containing the paper or re-exporting the data to Excel to make any plots.↩︎\nWRDS data are not project-specific data sets pretty much by definition.↩︎\nI’d guess that such a thing would amount to the equivalent of SQL’s INSERT, UPDATE, and DELETE commands.↩︎\nIf table_name is omitted and schema is specified, then the function will return a tibble with information on the files in the data directory for the schema (if archive = FALSE, as is the default) or in the archive directory (if archive = TRUE).↩︎\nThe information returned by PROC CONTENTS is assumed to be expressed in US Eastern local time (i.e., America/New_York). The PostgreSQL comments generally only indicate a date, and the db2pq assumes that the update occurred at 02:00 US Eastern time.↩︎\nIn addition to pq_restore(), the db2pq package also offers pq_archive() functions.↩︎\nAn update can always be forced using force = TRUE.↩︎\nOf course, if a user cared about performance with data manipulation, he probably wouldn’t be using Stata to begin with.↩︎\nEach of the two Voyager spacecraft, launched by NASA in 1977, carry the Voyager Golden Record, a gold-plated copper phonograph record intended as a message to any intelligent extraterrestrial lifeforms that might encounter the probes. If we wanted to give data to such lifeforms, I think it would be (quoted) CSV data and written on paper.↩︎\nBecause the data appear to be sorted by bank ID, I should retrieve the observation above, even though I’m only getting 100 rows of data.↩︎\nThe only option that should be used with PostgreSQL is TIMESTAMP WITH TIME ZONE.↩︎\nHere I use keep to limit my download to the fields of interest.↩︎"
  },
  {
    "objectID": "published/curate_call_reports.html",
    "href": "published/curate_call_reports.html",
    "title": "Data curation: The case of Call Reports",
    "section": "",
    "text": "I recently (Gow, 2026) proposed an extension to the data science “whole game” of R for Data Science (Wickham et al., 2023). In Gow (2026), I used Australian stock price data to illustrate the data curation process and, in this note, I use US bank “Call Report” data as a second illustration. In effect, I provide complete instructions for building a high-performance data library covering all Call Reports data provided by the FFIEC Bulk Data website that can be constructed in less than ten minutes on fast hardware (or a couple of hours on an older machine). I also give a few brief demonstrations of how to use the curated data, with examples for both R and Python. I conclude by discussing challenges encountered during processing and offering some observations about AI and data curation.\nMy extension of the data science “whole game”—depicted in Figure 1 below—adds a persist step to the original version, groups it with import and tidy into a single process, which I call Curate. As a complement to the new persist step, I also add a load step to the Understand process.1\nFigure 1: A representation of the data science workflow\nIn this note, as in Gow (2026), I focus on the data curation (Curate) process. My rationale for separating Curate from Understand is that I believe that thinking about these separately clarifies certain best practices in the curation of data. In Gow (2026), I used the notion of a service-level agreement to explain how the two processes can be delineated. My conception of Curate (Gow, 2026) encompasses some tasks that are included in the transform step (part of the Understand process) of Wickham et al. (2023).\nWhile I will argue that even the sole analyst who will perform all three processes can benefit from thinking about Curate separate from Understand, it is perhaps easiest to conceive of the Curate and Understand processes as involving different individuals or organizational units of the “whole game” of a data analysis workflow. In Gow (2026), I used the idea of a service-level agreement to delineate the division of responsibilities between the Curate and Understand teams. In effect, I will act as a self-appointed, single-person, unpaid Curate team and I imagine potential users of call report data as my Understand clients."
  },
  {
    "objectID": "published/curate_call_reports.html#sec-raw-data",
    "href": "published/curate_call_reports.html#sec-raw-data",
    "title": "Data curation: The case of Call Reports",
    "section": "1.1 Getting the raw data",
    "text": "1.1 Getting the raw data\nThe FFIEC Bulk Data Download site provides the Call Report data in two forms. The first is as zipped tab-delimited data files, one for each quarter. The second is as zipped XBRL data files, one for each quarter. At the time of writing, the standard approach to getting the complete data archive amounts to pointing and clicking to download each of the roughly 100 files for each format.4\nThe FFIEC data sets are not as amenable to automated downloading as those offered by other government agencies such as the SEC (see my earlier note on XBRL data), the PCAOB (see my note on Form AP data), or even the Federal Reserve itself (I used data from the MDRM site in preparing this note). However, a group of individuals has contributed the Python package ffiec_data_collector that we can use to collect the data.5\nTo install this Python package, you first need to install Python and then install the ffiec_data_collector using a command like pip install ffiec_data_collector.\nAs discussed in Appendix E of Gow and Ding (2024), I organize my raw and processed data in a repository comprising a single parent directory and several sub-directories corresponding to various data sources and projects. For some data sets, this approach to organization facilitates switching code from using (say) data sources provided by Wharton Research Data Services (WRDS) to using local data in my data repository. I will adopt that approach for the purposes of this note.\nAs the location of my “raw data” repository is found in the the environment variable RAW_DATA_DIR, I can identify that location in Python easily. The following code specifies the download directory as the directory ffiec within my raw data repository.6\n\nimport os\nfrom pathlib import Path\nprint(os.environ['RAW_DATA_DIR'])\n\n/Users/igow/Dropbox/raw_data\n\ndownload_dir = Path(os.environ['RAW_DATA_DIR']) / \"ffiec\"\n\nHaving specified a location to put the downloaded files, it is a simple matter to adapt a script provided on the package website to download the raw data files.\n\nimport ffiec_data_collector as fdc\nimport time\n\ndownloader = fdc.FFIECDownloader(download_dir=download_dir)\n\nperiods = downloader.select_product(fdc.Product.CALL_SINGLE)\n\nresults = []\nfor period in periods[:4]:\n    \n    print(f\"Downloading {period.yyyymmdd}...\", end=\" \")\n    result = downloader.download(\n        product=fdc.Product.CALL_SINGLE,\n        period=period.yyyymmdd,\n        format=fdc.FileFormat.TSV\n    )\n    results.append(result)\n    \n    if result.success:\n        print(f\"✓ ({result.size_bytes:,} bytes)\")\n    else:\n        print(f\"✗ Failed: {result.error_message}\")\n    \n    # IMPORTANT: Be respectful to government servers\n    # Add delay between requests to avoid overloading the server\n    time.sleep(1)  # 1 second delay - adjust as needed\n\nDownloading 20251231... ✓ (6,402,952 bytes)\nDownloading 20250930... ✓ (5,687,172 bytes)\nDownloading 20250630... ✓ (6,231,175 bytes)\nDownloading 20250331... ✓ (5,704,772 bytes)\n\n# Summary\nsuccessful = sum(1 for r in results if r.success)\nprint(f\"\\nCompleted: {successful}/{len(results)} downloads\")\n\n\nCompleted: 4/4 downloads\n\n\nNote that the code above downloads just the most recent four files available on the site. Remove [:4] from the line for period in periods[:4]: to download all files. Note that the package website recommends using time.sleep(5) in place of time.sleep(1) to create a five-second delay and this may be a more appropriate choice if you are downloading all 99 files using this code. Note that the downloaded files occupy about 800 MB of disk space, so make sure you have that available if running this code.\n\n1.1.1 XBRL files\nWhile this note does not use the XBRL files, you can download them using ffiec_data_collector by simply replacing TSV with XBRL in the code above. These zip files are larger than the TSV zip files, occupying about 6 GB of disk space. The ffiec.pq package does offer some rudimentary ability to process these files, but working with them is slow. To illustrate I process just one XBRL zip file.\n\nzipfiles &lt;- ffiec_list_zips(type = \"xbrl\")\nffiec_process_xbrls(zipfiles$zipfile[1]) |&gt; system_time()\n\n   user  system elapsed \n190.595  69.567 261.233 \n\n\n# A tibble: 1 × 4\n  zipfile                               date_raw date       parquet             \n  &lt;chr&gt;                                 &lt;chr&gt;    &lt;date&gt;     &lt;chr&gt;               \n1 FFIEC CDR Call Bulk XBRL 03312001.zip 20010331 2001-03-31 xbrl_20010331.parqu…"
  },
  {
    "objectID": "published/curate_call_reports.html#processing-the-data",
    "href": "published/curate_call_reports.html#processing-the-data",
    "title": "Data curation: The case of Call Reports",
    "section": "1.2 Processing the data",
    "text": "1.2 Processing the data\nWith the raw data files in hand, the next task is to process these into files useful for analysis. For reasons I will discuss below, I will process the data into Parquet files. The Parquet format is described in R for Data Science (Wickham et al., 2023, p. 393) as “an open standards-based format widely used by big data systems.” Parquet files provide a format optimized for data analysis, with a rich type system. More details on the Parquet format can be found in Chapter 22 of Wickham et al. (2023) and every code example in Gow and Ding (2024) can be executed against Parquet data files created using my db2pq Python package as described in Appendix E of that book.\nThe easiest way to run the code I used to process the data is to install the ffiec.pq R package I have made available on GitHub. And the easiest way to use the package is to set the locations for the downloaded raw data files from above and for the processed data using the environment variables RAW_DATA_DIR and DATA_DIR, respectively. By default, the ffiec.pq package assumes that the raw data files can be found in a directory ffiec that is a subdirectory of RAW_DATA_DIR. Also, the ffiec.pq package will place the processed data it creates in a directory ffiec that is a subdirectory of DATA_DIR.\nI already have these environment variables set:\n\nSys.getenv(\"RAW_DATA_DIR\")\n\n[1] \"/Users/igow/Dropbox/raw_data\"\n\nSys.getenv(\"DATA_DIR\")\n\n[1] \"/Users/igow/Dropbox/pq_data\"\n\n\nBut, even if I did not, I could set them within R using commands like the following. You should set RAW_DATA_DIR to match what you used above in Python and you should set DATA_DIR to point to the location where you want to put the processed files. The processed files will occupy about 3 GB of disk space, so make sure you have room for these there.\n\nSys.setenv(RAW_DATA_DIR=\"/Users/igow/Dropbox/raw_data\")\nSys.setenv(DATA_DIR=\"/Users/igow/Dropbox/pq_data\")\n\nHaving set these environment variables, I can load my package and run a single command ffiec_process() without any arguments to process all the raw data files.7 This takes about five minutes to run (for me):\n\nresults &lt;- ffiec_process(use_multicore = TRUE) |&gt; system_time()\n\n   user  system elapsed \n  7.297   2.181 200.868 \n\n\nNote that, behind the scenes, the ffiec_process() extracts the data in two phases. In the first phase, it processes the data for each schedule for each quarter into Parquet file. This results in 3713 Parquet files. In the second phase, ffiec_process() proceeds to organize the data in the 3713 Parquet files by variable type to facilitate working with the data. Once the data have been organized, the 3713 schedule-and-quarter-specific Parquet files are discarded.\nThe results table returned by the ffiec_process() function above reflects the outcome of the first phase, as that is when any problems arising from malformed data are expected to arise. If there were any issues in reading the data for a schedule in a quarter, then the variable ok for the corresponding row of results will be FALSE. We can easily confirm that all rows have ok equal to TRUE:\n\nresults |&gt; count(ok)\n\n# A tibble: 1 × 2\n  ok        n\n  &lt;lgl&gt; &lt;int&gt;\n1 TRUE   3713\n\n\nThe results table also includes the field repairs that we can inspect to determine if any “repairs” were made to the data as it was processed in the first phase. As can be seen, a minority of the 3713 first-phase files needed repairs. I discuss these repairs in more detail in Section 3.1.1.\n\nresults |&gt;\n  unnest(repairs) |&gt;\n  count(repairs)\n\n# A tibble: 2 × 2\n  repairs          n\n  &lt;chr&gt;        &lt;int&gt;\n1 newline-gsub   102\n2 tab-repair       2"
  },
  {
    "objectID": "published/curate_call_reports.html#using-the-data-with-r",
    "href": "published/curate_call_reports.html#using-the-data-with-r",
    "title": "Data curation: The case of Call Reports",
    "section": "2.1 Using the data with R",
    "text": "2.1 Using the data with R\nSo what has the ffiec.pq package just done? In a nutshell, it have processed each of the nearly 100 zip files into seven Parquet files, and I discuss these in turn.\n\n2.1.1 “Panel of Reporters” (POR) data\nThe first file is the “Panel of Reporters” (POR) table, which provides details on the financial institutions filing in the respective quarter.\nTo access the data using the ffiec.pq functions, we just need to create a connection to an in-memory DuckDB database, which is a simple one-liner.\n\ndb &lt;- dbConnect(duckdb::duckdb())\n\nFrom there we have the option to load a single Parquet file using the pq_file argument of the ffiec_scan_pqs() function:8\n\npor_20250930 &lt;- ffiec_scan_pqs(db, pq_file=\"por_20250930.parquet\")\npor_20250930 |&gt;\n  select(IDRSSD, financial_institution_name, everything()) |&gt;\n  head() |&gt;\n  collect()\n\n# A tibble: 6 × 13\n  IDRSSD financial_institution_name    fdic_certificate_num…¹ occ_charter_number\n   &lt;int&gt; &lt;chr&gt;                         &lt;chr&gt;                  &lt;chr&gt;             \n1     37 BANK OF HANCOCK COUNTY        10057                  &lt;NA&gt;              \n2    242 FIRST COMMUNITY BANK XENIA-F… 3850                   &lt;NA&gt;              \n3    279 BROADSTREET BANK, SSB         28868                  &lt;NA&gt;              \n4    354 BISON STATE BANK              14083                  &lt;NA&gt;              \n5    457 LOWRY STATE BANK              10202                  &lt;NA&gt;              \n6    505 BALLSTON SPA NATIONAL BANK    6959                   1253              \n# ℹ abbreviated name: ¹​fdic_certificate_number\n# ℹ 9 more variables: ots_docket_number &lt;chr&gt;,\n#   primary_aba_routing_number &lt;chr&gt;, financial_institution_address &lt;chr&gt;,\n#   financial_institution_city &lt;chr&gt;, financial_institution_state &lt;chr&gt;,\n#   financial_institution_zip_code &lt;chr&gt;,\n#   financial_institution_filing_type &lt;chr&gt;,\n#   last_date_time_submission_updated_on &lt;dttm&gt;, date &lt;date&gt;\n\npor_20250930 |&gt; count() |&gt; collect()\n\n# A tibble: 1 × 1\n      n\n  &lt;dbl&gt;\n1  4435\n\n\nBut it will generally be more convenient to just read all files in one step, which we can do like this.\n\npor &lt;- ffiec_scan_pqs(db, \"por\")\npor |&gt;\n  select(IDRSSD, financial_institution_name, everything()) |&gt;\n  head() |&gt;\n  collect() \n\n# A tibble: 6 × 13\n  IDRSSD financial_institution_name    fdic_certificate_num…¹ occ_charter_number\n   &lt;int&gt; &lt;chr&gt;                         &lt;chr&gt;                  &lt;chr&gt;             \n1     37 BANK OF HANCOCK COUNTY        10057                  &lt;NA&gt;              \n2    242 FIRST NATIONAL BANK OF XENIA… 3850                   12096             \n3    279 MINEOLA COMMUNITY BANK, SSB   28868                  &lt;NA&gt;              \n4    354 BISON STATE BANK              14083                  &lt;NA&gt;              \n5    439 PEOPLES BANK                  16498                  &lt;NA&gt;              \n6    457 LOWRY STATE BANK              10202                  &lt;NA&gt;              \n# ℹ abbreviated name: ¹​fdic_certificate_number\n# ℹ 9 more variables: ots_docket_number &lt;chr&gt;,\n#   primary_aba_routing_number &lt;chr&gt;, financial_institution_address &lt;chr&gt;,\n#   financial_institution_city &lt;chr&gt;, financial_institution_state &lt;chr&gt;,\n#   financial_institution_zip_code &lt;chr&gt;,\n#   financial_institution_filing_type &lt;chr&gt;,\n#   last_date_time_submission_updated_on &lt;dttm&gt;, date &lt;date&gt;\n\npor |&gt; count() |&gt; collect()\n\n# A tibble: 1 × 1\n       n\n   &lt;dbl&gt;\n1 662363\n\n\n\n\n2.1.2 Item-schedules data\nThe second data set is ffiec_schedules. The zip files provided by the FFIEC Bulk Data site comprise several TSV files organized into “schedules” corresponding the particular forms on which the data are submitted by filers. While the ffiec.pq package reorganizes the data by data type, information about the original source files for the data are retained in ffiec_schedules. We can load this using the following code:\n\nffiec_schedules &lt;- ffiec_scan_pqs(db, \"ffiec_schedules\")\n\nAnd here are the first 10 rows of this data set.\n\nffiec_schedules |&gt; head(10) |&gt; collect()\n\n# A tibble: 10 × 3\n   item     schedule  date      \n   &lt;chr&gt;    &lt;list&gt;    &lt;date&gt;    \n 1 RCFD0010 &lt;chr [2]&gt; 2001-03-31\n 2 RCFD0022 &lt;chr [1]&gt; 2001-03-31\n 3 RCFD0071 &lt;chr [1]&gt; 2001-03-31\n 4 RCFD0073 &lt;chr [1]&gt; 2001-03-31\n 5 RCFD0074 &lt;chr [1]&gt; 2001-03-31\n 6 RCFD0081 &lt;chr [1]&gt; 2001-03-31\n 7 RCFD0083 &lt;chr [1]&gt; 2001-03-31\n 8 RCFD0085 &lt;chr [1]&gt; 2001-03-31\n 9 RCFD0090 &lt;chr [1]&gt; 2001-03-31\n10 RCFD0211 &lt;chr [1]&gt; 2001-03-31\n\n\nFocusing on one item, RIAD4230, we can see from the output below that this item was provided on both Schedule RI (ri) and Schedule RI-BII (ribii) from 2001-03-31 until 2018-12-31, but since then has only been provided on Schedule RI-BII.\n\nffiec_schedules |&gt; \n  filter(item == \"RIAD4230\") |&gt;\n  mutate(schedule = unnest(schedule)) |&gt;\n  group_by(item, schedule) |&gt;\n  summarize(min_date = min(date, na.rm = TRUE),\n            max_date = max(date, na.rm = TRUE),\n            .groups = \"drop\") |&gt;\n  collect()\n\n# A tibble: 2 × 4\n  item     schedule min_date   max_date  \n  &lt;chr&gt;    &lt;chr&gt;    &lt;date&gt;     &lt;date&gt;    \n1 RIAD4230 ri       2001-03-31 2018-12-31\n2 RIAD4230 ribii    2001-03-31 2025-12-31\n\n\nThe next question might be: What is RIAD4230? We can get the answer from ffiec_items, a data set included with the ffiec.pq package:\n\nffiec_items |&gt; filter(item == \"RIAD4230\")\n\n# A tibble: 1 × 5\n  item     mnemonic item_code item_name                           data_type\n  &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;                               &lt;chr&gt;    \n1 RIAD4230 RIAD     4230      Provision for loan and lease losses Float64  \n\n\nSchedule RI is the income statement and “Provision for loan and lease losses” is an expense we would expect to see there for a financial institution. Schedule RI-BII is “Charge-offs and Recoveries on Loans and Leases” and provides detail on loan charge-offs and recoveries, broken out by loan category, for the reporting period. As part of processing the data, the ffiec.pq package confirms that the value for any given item for a specific IDRSSD and date is the same across schedules for all items in the data.\nEach of the other five files represents data from the schedules for that quarter for a particular data type, as shown in Table 1:\n\n\n\nTable 1: Table keys, arrow types, and access code\n\n\n\n\n\nKey\nArrow type\nAccess code\n\n\n\n\nfloat\nFloat64\nffiec_scan_pqs(db, \"ffiec_float\")\n\n\nint\nInt32\nffiec_scan_pqs(db, \"ffiec_int\")\n\n\nstr\nUtf8\nffiec_scan_pqs(db, \"ffiec_str\")\n\n\ndate\nDate32\nffiec_scan_pqs(db, \"ffiec_date\")\n\n\nbool\nBoolean\nffiec_scan_pqs(db, \"ffiec_bool\")\n\n\n\n\n\n\nWe can use the data set ffiec_items to find out where a variable is located, based on its Arrow type.\n\nffiec_items\n\n# A tibble: 5,141 × 5\n   item     mnemonic item_code item_name                               data_type\n   &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;                                   &lt;chr&gt;    \n 1 RCFA2170 RCFA     2170      Total assets                            Float64  \n 2 RCFA3128 RCFA     3128      Allocated transfer risk reserves        Float64  \n 3 RCFA3792 RCFA     3792      Total qualifying capital allowable und… Float64  \n 4 RCFA5310 RCFA     5310      General loan and lease valuation allow… Float64  \n 5 RCFA5311 RCFA     5311      Tier 2 (supplementary) capital          Float64  \n 6 RCFA7204 RCFA     7204      Tier 1 leverage capital ratio           Float64  \n 7 RCFA7205 RCFA     7205      Total risk-based capital ratio          Float64  \n 8 RCFA7206 RCFA     7206      Tier 1 risk-based capital ratio         Float64  \n 9 RCFA8274 RCFA     8274      Tier 1 capital allowable under the ris… Float64  \n10 RCFAA223 RCFA     A223      Risk-weighted assets (net of allowance… Float64  \n# ℹ 5,131 more rows\n\n\nAs might be expected, most variables have type Float64 and will be found in the ffiec_float tables.\n\nffiec_items |&gt; count(data_type, sort = TRUE)\n\n# A tibble: 5 × 2\n  data_type     n\n  &lt;chr&gt;     &lt;int&gt;\n1 Float64    4909\n2 Int32       115\n3 String       73\n4 Boolean      42\n5 Date32        2\n\n\n\n\n2.1.3 Example 1: Do bank balance sheets balance?\nIf we were experts in Call Report data, we might know that domestic total assets is reported as item RCFD2170 (on Schedule RC) for banks reporting on a consolidated basis and as item RCON2170 for banks reporting on an unconsolidated basis. We might also know about RCFD2948 and RCFD3210 and so on. But we don’t need to be experts to see what these items relate to:\n\nbs_items &lt;- c(\"RCFD2170\", \"RCON2170\",\n              \"RCFD2948\", \"RCON2948\",\n              \"RCFD3210\", \"RCON3210\",\n              \"RCFD3000\", \"RCON3000\")\n\nffiec_items |&gt; filter(item %in% bs_items)\n\n# A tibble: 8 × 5\n  item     mnemonic item_code item_name                                data_type\n  &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;                                    &lt;chr&gt;    \n1 RCFD2170 RCFD     2170      Total assets                             Float64  \n2 RCFD2948 RCFD     2948      Total liabilities and minority interest  Float64  \n3 RCFD3000 RCFD     3000      Minority interest in consolidated subsi… Float64  \n4 RCFD3210 RCFD     3210      Total equity capital                     Float64  \n5 RCON2170 RCON     2170      Total assets                             Float64  \n6 RCON2948 RCON     2948      Total liabilities and minority interest  Float64  \n7 RCON3000 RCON     3000      Minority interest in consolidated subsi… Float64  \n8 RCON3210 RCON     3210      Total equity capital                     Float64  \n\n\nThe output above suggests we can make a “top level” balance sheet table using these items. The following code uses the DuckDB instance we created above (db) and the code provided in Table 1, to create ffiec_float, a “lazy” data table. Here “lazy” is a good thing, as it means we have access to all the data without having to load anything into RAM. As a result, this operation takes almost no time.\n\nffiec_float &lt;- ffiec_scan_pqs(db, \"ffiec_float\") |&gt; system_time()\n\n   user  system elapsed \n  0.036   0.016   0.015 \n\nffiec_float\n\n# A query:  ?? x 4\n# Database: DuckDB 1.4.4 [igow@Darwin 25.4.0:R 4.5.2/:memory:]\n   IDRSSD date       item        value\n    &lt;int&gt; &lt;date&gt;     &lt;chr&gt;       &lt;dbl&gt;\n 1     37 2001-03-31 RCON3562   0     \n 2     37 2001-03-31 RCON7701   0     \n 3     37 2001-03-31 RCON7702   0     \n 4    242 2001-03-31 RCON3562 329     \n 5    242 2001-03-31 RCON7701   0.08  \n 6    242 2001-03-31 RCON7702   0.085 \n 7    279 2001-03-31 RCON3562  27     \n 8    279 2001-03-31 RCON7701   0.0745\n 9    279 2001-03-31 RCON7702   0.08  \n10    354 2001-03-31 RCON3562   4     \n# ℹ more rows\n\n\nAs can be seen, the data in ffiec_float are in a long format, with each item for each bank for each period being a single row.\nI then filter() to get data on just the items in bs_items and then use the the convenience function ffiec_pivot() from the ffiec.pq package to turn the data into a more customary wide form. I then use coalesce() to get the consolidated items (RCFD) where available and the unconsolidated items (RCON) otherwise. Because I compute() this table (i.e., actually calculate the values for each row and column), this step takes a relatively long time, but not too long given that the underlying data files are in the order of tens of gigabytes if loaded in RAM.9\n\nbs_data &lt;-\n  ffiec_float |&gt; \n  ffiec_pivot(items = bs_items) |&gt;\n  mutate(total_assets = coalesce(RCFD2170, RCON2170),\n         total_liabilities = coalesce(RCFD2948, RCON2948),\n         equity = coalesce(RCFD3210, RCON3210),\n         nci = coalesce(RCFD3000, RCON3000)) |&gt;\n  mutate(eq_liab = total_liabilities + equity + nci) |&gt;\n  compute() |&gt;\n  system_time()\n\n   user  system elapsed \n  5.341   0.666   0.748 \n\n\nSo, do balance sheets balance? Well, not always.10\n\nbs_data |&gt;\n  count(bs_balance = eq_liab == total_assets) |&gt;\n  collect()\n\n# A tibble: 2 × 2\n  bs_balance      n\n  &lt;lgl&gt;       &lt;dbl&gt;\n1 TRUE       640570\n2 FALSE       21793\n\n\nWhat’s going on? Well, one possibility is simply rounding error. So in the following code, I set imbalance_flag to TRUE only if the gap is more than 1 (these are in thousands of USD).\n\nbalanced &lt;-\n  bs_data |&gt;\n  mutate(imbalance = total_assets - eq_liab,\n         imbalance_flag = abs(total_assets - eq_liab) &gt; 1) |&gt;\n  select(-starts_with(\"RC\"), -total_liabilities) |&gt;\n  collect()\n\nThis helps a lot. Now it seems that balance sheets usually balance, but not always.\n\nbalanced |&gt;\n  count(imbalance_flag)\n\n# A tibble: 2 × 2\n  imbalance_flag      n\n  &lt;lgl&gt;           &lt;int&gt;\n1 FALSE          662164\n2 TRUE              199\n\n\nThe vast majority of apparent imbalances are small …\n\nbalanced |&gt;\n  filter(imbalance_flag) |&gt;\n  select(IDRSSD, date, total_assets, imbalance) |&gt;\n  arrange(desc(imbalance))\n\n# A tibble: 199 × 4\n    IDRSSD date       total_assets imbalance\n     &lt;int&gt; &lt;date&gt;            &lt;dbl&gt;     &lt;dbl&gt;\n 1 1362246 2002-06-30       278586      5334\n 2  664653 2002-03-31        25490       224\n 3 2821825 2001-03-31        55220        30\n 4  678931 2001-09-30        31237        10\n 5  178150 2004-03-31       103939        10\n 6  842358 2002-06-30       202388        10\n 7 3097243 2005-06-30        40939        10\n 8  545604 2001-06-30       246223        10\n 9  293053 2002-03-31       314157        10\n10  920733 2001-12-31        48439        10\n# ℹ 189 more rows\n\n\n… and they’re all at least twenty years ago.\n\nbalanced |&gt;\n  filter(imbalance_flag) |&gt;\n  select(IDRSSD, date, total_assets, imbalance) |&gt;\n  arrange(desc(date))\n\n# A tibble: 199 × 4\n    IDRSSD date       total_assets imbalance\n     &lt;int&gt; &lt;date&gt;            &lt;dbl&gt;     &lt;dbl&gt;\n 1   33549 2005-06-30        43865         3\n 2 3097243 2005-06-30        40939        10\n 3   20147 2005-06-30        48130       -10\n 4   22954 2005-06-30        65027       -10\n 5  773546 2005-06-30        97558         2\n 6  475756 2005-06-30        68158         2\n 7  131940 2005-06-30        27432         8\n 8   42037 2005-03-31        60913         2\n 9 2646327 2005-03-31        98814        -8\n10  827953 2005-03-31        50080         5\n# ℹ 189 more rows\n\n\n\n\n2.1.4 Example 2: When do banks submit their Call Reports?\nWorking with dates and times (temporal data) can be a lot more complicated than is generally appreciated. Broadly speaking we might think of temporal data as referring to points in time, or instants, or to time spans, which include durations, periods, and intervals.11 Instants might refer to moments in time (e.g., in UTC) or as times of day (in local time).\nSuppose I were interested in understanding the times of day at which financial institutions submit their Call Reports. It seems that financial institutions file Call Reports with their primary federal regulator through the FFIEC’s Central Data Repository. So I am going to use the America/New_York time zone as the relevant local time zone for this analysis, as the FFIEC is based in Washington, DC.\nTo illustrate some subtleties of working with time zones, I will set my computer to a different time zone from that applicable to where I am: Australia/Sydney, as seen in Figure 2.12\n\n\n\n\n\n\n\n\nFigure 2: Setting my computer to a different time zone\n\n\n\n\n\nNow, R sees my time zone as Australia/Sydney:\n\nSys.timezone()\n\n[1] \"America/New_York\"\n\n\nIf we look at the underlying zip file for 2025-09-30, you will see that last_date_time_submission_updated_on for the bank with IDRSSD of 37 is \"2026-01-13T10:13:21\". In creating the ffiec.pq package, I assumed that this is a timestamp in America/New_York time.\nHow does that show up when I look at the processed data in R using DuckDB?\n\ndb &lt;- dbConnect(duckdb::duckdb())\n\npor &lt;- ffiec_scan_pqs(db, \"por\")\n\npor_default &lt;-\n  por |&gt; \n  filter(IDRSSD == 37, date == \"2025-09-30\") |&gt; \n  rename(dttm = last_date_time_submission_updated_on) |&gt;\n  mutate(dttm_text = as.character(dttm)) |&gt; \n  select(IDRSSD, date, dttm, dttm_text) |&gt; \n  collect()\n\ndbDisconnect(db)\n\npor_default\n\n# A tibble: 1 × 4\n  IDRSSD date       dttm                dttm_text             \n   &lt;int&gt; &lt;date&gt;     &lt;dttm&gt;              &lt;chr&gt;                 \n1     37 2025-09-30 2026-01-13 15:13:21 2026-01-13 15:13:21+00\n\npor_default$dttm[1]\n\n[1] \"2026-01-13 15:13:21 UTC\"\n\n\nBy default, R/DuckDB is showing this to me as UTC. This is fine, but I want to analyse this as a local time. Here is how I can achieve this. First, I set the R variable tz to \"America/New_York\".\n\ntz &lt;- \"America/New_York\"\n\nSecond, I connect to DuckDB anew, but I tell it I want it to use \"America/New_York\" as the time zone of output. But it’s important to note that this is just a “presentation layer” and doesn’t change how the database itself “thinks about” timestamps.\n\ndb &lt;- dbConnect(duckdb::duckdb(), timezone_out = tz)\n\nThird, I make DuckDB a “time zone wizard” but installing and loading the icu extension. This extension enables region-dependent collations and time zones. The icu extension is probably not installed and enabled by default because it is large and not all applications need these features. This allows me to set the DuckDB server’s time zone to America/New_York.\n\nrs &lt;- dbExecute(db, \"INSTALL icu\")\nrs &lt;- dbExecute(db, \"LOAD icu\")\nrs &lt;- dbExecute(db, str_glue(\"SET TimeZone TO '{tz}'\"))\n\nThen I run the query from above again.\n\npor &lt;- ffiec_scan_pqs(db, \"por\")\npor_ny &lt;-\n  por |&gt; \n  filter(IDRSSD == 37, date == \"2025-09-30\") |&gt; \n  rename(dttm = last_date_time_submission_updated_on) |&gt;\n  mutate(dttm_text = as.character(dttm)) |&gt; \n  select(IDRSSD, date, dttm, dttm_text) |&gt;\n  collect()\n\npor_ny\n\n# A tibble: 1 × 4\n  IDRSSD date       dttm                dttm_text             \n   &lt;int&gt; &lt;date&gt;     &lt;dttm&gt;              &lt;chr&gt;                 \n1     37 2025-09-30 2026-01-13 10:13:21 2026-01-13 10:13:21-05\n\npor_ny$dttm[1]\n\n[1] \"2026-01-13 10:13:21 EST\"\n\n\nNow, we see that everything is in America/New_York local time, including the way the server sees the data (dttm_text) and how it’s presented to the R user.\nNow that we have things working in local time, I will make a couple of plots of submission times. To show times on a single scale, I use the fudge of making them all times on a given day, which I somewhat arbitrarily choose to be 2025-01-01.13\nIn the first plot—Figure 3—I present submission times divided by whether banks are located in “western states” or not. It does seem that western banks file later.\n\nwestern_states &lt;- c(\"HI\", \"WA\", \"CA\", \"AK\", \"OR\", \"NV\")\n\nplot_data &lt;-\n  por |&gt; \n  rename(last_update = last_date_time_submission_updated_on) |&gt;\n  mutate(\n    q4 = quarter(date) == 4,\n    offset = last_update - sql(\"last_update AT TIME ZONE 'UTC'\"),\n    offset = date_part(\"epoch\", offset) / 3600,\n    tzone = if_else(offset == -4, \"EDT\", \"EST\"),\n    west = financial_institution_state %in% western_states,\n    ref = sql(str_glue(\"TIMESTAMPTZ '2025-01-01 00:00:00 {tz}'\")),\n    sub_date = date_trunc('days', last_update)) |&gt;\n  mutate(time_adj = last_update - sub_date + ref) |&gt;\n  select(IDRSSD, date, last_update, time_adj, west, q4, offset, tzone) |&gt;\n  collect()\n\n\n\n\n\n\n\n\n\nFigure 3: Submission times by year and region\n\n\n\n\n\nIn the second plot—Figure 4—I present submission times divided by whether America/New_York is on Eastern Daylight Time (EDT) or Eastern Standard Time (EST). Looking at the plot, it seems that submission times have a similar distribution across the two time zones, suggesting that banks do not follow UTC, in which case there should be a difference in distributions for EDT and EST.\nOne can definitely see a “lunch hour” and the submissions appear more likely to involve someone clicking a “Submit” button in some software package rather than IT setting up some overnight automated submission.\n\n\n\n\n\n\n\n\nFigure 4: Submission times by year and US/Eastern time zone"
  },
  {
    "objectID": "published/curate_call_reports.html#using-the-data-with-python",
    "href": "published/curate_call_reports.html#using-the-data-with-python",
    "title": "Data curation: The case of Call Reports",
    "section": "2.2 Using the data with Python",
    "text": "2.2 Using the data with Python\n\n2.2.1 Example 3: Plotting trends in total assets for the biggest banks\nLest you think that, because ffiec.pq is an R package, the processed data are of no interest to others, I now provide some basic analysis using Python. For this, I am going to use the Polars package rather than pandas.\nWhile pandas is the dominant data frame library in Python, it would struggle to work with Parquet data on the scale of what ffiec.pq has produced, even though it’s a fairly modest amount of data. Loading 50-100 GB of data into RAM is not fun for most people’s computer set-ups. Even if you have RAM in the hundreds of GBs, not loading it will save you time.\nAs we shall see, Polars does fine with the data and, if anything, is noticeably faster than DuckDB (using dplyr) for the queries I use in this note.\n\nfrom pathlib import Path\nimport polars as pl\nimport os\n\nBecause there is no ffiec.pq package for Python, I mimic the ffiec_scan_pqs() function using the following code.\n\ndef ffiec_scan_pqs(schedule=None, *, \n                   schema=\"ffiec\", data_dir=None):\n    if data_dir is None:\n        data_dir = Path(os.environ[\"DATA_DIR\"]).expanduser()\n\n    path = data_dir / schema if schema else data_dir\n\n    if schedule is None:\n        raise ValueError(\"You must supply `schedule`.\")\n    files = list(path.glob(f\"{schedule}_*.parquet\"))\n    if not files:\n        raise FileNotFoundError(\n          f\"No Parquet files found for schedule '{schedule}' in {path}\"\n        )\n    \n    return pl.concat([pl.scan_parquet(f) for f in files])\n\nNow I can “load” the data much as I did with R.\n\nffiec_float = ffiec_scan_pqs(\"ffiec_float\")\npor = ffiec_scan_pqs(schedule=\"por\")\n\nWhile I am going to focus on total assets in this analysis, I show the parallels between the R code and the Polars code by collecting data on the same items. I don’t need ffiec_pivot() with Polars because the built-in .pivot() method does everything I need. Polars is even faster than R/DuckDB.\n\nimport time\n\nbs_items = [\"RCFD2170\", \"RCON2170\",\n            \"RCFD2948\", \"RCON2948\",\n            \"RCFD3210\", \"RCON3210\",\n            \"RCFD3000\", \"RCON3000\"]\nstart = time.perf_counter()\nbs_data = (\n    ffiec_float\n    .filter(pl.col(\"item\").is_in(bs_items))\n    .pivot(\n        on = \"item\",\n        on_columns = bs_items,\n        index = [\"IDRSSD\", \"date\"],\n        values = \"value\")\n    .with_columns(\n        total_assets = pl.coalesce(pl.col(\"RCFD2170\"), pl.col(\"RCON2170\")),\n        total_liabs = pl.coalesce(pl.col(\"RCFD2948\"), pl.col(\"RCON2948\")),\n        equity = pl.coalesce(pl.col(\"RCFD3210\"), pl.col(\"RCON3210\")),\n        nci = pl.coalesce(pl.col(\"RCFD3000\"), pl.col(\"RCON3000\")),\n    )\n    .with_columns(\n        eq_liab = pl.col(\"total_liabs\") + pl.col(\"equity\") + pl.col(\"nci\")\n    )\n    .collect()\n)\nend = time.perf_counter()\nelapsed = end - start\nprint(f'Time taken: {elapsed:.6f} seconds')\n\nTime taken: 0.221169 seconds\n\n\nWe can peek at the data. So Polars has processed GBs of data and created a table with over 600,000 rows in well under a second. Don’t try this at home … if you’re using pandas.\nNote that ffiec_float is a pl.LazyFrame, but .collect() creates a non-lazy pl.DataFrame.\n\nimport polars.selectors as cs\n\nbs_data.select(cs.exclude(\"^(RCON|RCFD).*$\"))\n\n\nshape: (662_363, 7)\n\n\n\nIDRSSD\ndate\ntotal_assets\ntotal_liabs\nequity\nnci\neq_liab\n\n\ni32\ndate\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n3715800\n2011-09-30\n96424.0\n84988.0\n11436.0\n0.0\n96424.0\n\n\n490937\n2008-06-30\n114703.0\n103084.0\n11619.0\n0.0\n114703.0\n\n\n570651\n2017-06-30\n53550.0\n47769.0\n5781.0\n0.0\n53550.0\n\n\n257756\n2002-09-30\n27819.0\n25752.0\n2067.0\n0.0\n27819.0\n\n\n863362\n2007-09-30\n16172.0\n13820.0\n2352.0\n0.0\n16172.0\n\n\n…\n…\n…\n…\n…\n…\n…\n\n\n858210\n2011-06-30\n356242.0\n325617.0\n30625.0\n0.0\n356242.0\n\n\n54441\n2008-12-31\n120192.0\n108404.0\n11788.0\n0.0\n120192.0\n\n\n374934\n2004-06-30\n235713.0\n214266.0\n21447.0\n0.0\n235713.0\n\n\n703039\n2024-09-30\n181001.0\n149305.0\n31696.0\n0.0\n181001.0\n\n\n535931\n2025-06-30\n243730.0\n227707.0\n16023.0\n0.0\n243730.0\n\n\n\n\n\n\nI identify the top 5 banks by assets on 2025-09-30. Because I will want to merge this with information on por, which is a lazy data frame (pl.LazyFrame), I make it top_5 “lazy” by appending .lazy() at the end.\n\ntop_5 = (\n    bs_data\n    .filter(pl.col(\"date\") == pl.date(2025, 9, 30))\n    .sort(\"total_assets\", descending=True)\n    .with_row_index(\"ta_rank\", offset=1)\n    .filter(pl.col(\"ta_rank\") &lt;= 5)\n    .lazy()\n)\n\nI then grab the names of the banks from por.\n\ntop_5_names = (\n    top_5\n    .join(\n        por.select([\"IDRSSD\", \"date\", \"financial_institution_name\"]),\n        on=[\"IDRSSD\", \"date\"],\n        how=\"inner\",\n    )\n    .sort(\"ta_rank\")\n    .select([\"IDRSSD\", \"financial_institution_name\", \"ta_rank\"])\n    .rename({\"financial_institution_name\": \"bank\"})\n    .collect()\n)\n\n\ntop_5_names\n\n\nshape: (5, 3)\n\n\n\nIDRSSD\nbank\nta_rank\n\n\ni32\nstr\nu32\n\n\n\n\n852218\n\"JPMORGAN CHASE BANK, NATIONAL …\n1\n\n\n480228\n\"BANK OF AMERICA, NATIONAL ASSO…\n2\n\n\n476810\n\"CITIBANK, N.A.\"\n3\n\n\n451965\n\"WELLS FARGO BANK, NATIONAL ASS…\n4\n\n\n504713\n\"U.S. BANK NATIONAL ASSOCIATION\"\n5\n\n\n\n\n\n\nI can combine the names with bs_data using .join().\n\nbs_panel_data = bs_data.join(top_5_names, on=\"IDRSSD\", how=\"inner\")\n\nUsers of pandas who are unfamiliar might be impressed by the performance of Polars, but wonder how they can fit it into their workflows. Of course, it is easy enough to call .to_pandas() and create a pandas pd.DataFrame:\n\npdf = (\n    bs_panel_data\n    .filter(pl.col(\"date\") &gt;= pl.date(2020, 1, 1))\n    .to_pandas()\n)\n\nThis means a pandas user can use all the familiar tools used for plotting or statistical analysis. Because the names are a little long for plotting purposes, I use a little dictionary to replace them.\n\nbank_names = {\n    476810: \"Citibank\",\n    504713: \"US Bank\",\n    852218: \"JPMorgan Chase\",\n    451965: \"Wells Fargo\",\n    480228: \"Bank of America\",\n}\n\npdf[\"bank\"] = pdf[\"IDRSSD\"].map(bank_names)\n\nAnd then I use Seaborn and Matplotlib to make a small (but uninspiring) plot.\n\n\n\n\n\n\n\n\nFigure 5: Total assets for top 5 banks"
  },
  {
    "objectID": "published/curate_call_reports.html#reading-the-data",
    "href": "published/curate_call_reports.html#reading-the-data",
    "title": "Data curation: The case of Call Reports",
    "section": "3.1 Reading the data",
    "text": "3.1 Reading the data\nEach quarter’s zip file (zipfile) actually contains dozens of text files (.txt) in TSV (“tab-separated values”) form. The TSV is a close relative of the CSV (“comma-separated values”) and the principles applicable to one form apply to the other.\nI have seen code that just imports from these individual files (what I call inner_file) in some location on the user’s hard drive. This approach is predicated on the user having downloaded the zip files and unzipped them. While we have downloaded all the zip files—assuming you followed the steps outlined in Section 1.1—I don’t want to be polluting my hard drive (or yours) with thousands of .txt files that won’t be used after reading them once.\nInstead, R allows me to simply say:\n\ncon &lt;- unz(zipfile, inner_file)\n\nThe resulting con object is a temporary read-only connection to a single text file (inner_file) stored inside the zip file zipfile. The object allows R to stream the file’s contents directly from the zip archive, line by line, without extracting it. Given con, the core function used to read the data into R has the following basic form, where read_tsv() comes from the readr package, part of the Tidyverse:\n\ndf &lt;- read_tsv(\n  con,\n  col_names = cols,\n  col_types = colspec,\n  skip = skip,\n  quote = \"\",\n  na = c(\"\", \"CONF\"),\n  progress = FALSE,\n  show_col_types = FALSE\n)\n\n\n3.1.1 Handling embedded newlines and tabs\nExperienced users of the readr package might wince a little at the quote = \"\" argument above. What this means is that the data are not quoted. Wickham et al. (2023, p. 101) points out that “sometimes strings in a CSV file contain commas. To prevent them from causing problems, they need to be surrounded by a quoting character, like \" or '. By default, read_csv() assumes that the quoting character will be \".”\nAdapting this to our context and expanding it slightly, I would say: “sometimes strings in a TSV file contain tabs (\\t) and newline characters (\\n).14 To prevent them from causing problems, they need to be surrounded by a quoting character, like \" or '.” While this is a true statement, the TSV files provided on the FFIEC Bulk Data website are not quoted, which means that tabs and newlines characters embedded in strings will cause problems.\nThe approach taken by the ffiec.pq package is to attempt to read the data using a call like that above, which I term the “fast path” (in part because it is indeed fast). Before making that call, the code has already inspected the first row of the file to determine the column names (stored in cols) and used those column names to look up the appropriate type for each column (stored in colspecs). Any anomaly caused by embedded newlines or embedded tabs will almost certainly cause this first read_tsv() call to fail. But if there are no issues, then we pretty much have the data as we want them and can return df to the calling function.15 Fortunately, over 95% of files can be read successfully on the “fast path”.\nIt turns out that if the “fast path” read fails, the most likely culprit is embedded newlines. Let’s say the table we’re trying to read has seven columns and the text field that is the fourth field in the file contains, in some row of the data, an embedded newline, because the data submitted by the reporting financial institution contained \\n in that field. Because read_tsv() processes the data line by line and lines are “delimited” by newline characters (\\n), it will see the problematic line as terminating part way through the fourth column and, because cols tells read_tsv() to expect seven columns, read_tsv() will issue a warning.\nWhen a warning occurs on the “fast path”, the read function in ffiec.pq moves to what I call (unimaginatively) the “slow path”. A “feature” (it turns out) of the TSV files provided on the FFIEC Bulk Data website is that each line ends with not just \\n, but \\t\\n. This means we can assume that any \\n not preceded by \\t is an embedded newline, not a line-terminating endline.16 So I can read the data into the variable txt using readLines() and use a regular expression to replace embedded newlines with an alternative character. The alternative I use is a space and I use the gsub() function to achieve this: gsub(\"(?&lt;!\\\\t)\\\\n\", \" \", txt, perl = TRUE) The regular expression here is (?&lt;!\\\\t)\\\\n is equivalent to (?&lt;!\\t)\\n in Perl or r\"(?&lt;!\\t)\\n\" in Python.17 In words, the regular expression literally means “any newline character that is not immediately preceded by a tab” and the gsub() function will replace such characters with spaces (\" \") because that is the second argument to gsub().\nThis fix addresses almost all the problematic files. “Almost all” means “all but two”. The issue with the remaining two files is (as you might have guessed) embedded tabs. Unfortunately, there’s no easy “identify the embedded tabs and replace them” fix, because there’s no easy way to distinguish embedded tabs from delimiting tabs. However, we can detect the presence of embedded tabs in an affected from the existence of too many tabs in that row.\nFor one of the two “bad” files, there is only one text field, so once we detect the presence of the embedded tab, we can assume that the extra tab belongs in that field: Problem solved. For the other “bad” file, there are several text fields and, while there is just one bad row, we cannot be sure which text field has the embedded tab. The ffiec.pq package just assumes that the embedded tab belongs in the last field and moves on. This means that the textual data for one row (i.e., one financial institution) for one schedule for one quarter cannot be guaranteed to be completely correct.18 Such is life.\nEven without addressing the issue, given that only two files are affected, it’s possible to measure the “damage” created by embedded tabs.\nLet’s look at the earlier file, which turns out to affect the Schedule RIE data for The Traders National Bank (IDRSSD of 490937) for June 2004.19 Traders National Bank in Tennessee was “Tullahoma’s second oldest bank”, until changing its name to Wellworth Bank (seemingly after some mergers), and it listed total assets of $117,335,000 in the affected Call Report.\nLet’s look at the textual data we have in our Parquet files for this case:20\n\nffiec_str &lt;- ffiec_scan_pqs(db, schedule = \"ffiec_str\") \n\nffiec_str |&gt; \n  filter(IDRSSD == \"490937\", date == \"2004-06-30\") |&gt; \n  select(IDRSSD, item, value) |&gt; \n  filter(!is.na(value)) |&gt;\n  collect() |&gt;\n  system_time()\n\n   user  system elapsed \n  0.019   0.028   0.021 \n\n\n# A tibble: 6 × 3\n  IDRSSD item     value                                                   \n   &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;                                                   \n1 490937 TEXT4464 ATM Fees, Exam Fees, Dues & Chairitable Cont            \n2 490937 TEXT4467 Telephone, BanClub, Federal Res Fees, NSF and Other Loss\n3 490937 TEXT4468 Courier, Audit Tax, Deff Comp, Other                    \n4 490937 TEXT4469 ns Exp, Bus Dev, P                                      \n5 490937 TEXT3549 IENC SECURITIES                                         \n6 490937 TEXT3550 IENC CD'S                                               \n\n\nWe can compare this with what we see in the Call Report in Figure 6, where we see that the value of TEXT4468 should be something like \"Courier, Audit Tax, Deff Comp, Other\\tns Exp, Bus Dev, P\". The embedded tab has split this into \"Courier, Audit Tax, Deff Comp, Other\" for TEXT4468 and \"ns Exp, Bus Dev, P\" for TEXT4469, which should be NA. If the values for TEXT4468and TEXT4469 for Traders National Bank in June 2004 are important to your analysis, you could fix this “by hand” easily enough.\n\n\n\n\n\n\n\n\nFigure 6: Extract from June 2004 Call Report for The Traders National Bank\n\n\n\n\n\nLooking at the later file, there were embedded tabs in two rows of Schedule NARR for December 2022. I compared the values in the Parquet file with those in the Call Reports for the two affected banks and the values in the Parquet file match perfectly.21 Because Schedule NARR (“Optional Narrative Statement Concerning the Amounts Reported in the Consolidated Reports of Condition and Income”) has just one text column (TEXT6980), the fix employed by the ffiec.pq package will work without issues.22\n\n\n3.1.2 Handling missing-value sentinels\nUsers familiar with both readr and Call Report data might also have noticed the use of na = c(\"\", \"CONF\") in the call to read_tsv() above. The default value for this function is na = c(\"\", \"NA\") means that empty values and the characters NA are treated as missing values. As I saw no evidence that the export process for FFIEC Bulk Data files used \"NA\" to mark NA values, I elected not to treat \"NA\" as NA. However, a wrinkle is that the reporting firms some times populate text fields—but not numerical fields—with the value \"NA\".23 While the most sensible interpretation of such values is as NA, without further investigation it is difficult to be sure that \"NA\" is the canonical form in which firms reported NA values rather than \"N/A\" or \"Not applicable\" or some other variant.\nThis approach seems validated by the fact that I see the value \"NR\" in text fields of PDF versions of Call Reports and these values show up as empty values in the TSV files, suggesting that \"NR\", not \"NA\" is the FFIEC’s canonical way of representing NA values in these files, while \"NA\" is literally the text value \"NA\", albeit perhaps one intended by the reporting firm to convey the idea of NA. Users of the FFIEC data created by the ffiec.pq package who wish to use textual data should be alert to the possibility that values in those fields may be intended by the reporting firm to convey the idea of NA, even if they are not treated as such by the FFIEC’s process for creating the TSV files.\nThe other value in the na argument used above is \"CONF\", which denotes that the the reported value is confidential and therefore not publicly disclosed. Ideally, we might distinguish between NA, meaning “not reported by the firm to the FFIEC” or “not applicable to this firm” or things like that, from \"CONF\", meaning the FFIEC has the value, but we do not. Unfortunately, the value \"CONF\" often appears in numeric fields and there is no simple way to ask read_tsv() to record the idea that “this value is confidential”, so I just read these in as NA.24\nI say “no simple way” because there are probably workarounds that allow \"CONF\" to be distinguished from true NAs. For example, I could have chosen to have read_tsv() read all numeric fields as character fields and then convert the value CONF in such fields to a sentinel value such as Inf (R’s way of saying “infinity” or \\(\\infty\\)).25 This would not be terribly difficult, but would have the unfortunate effect of surprising users of the data who (understandably) didn’t read the manual and starting finding that the mean values of some fields are Inf. Perhaps the best way to address this would allow the user of ffiec.pq to choose that behaviour as an option, but I did not implement this feature at this time.\nIn addition to these missing values, I discovered in working with the data that the FFIEC often used specific values as sentinel values for NA. For example, \"0\" is used for some fields, while \"00000000\" is used to mark dates as missing, and \"12/31/9999 12:00:00 AM\" is used for timestamps.26 I recoded such sentinel values as NA in each case."
  },
  {
    "objectID": "published/curate_call_reports.html#storage-format",
    "href": "published/curate_call_reports.html#storage-format",
    "title": "Data curation: The case of Call Reports",
    "section": "4.1 Storage format",
    "text": "4.1 Storage format\nIn principle, the storage format should fairly minor detail determined by the needs of the Understand team. For example, if the Understand team works in Stata or Excel, then perhaps they will want the data in some kind of Stata format or as Excel files. However, I think it can be appropriate to push back on notions that data will be delivered in form that involves downgrading the data or otherwise compromises the process in a way that may ultimately add to the cost and complexity of the task for the Curate team. For example, “please send the final data as an Excel file attachment as a reply email” might be a request to be resisted because the process of converting to Excel can entail the degradation of data (e.g., time stamps or encoding of text).27 Instead it may be better to choose a more robust storage format and supply a script for turning that into a preferred format.\nOne storage format that I have used in the past would deliver data as tables in a (PostgreSQL) database. The Understand team could be given access data from a particular source organized as a schema in a database. Accessing the data in this form is easy for any modern software package. One virtue of this approach is that the data might be curated using, say, Python even though the client will analyse it using, say, Stata.28 I chose to use Parquet files for ffiec.pq, in part because I don’t have a PostgreSQL server to put the data into and share with you. But Parquet files offer high performance, are space-efficient, and can be used with any modern data analysis tool."
  },
  {
    "objectID": "published/curate_call_reports.html#good-database-principles",
    "href": "published/curate_call_reports.html#good-database-principles",
    "title": "Data curation: The case of Call Reports",
    "section": "4.2 Good database principles",
    "text": "4.2 Good database principles\nWhile I argued that one does not want to get “particularly fussy about database normalization”, if anything I may have pushed this further than some users might like. However, with ffiec_pivot(), it is relatively easy (and not too costly) to get the data into a “wide” form if that is preferred. The legacy version of Call Reports data offered by WRDS went to the other extreme with a “One Big Table” approach, which meant that this data set never moved to PostgreSQL because of limits there.29"
  },
  {
    "objectID": "published/curate_call_reports.html#primary-keys",
    "href": "published/curate_call_reports.html#primary-keys",
    "title": "Data curation: The case of Call Reports",
    "section": "4.3 Primary keys",
    "text": "4.3 Primary keys\nIn Gow (2026), I suggested that “the Curate team should communicate the primary key of each table to the Understand team. A primary key of a table will be a set of variables that can be used to uniquely identify a row in that table. In general a primary key will have no missing values. Part of data curation will be confirming that a proposed primary key is in fact a valid primary key.”\n\n\n\n\nTable 2: Primary key checks\n\n\n\n\n\n\nSchedule\nPrimary key\nCheck\n\n\n\n\npor\nIDRSSD, date\nTRUE\n\n\nffiec_float\nIDRSSD, date, item\nTRUE\n\n\nffiec_int\nIDRSSD, date, item\nTRUE\n\n\nffiec_str\nIDRSSD, date, item\nTRUE\n\n\nffiec_bool\nIDRSSD, date, item\nTRUE\n\n\nffiec_date\nIDRSSD, date, item\nTRUE\n\n\nffiec_schedules\nitem, date\nTRUE\n\n\n\n\n\n\n\n\nValid primary keys for each schedule are shown in Table 2. To checking these, I used the function ffiec_check_pq_keys(), which checks the validity of a proposed primary key for a schedule. That every column except value forms part of the primary key is what allows us to use ffiec_pivot() to create unique values in the resulting “wide” tables."
  },
  {
    "objectID": "published/curate_call_reports.html#data-types",
    "href": "published/curate_call_reports.html#data-types",
    "title": "Data curation: The case of Call Reports",
    "section": "4.4 Data types",
    "text": "4.4 Data types\nIn Gow (2026), I proposed that “each variable of each table should be of the correct type. For example, dates should be of type DATE, variables that only take integer values should be of INTEGER type. Date-times should generally be given with TIMESTAMP WITH TIME ZONE type. Logical columns should be supplied with type BOOLEAN.”30\nThis element is (to the best of my knowledge) satisfied with one exception. The Parquet format is a bit like the Model T Ford: it supports time zones, and you can use any time zone you want, so long as it is UTC.31 As discussed above, there is only one timestamp in the whole set-up, last_date_time_submission_updated_on on the POR files and I discussed this field above."
  },
  {
    "objectID": "published/curate_call_reports.html#no-manual-steps",
    "href": "published/curate_call_reports.html#no-manual-steps",
    "title": "Data curation: The case of Call Reports",
    "section": "4.5 No manual steps",
    "text": "4.5 No manual steps\nWhen data vendors are providing well-curated data sets, much about the curation process will be obscure to the user. This makes some sense, as the data curation process has elements of trade secrets. But often data will be supplied by vendors in an imperfect state and significant data curation will be performed by the Curate team working for or within the same organization as the Understand team.\nFocusing on the case where the data curation process transforms an existing data set—say, one purchased from an outside vendor—into a curated data set in sense used here, there are a few ground rules regarding manual steps.\n“First, the original data files should not be modified in any way.” Correct. The ffiec.pq package does not modify the FFIEC Bulk Data files after downloading them. I do make some corrections to the item_name variable in the ffiec_items package, but these “manual steps [are] extensively documented and applied in a transparent, automated fashion.” The code for these steps can be found on the GitHub page for the ffiec.pq package."
  },
  {
    "objectID": "published/curate_call_reports.html#documentation",
    "href": "published/curate_call_reports.html#documentation",
    "title": "Data curation: The case of Call Reports",
    "section": "4.6 Documentation",
    "text": "4.6 Documentation\n“The process of curating the data should be documented sufficiently well that someone else could perform the curation steps should the need arise.” I regard that having the ffiec.pq package do all the work of processing the data satisfies this requirement.\nA important idea here is that the code for processing the data is documentation in its own right. Beyond that the document you are reading now is a form of documentation, as is the documentation in the ffiec.pq package."
  },
  {
    "objectID": "published/curate_call_reports.html#update-process",
    "href": "published/curate_call_reports.html#update-process",
    "title": "Data curation: The case of Call Reports",
    "section": "4.7 Update process",
    "text": "4.7 Update process\nIf a new zip file appears on the FFIEC Bulk Data website, you can download it using the process outlined in Section 1.1. Just changing the [:4] to [0] and the script downloads the latest file.\nThen run the following code and the data will be updated:\n\nresults &lt;-\n  ffiec_list_zips() |&gt;\n  filter(date == max(date)) |&gt;\n  select(zipfile) |&gt;\n  pull() |&gt;\n  ffiec_process() |&gt;\n  system_time()\n\n   user  system elapsed \n 12.561   1.644  11.427 \n\nresults |&gt; count(date, ok)\n\n# A tibble: 1 × 3\n  date       ok        n\n  &lt;date&gt;     &lt;lgl&gt; &lt;int&gt;\n1 2025-12-31 TRUE     39"
  },
  {
    "objectID": "published/curate_call_reports.html#data-version-control",
    "href": "published/curate_call_reports.html#data-version-control",
    "title": "Data curation: The case of Call Reports",
    "section": "4.8 Data version control",
    "text": "4.8 Data version control\nWelch (2019) argues that, to ensure that results can be reproduced, “the author should keep a private copy of the full data set with which the results were obtained.” This imposes a significant cost on the Understand team to maintain archives of data sets that may run to several gigabytes or more and it would seem much more efficient for these obligations to reside with the parties with the relevant expertise. Data version control is a knotty problem and one that even some large data providers don’t appear to have solutions for.\nI am delivering the Call Report data not as the data files, but as an R package along with instructions for obtaining the zip files from the FFIEC Bulk Data website. So I cannot be said to be providing much version control of data here. That said, if a user retains the downloaded zip files, the application of the ffiec.pq functions to process these into Parquet files should provide a high degree of reproducibility of the data for an individual researcher.32\nFor my own purposes, I achieve a modest level of data version control by using Dropbox, which offers the ability to restore some previous versions of data files."
  },
  {
    "objectID": "published/curate_call_reports.html#footnotes",
    "href": "published/curate_call_reports.html#footnotes",
    "title": "Data curation: The case of Call Reports",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAs will be seen this, this load step will not generally be an elaborate one. The inclusion of a separate load step serves more to better delineate the distinction between the Curate process and the Understand process.↩︎\nExecute install.packages(c(\"tidyverse\", \"farr\", \"DBI\", \"duckdb\", \"pak\", dbplyr\") within R to install all the packages other than ffiec.pq that you will need to run the R code in this note.↩︎\nI already included pak in the command in the footnote to the previous paragraph.↩︎\nAt the time of writing, there are 99 files, but each quarter will bring a new file to be processed.↩︎\nI discovered this package after writing most of this note. In my case, I pointed-and-clicked to get many of the files and Martien Lubberink of Victoria University of Wellington kindly provided the rest.↩︎\nI recommend that readers follow a similar approach if following along with this note, as it makes subsequent steps easier to implement. A reader can simply specify os.environ['RAW_DATA_DIR'] = \"/Users/igow/Dropbox/raw_data\", substituting a location where the data should go on his or her computer.↩︎\nNote I am using the argument use_multicore = TRUE, which gives me about a five-time improvement in performance, but you will see different results depending on your system. You might test the code on a few files before running the whole thing. The ffiec_process() can accept a list of fully qualified paths to zipfiles, which can be created with the help of ffiec_list_zips().↩︎\nNote that nothing is being “loaded” into RAM, the file is merely being scanned by DuckDB.↩︎\nParquet files are compressed, so they use less space on disk than they do when they are loaded into RAM.↩︎\nNote that collect() here actually brings the data into R; compute() merely materializes the data as a table in the DuckDB database.↩︎\nSee Wickham et al. (2023), p. 311-315, for discussion of time spans.↩︎\nI am writing this from the Boston area, so it would be insufficiently confusing if I did not make this change to my settings. In effect, I want to ensure that the code works for someone in a different time zone.↩︎\nI say somewhat arbitrarily because you probably don’t want to choose a day that is missing an hour due to shift from EST to EDT.↩︎\nTabs and newlines are what are sometimes called invisibles because their presence is not apparent from viewing their usual representation as text (for example, a tab might look the same as a series of spaces). The \\t and \\n representations are quite standard ways of making these characters visible to humans.↩︎\nIn practice, there’s a little clean-up to be done before returning df, as I will explain shortly.↩︎\nUnfortunately, the read_tsv() function does not allow us to specify an alternative to the default for line-terminating characters. It seems that other R and Python packages also do not offer this option.↩︎\nThere are extra backslashes in the R version to specify that the backslashes represent backslashes to be passed along to gsub(), not special characters to be interpreted by R itself.↩︎\nThis is not a completely insoluble problem in that we could inspect the XBRL data to determine the correct form of the data in this case. This is “above my pay grade” in this setting. (I’m not being paid to do this!)↩︎\nNote that it would be somewhat faster to use ffiec_text &lt;- load_parquet(db, \"ffiec_str_20040630\", \"ffiec\"), but this code doesn’t take too long to run.↩︎\nOnly the textual data will have embedded tabs and, because such data is arranged after numerical data, only text data will be affected by embedded tabs.↩︎\nSee here for the gory details. Interestingly, the WRDS Call Report data have the same issue with the earlier case and have incorrect data for one of the banks in the latter case. This seems to confirm that WRDS uses the TSV data itself in creating its Call Report data sets.↩︎\nRecall that the “fix” assumes that embedded tab belongs in last available text column.↩︎\nIf \"NA\" appeared in a numeric field, my code would report an error. As I detected no errors in importing the data, I know there are no such values.↩︎\nThis is the kind of situation where SAS’s approach to coding missing values would be helpful.↩︎\nIn a sense, this would be doing the opposite of what the Python package pandas did in treating np.NaN as the way of expressing what later became pd.NA; I’d be using Inf to distinguish different kinds of missing values.↩︎\nNote that this timestamp sentinel appears in the “MDRM” data from the Federal Reserve that I used to construct ffiec_items, not in the FFIEC data sets themselves.↩︎\nI discuss some of the issues with Excel as a storage format below.↩︎\nOne project I worked on involved Python code analysing text and putting results in a PostgreSQL database and a couple of lines of code were sufficient for a co-author in a different city to load these data into Stata.↩︎\nA rule of thumb might be that, if you cannot store your fairly standard data in PostgreSQL, then perhaps you need to revisit the structure of the data.↩︎\nGow (2026) is referring to PostgreSQL types. The ffiec.pq package uses (logical) Parquet types DATE, INT32, TIMESTAMP(isAdjustedToUTC = true), and BOOLEAN types, respectively for these types. Floating-point numbers are stored as FLOAT64 and strings as STRING.↩︎\nHenry Ford famously said of the Model T that “any customer can have a car painted any color that he wants so long as it is black.” Strictly speaking, the Parquet format supports timezone-aware timestamps, but only as UTC instants, as other time zones are not supported.↩︎\nNote that a researcher might need to use a specific version of the ffiec.pq package to achieve full reproducibility, but the pak package allows for that.↩︎"
  },
  {
    "objectID": "published/crsp-daily-py.html",
    "href": "published/crsp-daily-py.html",
    "title": "Using DuckDB with WRDS: Python version",
    "section": "",
    "text": "In this note, I demonstrate how one can use the Ibis package in Python to get data from a PostgreSQL database and some advantages of using Ibis over a more traditional approach of using SQL to create Pandas data frames and then using Pandas for further data processing. As the working example, I focus on the creation of “daily CRSP data” as found in Tidy Finance with Python.\nI show that using Ibis has a number of advantages over the approach used in Tidy Finance.\n\nData retrieval is faster\nThe code used is simpler and more “Pythonic”\nThe code created with Ibis facilitates shifting from remote PostgreSQL data to a local data source (e.g., parquet files)\nWorking with the final data is a lot easier and more performant\n\nThis note was written using Quarto and compiled with RStudio, an integrated development environment (IDE) for working with R and, to some extent, Python. The source code for this note is available here and the latest version of this PDF is here."
  },
  {
    "objectID": "published/crsp-daily-py.html#general-set-up",
    "href": "published/crsp-daily-py.html#general-set-up",
    "title": "Using DuckDB with WRDS: Python version",
    "section": "2.1 General set-up",
    "text": "2.1 General set-up\nBecause this note uses a number of different approaches to a set of problems, there are many packages I need to load:\n\nimport pandas as pd\nimport numpy as np\nimport duckdb\nimport pandas_datareader as pdr\nimport timeit\nimport os\nimport sqlite3\nimport ibis\nfrom ibis import _\nfrom os.path import join\nfrom sqlalchemy import create_engine\nfrom db2pq import wrds_update_pq\n\nI follow Tidy Finance in focusing on data for the year 1960 to 2023.\n\nstart_date = \"1960-01-01\"\nend_date = \"2023-12-31\""
  },
  {
    "objectID": "published/crsp-daily-py.html#fama-french-data",
    "href": "published/crsp-daily-py.html#fama-french-data",
    "title": "Using DuckDB with WRDS: Python version",
    "section": "2.2 Fama-French data",
    "text": "2.2 Fama-French data\nTo keep this note self-contained, here I include code to get the Fama-French data used below. Note that this code is similar to code included in the Chapter 2 of Tidy Finance with Python.\nIf you are a reader of Tidy Finance with Python and already have the SQLite database created by running the code in Chapter 2 of that book, you can skip ahead to Section 2.3.\n\nfactors_ff3_daily_raw = pdr.DataReader(\n  name=\"F-F_Research_Data_Factors_daily\",\n  data_source=\"famafrench\",\n  start=start_date,\n  end=end_date)[0]\n\nfactors_ff3_daily = (factors_ff3_daily_raw\n  .divide(100)\n  .reset_index(names=\"date\")\n  .rename(str.lower, axis=\"columns\")\n  .rename(columns={\"mkt-rf\": \"mkt_excess\"})\n)\n\n\nfrom pathlib import Path\nimport os\n\ndata_dir = Path(os.environ[\"DATA_DIR\"]).expanduser()\n\ndb_path = data_dir / \"tidy_finance\" / \"tidy_finance_python.sqlite\"\n\ntidy_finance = sqlite3.connect(database=db_path)\n\nfactors_ff3_daily.to_sql(name=\"factors_ff3_daily\",\n                         con=tidy_finance,\n                         if_exists=\"replace\",\n                         index=False)\n\n16108"
  },
  {
    "objectID": "published/crsp-daily-py.html#sec-orig",
    "href": "published/crsp-daily-py.html#sec-orig",
    "title": "Using DuckDB with WRDS: Python version",
    "section": "2.3 Daily CRSP data – Tidy Finance version",
    "text": "2.3 Daily CRSP data – Tidy Finance version\nTidy Finance say “while [earlier data sets] can be downloaded in a meaningful amount of time, this is usually not true for daily return data. The daily CRSP data file is substantially larger than monthly data and can exceed 20 GB.””\nTidy Finance suggest that “this has two important implications: you cannot hold all the daily return data in your memory (hence it is not possible to copy the entire dataset to your local database), and in our experience, the download usually crashes (or never stops) because it is too much data for the WRDS cloud to prepare and send to your Python session.”\nThe solution proposed by Tidy Finance is to “split up the big task into several smaller tasks that are easier to handle.” The following code is more or less copy-pasted from Tidy Finance with Python. The main change I made was to increase the batch size from 500 to 2,000, as doing so has little impact on performance (for me), but reduces the lines of output.\nNote that I omit the code used to set the WRDS_USER environment variable, but you can set that using code provided by Tidy Finance on their website.\n\ntic = timeit.default_timer()\n\nconnection_string = (\n  \"postgresql+psycopg2://\"\n f\"{os.getenv('WRDS_USER')}\"\n  \"@wrds-pgdata.wharton.upenn.edu:9737/wrds\"\n)\n\nwrds = create_engine(connection_string, pool_pre_ping=True)\n\nfactors_ff3_daily = pd.read_sql(\n  sql=\"SELECT * FROM factors_ff3_daily\",\n  con=tidy_finance,\n  parse_dates={\"date\"}\n)\n\npermnos = pd.read_sql(\n  sql=\"SELECT DISTINCT permno FROM crsp.stksecurityinfohist\",\n  con=wrds,\n  dtype={\"permno\": int}\n)\n\npermnos = list(permnos[\"permno\"].astype(str))\n\nbatch_size = 2000\nbatches = np.ceil(len(permnos)/batch_size).astype(int)\n\nfor j in range(1, batches+1):\n\n  permno_batch = permnos[\n    ((j-1)*batch_size):(min(j*batch_size, len(permnos)))\n  ]\n\n  permno_batch_formatted = (\n    \", \".join(f\"'{permno}'\" for permno in permno_batch)\n  )\n  permno_string = f\"({permno_batch_formatted})\"\n\n  crsp_daily_sub_query = (\n    \"SELECT dsf.permno, dlycaldt AS date, dlyret AS ret \"\n      \"FROM crsp.dsf_v2 AS dsf \"\n      \"INNER JOIN crsp.stksecurityinfohist AS ssih \"\n      \"ON dsf.permno = ssih.permno AND \"\n         \"ssih.secinfostartdt &lt;= dsf.dlycaldt AND \"\n         \"dsf.dlycaldt &lt;= ssih.secinfoenddt \"\n      f\"WHERE dsf.permno IN {permno_string} \"\n           f\"AND dlycaldt BETWEEN '{start_date}' AND '{end_date}' \"\n            \"AND ssih.sharetype = 'NS' \"\n            \"AND ssih.securitytype = 'EQTY' \"\n            \"AND ssih.securitysubtype = 'COM' \"\n            \"AND ssih.usincflg = 'Y' \"\n            \"AND ssih.issuertype in ('ACOR', 'CORP') \"\n            \"AND ssih.primaryexch in ('N', 'A', 'Q') \"\n            \"AND ssih.conditionaltype in ('RW', 'NW') \"\n            \"AND ssih.tradingstatusflg = 'A'\"\n  )\n\n  crsp_daily_sub = (pd.read_sql_query(\n      sql=crsp_daily_sub_query,\n      con=wrds,\n      dtype={\"permno\": int}\n    )\n    .dropna()\n   )\n\n  crsp_daily_sub['date'] = pd.to_datetime(crsp_daily_sub.date,\n                                          format='%Y%m%d%H%M%S')\n\n  if not crsp_daily_sub.empty:\n\n      crsp_daily_sub = (crsp_daily_sub\n        .merge(factors_ff3_daily[[\"date\", \"rf\"]],\n               on=\"date\", how=\"left\")\n        .assign(\n          ret_excess = lambda x:\n            ((x[\"ret\"] - x[\"rf\"]).clip(lower=-1))\n        )\n        .get([\"permno\", \"date\", \"ret_excess\"])\n      )\n\n      if j == 1:\n        if_exists_string = \"replace\"\n      else:\n        if_exists_string = \"append\"\n\n      crsp_daily_sub.to_sql(\n        name=\"crsp_daily\",\n        con=tidy_finance,\n        if_exists=if_exists_string,\n        index=False\n      )\n\n  print(f\"Batch {j} out of {batches} done ({(j/batches)*100:.2f}%)\")\n\ntoc = timeit.default_timer()\ntoc - tic\n\nBatch 1 out of 21 done (4.76%)\nBatch 2 out of 21 done (9.52%)\nBatch 3 out of 21 done (14.29%)\nBatch 4 out of 21 done (19.05%)\nBatch 5 out of 21 done (23.81%)\nBatch 6 out of 21 done (28.57%)\nBatch 7 out of 21 done (33.33%)\nBatch 8 out of 21 done (38.10%)\nBatch 9 out of 21 done (42.86%)\nBatch 10 out of 21 done (47.62%)\nBatch 11 out of 21 done (52.38%)\nBatch 12 out of 21 done (57.14%)\nBatch 13 out of 21 done (61.90%)\nBatch 14 out of 21 done (66.67%)\nBatch 15 out of 21 done (71.43%)\nBatch 16 out of 21 done (76.19%)\nBatch 17 out of 21 done (80.95%)\nBatch 18 out of 21 done (85.71%)\nBatch 19 out of 21 done (90.48%)\nBatch 20 out of 21 done (95.24%)\nBatch 21 out of 21 done (100.00%)\n\n\n473.0824100000318\n\n\nSo the code takes about 5 minutes and the result is a table crsp_daily inside an SQLite database."
  },
  {
    "objectID": "published/crsp-daily-py.html#sec-ibis",
    "href": "published/crsp-daily-py.html#sec-ibis",
    "title": "Using DuckDB with WRDS: Python version",
    "section": "2.4 Daily CRSP data – Ibis/DuckDB version",
    "text": "2.4 Daily CRSP data – Ibis/DuckDB version\nI now generate the same data table, but using Ibis to connect to PostgreSQL instead of using the Python package sqlachemy, which in turn uses the psycopg2 package. Note I could use Ibis with a PostgreSQL backend directly, but I instead use a DuckDB backend and (implicitly) use the postgres extension for DuckDB to access PostgreSQL data within DuckDB. I say “implicitly” because the Ibis package is taking care of most of the details for me.\nIf you are working through this code interactively, you might find it helpful to set the following option:\n\nibis.options.interactive = True\n\n\ntic = timeit.default_timer()\n\nI begin by connecting to an in-memory DuckDB database, which is created with the following command:\n\ncon = ibis.duckdb.connect()\n\nRather than loading the factors_ff3_daily data from the SQLite database into a Pandas data frame, I load it into my DuckDB database. The following code is very similar to the above apart from the use of con.read_sqlite() in place of pd.read_sql().\n\nfactors_ff3_daily = con.read_sqlite(db_path, table_name=\"factors_ff3_daily\")\n\nAgain rather than connecting to the WRDS PostgreSQL database directly using sqlalchemy, I connect via my DuckDB database. Note that this requires a slightly different form for the URI (i.e., postgres:// in place of postgresql+psycopg2://) and using con.read_postgres() in place of pd.read_sql().\n\nwrds_uri = (\n  \"postgres://\"\n f\"{os.getenv('WRDS_USER')}\"\n  \"@wrds-pgdata.wharton.upenn.edu:9737/wrds\"\n)\n\n\ndsf = con.read_postgres(wrds_uri,\n                        table_name=\"dsf_v2\", database=\"crsp\")\n                        \nssih = con.read_postgres(wrds_uri,\n                         table_name=\"stksecurityinfohist\", database=\"crsp\")\n\nNow the pièce de résistance: I create crsp_daily using the following code. There are a few things to note about this code. First, creating it involves fairly trivial translation of the SQL above. For example, ssih.issuertype IN ('ACOR', 'CORP') becomes the more Pythonic ssih.issuertype.isin(['ACOR', 'CORP']) and ssih.securitytype = 'EQTY' sees the SQL equality operator replaced by the Pythonic ==. But if you can read the SQL, you should be able to understand this code quite easily.\nNote that the code above used Pandas to do some calculations that could have been performed using SQL. Specifically, the .clip(lower=-1) could be CASE WHEN ret_excess &lt; -1 THEN -1 ELSE ret_excess and the (_.ret_excess &lt; -1).ifelse(-1, _.ret_excess)) is actually implemented by Ibis in this way.1\n\ncrsp_daily = (\n  dsf\n  .inner_join(ssih, \"permno\")\n  .filter(ssih.secinfostartdt &lt;= dsf.dlycaldt,\n          dsf.dlycaldt &gt;= start_date,\n          dsf.dlycaldt &lt;= end_date,\n          ssih.sharetype == 'NS',\n          ssih.securitytype == 'EQTY',\n          ssih.securitysubtype == 'COM',\n          ssih.usincflg == 'Y',\n          ssih.issuertype.isin(['ACOR', 'CORP']),\n          ssih.primaryexch.isin(['N', 'A', 'Q']),\n          ssih.conditionaltype.isin(['RW', 'NW']),\n          ssih.tradingstatusflg == 'A')\n  .rename(date=\"dlycaldt\", ret=\"dlyret\")\n  .left_join(factors_ff3_daily, \"date\")\n  .mutate(ret_excess = _.ret - _.rf)\n  .mutate(ret_excess = (_.ret_excess &lt; -1).ifelse(-1, _.ret_excess))\n  .select(\"permno\", \"date\", \"ret_excess\"))\n\nRather than “downgrading” the data to an SQLite table, I will just export the data to a parquet file.2\n\ncrsp_daily.to_parquet(join(data_dir, \"tidy_finance\", \"crsp_daily.parquet\"))\n\n\ntoc = timeit.default_timer()\nprint(f\"Elapsed time: {(toc - tic):.2f} seconds.\")\n\nElapsed time: 109.46 seconds.\n\n\nSo one benefit is seen immediately: the code runs in about 2 minutes, or about 2.5 times faster than the original Tidy Finance code. A second benefit is that the code is much simpler (and shorter). There is no need to create permnos, run a for-loop, specify batch sizes, or blend SQL with Python variables using f-strings. A final benefit is that this code is more versatile, which we illustrate in a moment.\nWhile we don’t need permnos here, we could create it easily using the following code:\n\npermnos = (ssih\n  .select(\"permno\")\n  .distinct()\n  .to_pandas())\n\npermnos = list(permnos[\"permno\"].astype(str))"
  },
  {
    "objectID": "published/crsp-daily-py.html#getting-original-data-tables",
    "href": "published/crsp-daily-py.html#getting-original-data-tables",
    "title": "Using DuckDB with WRDS: Python version",
    "section": "2.5 Getting original data tables",
    "text": "2.5 Getting original data tables\nRather than creating a special local “daily CRSP” file, we could instead get and retain the original data files. An advantage of this approach is that it means we can write code that creates data tables directly off WRDS data files without requiring minutes to compute.\nI have used two main approaches to maintaining a local data repository:\n\nPostgreSQL database. In 2011, long before WRDS had its own PostgreSQL database, I wrote Perl scripts to export data from SAS files on the WRDS server to tables in a local PostgreSQL database. These Perl scripts eventually became the wrds2pg Python package.3 I wrote more about creating such a database in Appendix D of Empirical Research in Accounting: Tools and Methods.]\nFolders of parquet files. More recently I have used a local repository of parquet files as an alternative to the WRDS PostgreSQL database. While I have more about creating such a repository in Appendix E of Empirical Research in Accounting: Tools and Methods, here I discuss how we can get just the two files we need from WRDS for our current exercise. This requires just the function wrds_update_pq() from my Python package db2pq.4\n\n\nfrom db2pq import wrds_update_pq\n\nThe wrds_update_pq() function keys off the environment variable DATA_DIR and stores WRDS tables as files organized into directories that match the schemas in the WRDS PostgreSQL database. I start by getting crsp.dsf_v2, which is about 29 GB in SAS form and 24 GB in PostgreSQL form on the WRDS servers. As can be seen, getting crsp.dsf_v2 requires a single line of code.5\n\ntic = timeit.default_timer()\nwrds_update_pq(\"dsf_v2\", schema='crsp')\ntoc = timeit.default_timer()\nprint(f\"Elapsed time: {(toc - tic):.2f} seconds.\")\n\ncrsp.dsf_v2 already up to date.\nElapsed time: 0.42 seconds.\n\n\nIt takes me about 22 minutes to get the data.6 The resulting parquet file is 4.35 GB due to the compression and efficient storage offered by the parquet format. This compares not too favourably with the 2.7 GB for an SQLite database containing just these two tables, which is a cut-down version of the data created by Tidy Finance to save on space.\nNote that the wrds_update_pq() checks the “last updated” value for the SAS data file on WRDS and compares that with the corresponding metadata in any parquet file in the existing location and only gets data from the PostgreSQL server if the last updated value has changed. The WRDS subscription at my institution only offers annual updates, to the 20-minute download of crsp.dsf_v2 only needs to happen once a year. Another feature of wrds_update_pq() from the db2pq package is that I wrote it to be very sparing in its use of RAM in creating the parquet file.\nGetting crsp.stksecurityinfohist takes only a few seconds because the underlying data table is much smaller.\n\ntic = timeit.default_timer()\nwrds_update_pq(\"stksecurityinfohist\", schema=\"crsp\")\ntoc = timeit.default_timer()\nprint(f\"Elapsed time: {(toc - tic):.2f} seconds.\")\n\ncrsp.stksecurityinfohist already up to date.\nElapsed time: 0.36 seconds."
  },
  {
    "objectID": "published/crsp-daily-py.html#using-local-data-files",
    "href": "published/crsp-daily-py.html#using-local-data-files",
    "title": "Using DuckDB with WRDS: Python version",
    "section": "2.6 Using local data files",
    "text": "2.6 Using local data files\nSo now that I have local copies of crsp.dsf_v2 and crsp.stksecurityinfohist, I can use those rather than going back to the WRDS PostgreSQL server to get data every time. Much of the code is unchanged from that in Section 2.4.\n\ntic = timeit.default_timer()\ncon = ibis.duckdb.connect()\n\n\nfactors_ff3_daily = con.read_sqlite(db_path, table_name=\"factors_ff3_daily\")\n\nThe only real difference in the code is in the lines creating dsf and ssih. Before I used con.read_postgres() to connect to the WRDS PostgreSQL server. Now I use con.read_parquet() to connect to local copies in parquet form.\n\ndata_dir = os.environ[\"DATA_DIR\"]\n\ndsf = con.read_parquet(join(data_dir, \"crsp\", \"dsf_v2.parquet\"))\nssih = con.read_parquet(join(data_dir, \"crsp\", \"stksecurityinfohist.parquet\"))\n\nThe following code is identical to the code in Section 2.4. But now it takes 3 or 4 seconds to run rather than the 120 or so seconds needed earlier.\n\ncrsp_daily = (\n  dsf\n  .inner_join(ssih, \"permno\")\n  .filter(ssih.secinfostartdt &lt;= dsf.dlycaldt,\n          dsf.dlycaldt &gt;= start_date,\n          dsf.dlycaldt &lt;= end_date,\n          ssih.sharetype == 'NS',\n          ssih.securitytype == 'EQTY',\n          ssih.securitysubtype == 'COM',\n          ssih.usincflg == 'Y',\n          ssih.issuertype.isin(['ACOR', 'CORP']),\n          ssih.primaryexch.isin(['N', 'A', 'Q']),\n          ssih.conditionaltype.isin(['RW', 'NW']),\n          ssih.tradingstatusflg == 'A')\n  .rename(date=\"dlycaldt\", ret=\"dlyret\")\n  .left_join(factors_ff3_daily, \"date\")\n  .mutate(ret_excess = _.ret - _.rf)\n  .mutate(ret_excess = (_.ret_excess &lt; -1).ifelse(-1, _.ret_excess))\n  .select(\"permno\", \"date\", \"ret_excess\"))\n\n\ncrsp_daily.to_parquet(join(data_dir, \"tidy_finance\", \"crsp_daily_alt.parquet\"))\n\ntoc = timeit.default_timer()\nprint(f\"Elapsed time: {(toc - tic):.2f} seconds.\")\n\nElapsed time: 1.76 seconds.\n\n\nWhile the field I know best—empirical accounting research—seems to be somewhere between uninterested and hostile to ideas of reproducibility (perhaps because of the implications for the p-hacking that dominates the field), readers of Tidy Finance are more likely to be in finance, which seems to be embracing reproducibility more enthusiastically. Being able to run code quite quickly (e.g., against local parquet files) that can be then passed along to a data editor who can run the same code against a standard data set (e.g., against the WRDS PostgreSQL server) has a lot to recommend it."
  },
  {
    "objectID": "published/crsp-daily-py.html#some-observations-on-using-python-versus-r",
    "href": "published/crsp-daily-py.html#some-observations-on-using-python-versus-r",
    "title": "Using DuckDB with WRDS: Python version",
    "section": "2.7 Some observations on using Python versus R",
    "text": "2.7 Some observations on using Python versus R\nIn some ways this note covers ground explored in an earlier note that used R. My impression is that some data analysts perceive Python to be somehow cooler and its users to be smarter than is the case for R, and a fortiori for the Tidyverse. But I believe this thinking is misguided.\nIn a lot of ways doing data analysis with the Tidyverse is more facile than doing the same with Python.\n\nThe Tidyverse feels snappier than Ibis. In situations where there is very little latency with remote data frames using dplyr (actually dbplyr behind the scenes), the Ibis equivalents would take a few seconds to generate. While not a huge deal, I feel that building queries up using dplyr is probably a better way of writing SQL for many users and this snappiness adds to the quality of life for a data analyst.\nAt times the Tidyverse code is a little less clunky to write and look at. Non-standard evaluation means I can write select(permno, date, ret_excess) instead of select(\"permno\", \"date\", \"ret_excess\") and mutate(ret_excess = ret - rf) instead of from ibis import _ followed by mutate(ret_excess = _.ret- _.rf).\nBetter workflow for creating documents. Python seems to work best with notebooks. Others have spoken about problems with notebooks (e.g., a YouTube presentation by Joel Grus). I think my issues are related, but are due more to my coming from a different workflow based on Quarto. In the “Quarto workflow” (as I use it), there is a clear distinction between source code and output. This is particularly clear when committing edits using Git.\nI don’t like how output and source code get mixed up in Jupyter notebooks. While one can write scripts to scrub output from notebooks before committing them, at some point it can be useful to have output committed to Git as well. Spliting the source code (.qmd) and output (say, .pdf) into two documents solves this problem.\nSome aspects of my Quarto-oriented workflow did not carry over from R to Python. For example, knitr (the package used in compiling R-based Quarto documents) offers caching at the level of a code chunk, while Jupyter appears to only offer document-level caching. This means I can tweak small portions of the R code without triggering costly recalculations. One solution to this problem in Python would be to switch to more of an interactive notebook-based approach, but this introduces all the issues raised by Joel Grus (e.g., I need to be thinking about what code I’ve run previously in a sesssion).\nBetter learning curve. A data analyst learning Python probaly starts out with Pandas. Moving up to larger data sets probably means learning a new language (e.g., SQL) or at least a new package (e.g., Ibis). In contrast, I think that a data analyst starting with R would learn Tidyverse approaches using local data frames and then “move up” to using dbplyr using remote data frames. That’s exactly the progression I use in Empirical Research in Accounting: Tools and Methods: from the introduction in Chapter 2 through to the introduction to remote data frames in Chapter 6, there is barely a shift in gears.7\nIn terms of languages, learning Pandas might be like learning Italian and then moving onto French when learning Ibis (SQL is German?). In contrast, knowing dplyr is like being a native speaker of American English when it comes to moving to dbplyr (standard British English). A few words are different, but you can understand pretty much everything and quickly start calling the local currency “pounds” not “dollars”.\nR is more purpose-built for data analysis. In both R and Python, there are the age-old issues of dependencies and code that works with one version of a package, but not another. But these issues seem heightened with Python. In writing Empirical Research in Accounting: Tools and Methods over several years, I would occasionally come across code that needed updating to work with the latest version or inconsistencies between packages (e.g., for a time, one package required an older version of another). But these issues seem to have diminished over time as (for example) the Tidyverse has matured and become quite stable. And my impression is that these issues are more acute with Python.\nAlmost any resource on Python will talk about setting up virtual environments and how to deal with multiple versions of Python on your computer. I feel that R is easier to navigate for the more casual user who wants to run an analysis and isn’t looking to put code into a production system or into a time capsule. One can worry about these things, but I think that one has much less need to do so with R. Roughly 99% of the time one can have just use system-level versions of packages that are whatever is the latest on CRAN. For one there’s no worry about messing up the “system version” of R as there is with Python.8\n\nNote that the above is by no means meant to claim that R is uniformly superior to Python, but simply that R has advantages and that these advantages are greater in certain situations. While I have never taken a course in computer science or data analysis, I have experience with both R and Python and find myself reaching for R in many cases and for Python in other cases. Sometimes I might use Python for one part of a problem and R for another part."
  },
  {
    "objectID": "published/crsp-daily-py.html#footnotes",
    "href": "published/crsp-daily-py.html#footnotes",
    "title": "Using DuckDB with WRDS: Python version",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote that “clipping” the excess returns in this way has no justification in financial economics. Excess returns are not bounded by -1 in the way that returns on unleveraged long positions in limited-liability securities are. I implement this here to be consistent with Tidy Finance and also to illustrate how this can be done using Ibis.↩︎\nI have written on the topic of creating a repository of Parquet data files in Appendix E of Empirical Research in Accounting: Tools and Methods.↩︎\nRun pip install wrds2pg to install this package.↩︎\nRun pip install db2pq to install this package.↩︎\nThe lines around this line are there just to record the time needed to do this.↩︎\nThis is on a MacBook Pro with M1 Pro chip and a 1 Gbps internet connection.↩︎\nIn this regard, I strongly disagree with the recommendation of Philipp Hauber, a data scientist at The Economist, to start with the “no-frills tutorial by Norm Matloff” given the orientation of that resource to “base R”. I think Empirical Research in Accounting: Tools and Methods demonstrates that you can go quite far with very little base R–specific knowledge.↩︎\nThough MacOS, for one, makes this kind of problem much harder to cause than it used to be.↩︎"
  },
  {
    "objectID": "published/convert_to_pq.html",
    "href": "published/convert_to_pq.html",
    "title": "Improving performance of SQLite data",
    "section": "",
    "text": "Tidy Finance with R and Tidy Finance with Python provide excellent introductions to doing data analysis for academic finance. Chapters 2–4 of either book provide code to establish an SQLite database that is used as the data source for the rest of the book. Recently I have been dabbling with the Python version of the book and have found the analyses to be surprisingly sluggish on my computer. This note explores some options for improving this performance while still getting the results found in the book.1\nHere I show how one can easily convert an SQLite database to a set of parquet files. A similar approach could be used to convert, say, a schema of a PostgreSQL database to parquet files.2 I may explore such an approach as an alternative to the wrds2pg package, which runs SAS code on the WRDS server to generate data used to construct parquet files.3\nA virtue of the approach used in Tidy Finance is that it is flexible. Readers of Tidy Finance can easily incorporate the approach used here as they work though the core parts of that book. The source code for this note can be found here.\nIn this note, I will load the following R packages. I will also use duckdb and RSQLite packages. use install.packages() to get any package that you don’t already have.\n#| label: setup\n#| include: false\nSys.setenv(RETICULATE_PYTHON = \"../notes_py/bin/python\")\nlibrary(reticulate)\npy_config()\n#| message: false\nlibrary(DBI)\nlibrary(dplyr)\nlibrary(dbplyr)\nlibrary(farr)\nlibrary(lubridate)\nWe will use the following small function to calculate the time taken for steps below.4\nsystem_time &lt;- function(x) {\n  print(system.time(x))\n  x\n}\nI have created an SQLite database comprising data generated from running code in chapter 2 and chapter 3.5\ndb_path &lt;- \"/Users/igow/Dropbox/data/tidy_finance/tidy_finance_python.sqlite\""
  },
  {
    "objectID": "published/convert_to_pq.html#calculating-monthly-betasagain",
    "href": "published/convert_to_pq.html#calculating-monthly-betasagain",
    "title": "Improving performance of SQLite data",
    "section": "4.1 Calculating monthly betas—again",
    "text": "4.1 Calculating monthly betas—again\nLet’s do the beta calculation again. Everything is as it was above except the first two lines, which point crsp_monthly and factors_ff3_monthly to parquet files in place of SQLite tables. Perhaps unsurprisingly, performance is similar—it would be difficult to beat what we saw earlier by much.\ncrsp_monthly &lt;- load_parquet(db, \"crsp_monthly\", data_dir = \"data\")\nfactors_ff3_monthly &lt;- load_parquet(db, \"factors_ff3_monthly\", \n                                    data_dir = \"data\")\nbeta_monthly &lt;-\n  crsp_monthly |&gt;\n  inner_join(factors_ff3_monthly, by = \"month\") |&gt;\n  mutate(beta = sql(paste(\"regr_slope(ret_excess, mkt_excess)\", w)),\n         n_rets = sql(paste(\"regr_count(ret_excess, mkt_excess)\", w))) |&gt;\n  filter(n_rets &gt;= 48) |&gt;\n  select(permno, month, beta) |&gt;\n  collect() |&gt;\n  system_time()\ndbDisconnect(db, shutdown = TRUE)"
  },
  {
    "objectID": "published/convert_to_pq.html#footnotes",
    "href": "published/convert_to_pq.html#footnotes",
    "title": "Improving performance of SQLite data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI ran the code in this note on an M1 Pro MacBook Pro. Running the same code on an i7-3770K running Linux Mint gave similar relative results. Fast code on an old computer handily beats slow code on a new computer.↩︎\nThis would involve using the postgres extension to DuckDB.↩︎\nIssue to be addressed would be supporting the various arguments to the wrds_update() function in wrds2pg, such as col_types and keep, and addressing memory limitations with large tables.↩︎\nUnlike the base R system.time(), this function works with assignment. If we put system.time() at the end of a pipe, then the value returned by system.time() would be stored rather than the result of the pipeline preceding it. Hadley Wickham explained to me that this function works because of lazy evaluation, which is discussed in “Advanced R” here. Essentially, x is evaluated just once—inside system.time()—and its value is returned in the next line.↩︎\nChapter 4 covers data that are not needed in the later chapters I am currently looking at.↩︎\nSubsequent calls to INSTALL appear to have no effect.↩︎\nI don’t have ready access to a Windows computer to check.↩︎\nUsually one would use group_by(), window_order(), and window_range() to generate the requisite PARTITION BY, ORDER BY, and ROWS BETWEEN SQL clauses. However, a gap in the dbplyr package means that it doesn’t recognize regr_slope() and regr_count() functions as aggregates that require them to be put in a “window context”. I will likely file an issue related to this on the dbplyr GitHub page soon.↩︎\nMake sure you don’t use this function with a database that already has a table named df in it, especially if you care about that table.↩︎"
  },
  {
    "objectID": "published/bklyz.html",
    "href": "published/bklyz.html",
    "title": "Should Bao et al. (2020) be retracted?",
    "section": "",
    "text": "Walker (2022) calls “for an investigation at the Journal of Accounting Research (JAR) into academic research misconduct” related to Bao et al. (2020). In this short note, I examine a somewhat different question: Should Bao et al. (2020) be retracted?\nI argue that the current presentation of Bao et al. (2020) (the original paper plus an erratum) is apt to mislead regarding its key findings. As such, some form of retraction seems appropriate, even if only to provide (through a “retract and republish” approach) a research record that is both clear and free from known error.\nI then identify a number of factors that seem relevant to evaluating the merits of providing the authors of Bao et al. (2020) the opportunity to republish if retraction were pursued.1"
  },
  {
    "objectID": "published/bklyz.html#is-there-evidence-of-misconduct",
    "href": "published/bklyz.html#is-there-evidence-of-misconduct",
    "title": "Should Bao et al. (2020) be retracted?",
    "section": "Is there evidence of misconduct?",
    "text": "Is there evidence of misconduct?\nW3 claims to “make the case that there is evidence of academic misconduct and make the recommendation that the Journal of Accounting Research launch a full and independent investigation into the matter.” I cannot imagine that the editors of the Journal of Accounting Research would be keen to get into questions of academic misconduct, given the implications of any such finding. Instead, it seems more relevant to focus on issues pertinent to what is published in JAR.\nWhile I argue that a better presentation of the research record requires some kind of retraction, the COPE guidelines cited above appear to afford some latitude as to whether a journal will “in some instances … wish to work with authors to concurrently retract an article that was found to be fundamentally flawed while simultaneously publishing a linked and corrected version of the work.” In other words, the Journal of Accounting Research arguably enjoys wide discretion over whether to “republish” a corrected version of BKLYZ1.\nThe remainder of this note collects some information that I conjecture the Journal of Accounting Research might consider in reaching its decision on the best course of action in response to the call from W3."
  },
  {
    "objectID": "published/bklyz.html#test-period",
    "href": "published/bklyz.html#test-period",
    "title": "Should Bao et al. (2020) be retracted?",
    "section": "Test period",
    "text": "Test period\nOne thing that BKLYZ1 is very clear on is that the test period is 2003–2008. The main results (“performance increase of 7.9% and 75%”) of that paper are all based on this test period.\nYet the erratum [BKLYZ3, p. 1636] mysteriously seems to emphasize 2003–2005 as the test period: “Using NDCG@k as a performance measure RUSBoost … continues the dominate the performance of the other models for the test period 2003–2005.” The published erratum [p. 1636] even suggests that BLKYZ1 “argued that this test period was the cleanest”. Yet this is simply an impossible reading of BKLYZ1. As a reader, one needs to be confident that the “best test period” is not simply the test period that delivers the most favorable “out-of-sample” performance for RUSBoost.\nAt this point, a careful reader might point out that 2003–2005 was actually the test period used in BKLYZ0. Hopefully, BKLYZ would not be the ones to point this out, because this choice of test period was justified in BKLYZ0 on the following two two bases. First, BKLYZ0 state [p. 4] that “the SEC’s Accounting and Auditing Enforcement Releases (AAERs) available to us end in September 2010”. Second, “there is an average of five-year gap between a fraud occurrence and the AAER publication date.”\nAs the sample period in BKLYZ1 includes AAERs that extend to 2014, the first item seems to suggest that (on the logic of BKLYZ0 itself) with the test period could be updated to from 2003–2005 to 2003–2009 (i.e., adding four years).\nBut it is important to note that BKLYZ0 included in their training sample frauds that were also in the test periods, even though the AAER publication dates would have been after the test periods in question.8\nIf the second basis were maintained, but the issues of “serial fraud” addressed, then by the logic of BKLYZ1, the “gap” of two years used in BKLYZ1 would have to be five years, and thus the feasible test sample could not begin until 2006. BKLYZ1 [p. 209] “require a gap of 24 months between the financial results announcement of the last training year and the results announcement of a test year … because Dyck, Morse, and Zingales (2010) find that it takes approximately 24 months, on average, for the initial disclosure of the fraud.”9"
  },
  {
    "objectID": "published/bklyz.html#the-coding-error",
    "href": "published/bklyz.html#the-coding-error",
    "title": "Should Bao et al. (2020) be retracted?",
    "section": "The “coding error”",
    "text": "The “coding error”\nBKLYZ3 states [p. 1635] that [W1 and W2] “identified an error in the program codes [sic] of BKLYZ1 posted on Github that led to an overstatement of model performance metrics. This erratum corrects this error ….” It is difficult to disagree with W3’s claim that this statement is false. There is nothing in the code posted on GitHub that created this error, instead the “error” was in data used by that code.\nW3 claims that “to this date, the authors have offered no explanation as to why they did what they did” in recoding certain frauds to have different identifiers. This seems correct. In BKLYZ2, BKLYZ provide what may be best described as a non-explanation for what they did.\nThe example in Figure 1 of BKLYZ2 illustrates the “coding error” made in BKLYZ1 if there are missing items for the affected firm in 2004. If such missing items exist, firm-years in 2001 through 2003 would be given a different fraud ID from the 2005 firm-year (in such cases BKLYZ appended the character “1” to the fraud ID for 2001 through 2003, and the character “2” to the fraud ID for 2005). Because the fraud IDs for 2001–2003 no longer appear in the test year (2005), they are not recoded as zero. In BKLYZ2, the practice of not recoding these frauds in this way is described as “Walker’s approach” even though (as W2 points out), this is not so much Walker’s approach as the approach described in BKLYZ1.\nEvidenced of this approach to recoding frauds is not legitimate is provided by the fact that doing it that way has been characterized as a “coding error” in BKLYZ3.\nBut relabelling “serial frauds” also does not make sense in that it is recoding a fraud as 1 in 2003 so that a “prediction” of the same underlying fraud can be made in 2005 even though, by the terms of the example itself (Figure 1 of BKLYZ), the SEC does not release an AAER until 2007.\nThat training models using data on frauds that are not released until after the test period is problematic seems obvious. And it seems clear from BKLYZ1’s discussion of “serial fraud” that the authors were well aware of these issues. In fact, they seem aware of the underlying issue even in BKLYZ0. As discussed above, the BKLYZ0 “sample ends in 2005 because … a significant portion of the accounting frauds that occurred over 2006–2010 are likely still unreported by the AAERs as of the end of 2010.”\nOne response the authors might have is that there might be a “key event [that] reveals 2001–2003 fraud labels” before 2004 (and before the AAER publication date in 2007). But this implies a completely different prediction problem from that studied in BKLYZ1 (or BKLYZ0), which identifies frauds as AAER events. The only reliable “key event” that identifies an AAER is the release of an AAER. As discussed above, disclosed accounting fraud might not result in AAERs for a number of reasons.\nSaying that sometimes information is released that suggests a high likelihood of a future AAER event transforms the prediction problem from one about predicting confirmed AAERs using financial statement features into one about predicting confirmed AAERs using financial statement features and also some unidentified information about possible future AAERs.\nEven if we expand the information set to include the “maybe-future-AAERs” as is done in the “not Walker’s approach” depicted in Figure 1 of BKLYZ2, there is no rationale provided as to why missing values for some items on Compustat in 2004 should be assumed to precipitate a fraud revelation event before 2004. In fact, it seems hard to conceive of one.\nAnd this “coding error” is surely not something that happened by mistake. The strident defence of their approach provided in BKLYZ2 suggests that the authors did this consciously, and only in BKLYZ3 did they suggest it was a “coding error”. A reproduction of the “coding error” is included in an appendix to this note and it seems difficult to see how this “coding error” could have been made inadvertently.\nAt the very least, I think the authors should be required to provide more information of how the “coding error” was implemented so that the editors can assess the likelihood that it was indeed a “coding error” (as it is framed in BKLYZ3) and not a deliberate research design choice (as it is framed in BKLYZ2). If it seems that it was a deliberate research design choice, then I think the authors need to explain the rationale for it and also the rationale for dissembling its existence in the code posted to GitHub upon publication of BKLYZ1.\nOne possible response by the BKLYZ team might be that they have more than complied with the JAR data policy in effect when they submitted their paper and therefore do not need to account for the “coding error” beyond the code they have already provided.\nI do not think this kind of response would be helpful in this case. There appears to be no data policy at The Accounting Review (see discussion here), but this did not prevent the retraction of Bird and Karolyi (2017), where the journal stated “the authors were unable to provide the original data and code requested by the publisher” to support an assertion made in the paper and therefore retracted the paper.\nGiven the “coding error” of BKLYZ3 appears to have been a conscious decision (see BKLYZ2), I think that the onus is on the authors of BKLYZ1 to show that the “coding error” was made in good faith."
  },
  {
    "objectID": "published/bklyz.html#meta-parameters",
    "href": "published/bklyz.html#meta-parameters",
    "title": "Should Bao et al. (2020) be retracted?",
    "section": "Meta-parameters",
    "text": "Meta-parameters\nIn BKLYZ2, the authors suggested that W1 was flawed because he “did not recalibrate the most important parameter of RUSBoost, number of trees, after changing the fraud training samples using his approach [i.e., the approach described in BKLYZ1].” This is somewhat understandable, as BKLYZ1 does not describe the process of calibrating these meta-parameters in the first place. Indeed, there are several meta-parameters used in BKLYZ1: number of trees, MinLeafSize, LearnRate, and RatioToSmallest. It is critical that such meta-parameters be fixed using the training and validation data prior to evaluating model performance against test data, lest these parameters be selected based on test performance, thus overstating the predictive value of the model.\nIn this regard, it is somewhat concerning that the number of trees parameter that is selected for the BKLYZ1 specification is 1,000 according to W3, not the 3,000 used in BKLYZ1.10 This creates the unfortunate impression that the idea of selecting meta-parameters in a transparent fashion using only validation data emerged only after observing a decline in test performance upon switching to “Walker’s approach” in preparing the erratum."
  },
  {
    "objectID": "published/bklyz.html#other-data-issues",
    "href": "published/bklyz.html#other-data-issues",
    "title": "Should Bao et al. (2020) be retracted?",
    "section": "Other data issues",
    "text": "Other data issues\nWhile it is possible to explain changes in the code between BKLYZ1 and BKLYZ3 using GitHub, some concerns remain. For example, as detailed here, the two data files provided are not consistent. While the PDF-rendered SAS code suggests that one data file depends on the other, there are observations on AAERs not found in the former."
  },
  {
    "objectID": "published/bklyz.html#how-unusual-are-the-issues-in-bklyz1",
    "href": "published/bklyz.html#how-unusual-are-the-issues-in-bklyz1",
    "title": "Should Bao et al. (2020) be retracted?",
    "section": "How unusual are the issues in BKLYZ1?",
    "text": "How unusual are the issues in BKLYZ1?\nSome readers may be surprised to learn that the core results of a published paper can disappear when a “coding error” is detected and corrected. I am not surprised. Having replicated many papers, I conjecture that the kinds of issues observed with BKLYZ1 are commonplace. Papers that say one thing, but do another (most papers using “regression discontinuity designs” fit here). Papers with genuine coding errors. Papers with results that are very sensitive to “design choices” that are difficult to rationalize (see here and here). If such issues abound, then the merit of singling out BKLYZ1 seems to be low.\nAnother factor that seems relevant is JAR’s data policy. While not perfect, arguably JAR’s policy was critical in helping to unearth the issues not only in BKLYZ1, but in the replications I refer to above.11 If BKLYZ1 had been accompanied by a very perfunctory effort to comply with the data policy (e.g., “here is the list of CIKs for the fraud firms in our sample”), then it would not have been possible to detect the issue raised by W1 and corrected in BKLYZ3."
  },
  {
    "objectID": "published/bklyz.html#the-source-data-sets",
    "href": "published/bklyz.html#the-source-data-sets",
    "title": "Should Bao et al. (2020) be retracted?",
    "section": "The source data sets",
    "text": "The source data sets\nPDF-rendered SAS course code supplied by BKLYZ suggests that the final data set used in Bao et al. (2020) was constructed by merging data on AAERs (aaer_firm_year) with data on raw Compustat variables (compustatindustrial7815). To reproduce the “coding error”, we need to retrace the process of merging these two tables.\nWhile BKLYZ provide a file AAER_firm_year.csv, it is easy to show that this was not the data file used to create the Bao et al. (2020) data set.13\nAs such, we use the “final” data set used by Bao et al. (2020) and reconstruct the relevant portions of the source data sets from that.\n\njar_data &lt;-\n    read_csv(paste0(\"https://raw.githubusercontent.com/JarFraud/\",\n                    \"FraudDetection/master/\",\n                    \"data_FraudDetection_JAR2020.csv\"),\n             col_types = \"d\") |&gt;\n    mutate(gvkey = str_pad(gvkey, 6, side = \"left\", pad = \"0\"),\n           fyear = as.integer(fyear),\n           p_aaer = as.character(p_aaer))\n\nWe first construct the original aaer_firm_year data set by filling in any gaps in firm-years for an AAER found in jar_data.\n\naaer_firm_year &lt;-\n    jar_data |&gt;\n    filter(!is.na(p_aaer)) |&gt;\n    group_by(p_aaer, gvkey) |&gt;\n    summarize(min_year = min(fyear), max_year = max(fyear), \n              .groups = 'drop') |&gt;\n    rowwise() |&gt;\n    mutate(fyear = list(seq(min_year, max_year, by = 1))) |&gt;\n    unnest(fyear) |&gt;\n    select(gvkey, fyear, p_aaer) \n\nAs we can see here, aaer_firm_year is the panel data set of firm-years affected by AAERs.\n\nhead(aaer_firm_year)\n\n# A tibble: 6 × 3\n  gvkey  fyear p_aaer\n  &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt; \n1 021110  1992 1033  \n2 008496  1992 1037  \n3 008496  1993 1037  \n4 028140  1993 1044  \n5 012455  1994 1047  \n6 025927  1993 1053"
  },
  {
    "objectID": "published/bklyz.html#firm-years-with-compustat-features",
    "href": "published/bklyz.html#firm-years-with-compustat-features",
    "title": "Should Bao et al. (2020) be retracted?",
    "section": "Firm-years with Compustat features",
    "text": "Firm-years with Compustat features\nWe next construct the data set comp_firm_years, which represents the firm-years in the Bao et al. (2020) sample. These will be missing some of the firm-years in aaer_firm_year because of missing items on Compustat.\n\ncomp_firm_years &lt;-\n  jar_data |&gt;\n  mutate(missing = 0) |&gt;\n  select(fyear, gvkey, missing)"
  },
  {
    "objectID": "published/bklyz.html#the-coding-error-1",
    "href": "published/bklyz.html#the-coding-error-1",
    "title": "Should Bao et al. (2020) be retracted?",
    "section": "The “coding error”",
    "text": "The “coding error”\nNow that we have the two data sets aaer_firm_year and comp_firm_years, we can reproduce the “coding error” from Bao et al. (2020).\nThe first step is to merge aaer_firm_year with comp_firm_years using the left_join function so that all observations on aaer_firm_year are retained even if there is no match on comp_firm_years. The cases where there is no match on comp_firm_years are indicated by the variable missing.\n\naaer_merged &lt;-\n  aaer_firm_year |&gt;\n  left_join(comp_firm_years, by = c(\"gvkey\", \"fyear\")) |&gt;\n  mutate(missing = coalesce(missing, 1)) |&gt;\n  select(gvkey, fyear, p_aaer, missing)\n\nhead(aaer_merged)\n\n# A tibble: 6 × 4\n  gvkey  fyear p_aaer missing\n  &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;\n1 021110  1992 1033         0\n2 008496  1992 1037         0\n3 008496  1993 1037         0\n4 028140  1993 1044         0\n5 012455  1994 1047         0\n6 025927  1993 1053         0\n\n\nNow we can recode AAERs following Bao et al. (2020). We do this by calculating sum_gap, a running sum of the indicator variable for one plus the number of gaps in the sample for a given AAER.14 This will start at 1 and increase to 2 after a “gap”. We then create the variable new_p_aaer by combining the original AAER identifier (p_aaer) with sum_gap.\n\naaer_sum_gap &lt;-\n  aaer_merged |&gt;\n  group_by(gvkey, p_aaer) |&gt;\n  arrange(fyear) |&gt;\n  mutate(lag_missing = coalesce(lag(missing), 0),\n         sum_gap = cumsum(lag_missing & !missing) + 1,\n         new_p_aaer = paste0(p_aaer, \n                             as.character(sum_gap))) |&gt;\n  select(gvkey, fyear, p_aaer, new_p_aaer) |&gt;\n  ungroup()\n\nIf successful, my code reproduces the “coding error” in Bao et al. (2020) in about 14 lines of code.15"
  },
  {
    "objectID": "published/bklyz.html#verifying-the-reproduction-of-the-coding-error",
    "href": "published/bklyz.html#verifying-the-reproduction-of-the-coding-error",
    "title": "Should Bao et al. (2020) be retracted?",
    "section": "Verifying the reproduction of the “coding error”",
    "text": "Verifying the reproduction of the “coding error”\nTo check that we have successfully reproduced the “coding error” corrected by Bao et al. (2022), we can reproduce Figure 2 of Walker (2021a), which we do with the following code.\n\nwalker_fig_2 &lt;-\n    aaer_sum_gap |&gt; \n    group_by(p_aaer) |&gt; \n    filter(n_distinct(new_p_aaer) &gt; 1) |&gt;\n    inner_join(comp_firm_years, by = c(\"gvkey\", \"fyear\")) |&gt;\n    pivot_wider(names_from = \"fyear\", id_cols = \"p_aaer\", \n              values_from = \"new_p_aaer\") |&gt;\n    ungroup() |&gt;\n    arrange(as.integer(p_aaer)) |&gt;\n    mutate(across(`1991`:`2014`, ~ coalesce(., \"\"))) \n\nFor reasons of space, we only include a portion of the table.\n\nwalker_fig_2 |&gt;\n    select(p_aaer, `1995`:`2004`) |&gt;\n    knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np_aaer\n1995\n1996\n1997\n1998\n1999\n2000\n2001\n2002\n2003\n2004\n\n\n\n\n857\n\n\n\n\n\n\n\n\n\n\n\n\n1542\n\n\n15421\n15421\n\n15422\n\n\n\n\n\n\n1839\n\n\n\n18391\n18391\n18391\n\n18392\n\n\n\n\n2472\n\n\n\n24721\n24721\n24721\n24721\n\n24722\n24722\n\n\n2504\n\n\n\n\n\n25041\n25041\n\n25042\n25042\n\n\n2591\n\n\n\n\n\n25911\n25911\n\n25912\n25912\n\n\n2754\n27541\n27541\n\n27542\n27542\n27542\n27542\n27542\n27542\n27542\n\n\n2894\n\n\n\n\n\n28941\n\n28942\n28942\n\n\n\n2937\n\n\n\n29371\n\n\n\n\n\n29372\n\n\n2949\n\n\n\n\n29491\n\n\n\n\n29492\n\n\n3022\n\n\n\n\n30221\n\n\n30222\n\n\n\n\n3045\n\n\n30451\n30451\n30451\n\n30452\n\n\n\n\n\n3156\n\n\n\n\n\n\n\n\n31561\n\n\n\n3217\n\n32171\n32171\n\n\n32172\n32172\n32172\n32172\n32172\n\n\n3909\n\n\n\n\n\n\n\n\n\n\n\n\n3996\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCareful comparison of my data with Figure 2 of Walker (2021a) suggests that I have almost perfectly reproduced the “coding error” of Bao et al. (2020). The one exception is that my table omits the AAER with p_aaer of 2957.\nExamining the underlying data, it seems that the issue here is the presence of multiple AAERs for the related firm for 2000 and 2001.\n\naaer_firm_year |&gt; \n    filter(gvkey == \"064630\") |&gt; \n    arrange(fyear)\n\n# A tibble: 8 × 3\n  gvkey  fyear p_aaer\n  &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt; \n1 064630  1997 2957  \n2 064630  1998 2957  \n3 064630  1999 2957  \n4 064630  2000 2259  \n5 064630  2000 2957  \n6 064630  2001 2259  \n7 064630  2001 2957  \n8 064630  2002 2957  \n\n\nReturning to the SAS code supplied with Bao et al. (2020), we see the following lines:\nPROC SORT DATA=temp nodupkey;\nBY gvkey fyear; RUN;\nThis code would have the effect of (essentially randomly) deleting data on one AAER for any firm-year where two AAERs apply. It is unclear whether the BKLYZ team would characterize this second basis for recoding AAERs as a research design choice (as the coding error was arguably characterized in Bao et al. 2021) or as an error like the “coding error” replicated above.\nThe use of PROC SORT nodupkey is common in accounting reserarch, but in general this is a problematic practice.16"
  },
  {
    "objectID": "published/bklyz.html#footnotes",
    "href": "published/bklyz.html#footnotes",
    "title": "Should Bao et al. (2020) be retracted?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe views expressed here are my own. I have no formal role in this process or association with any of the papers published in the Journal of Accounting Research or Econ Journal Watch. This note arises from observations I have made in preparing a chapter on prediction for a course book on accounting research. While I have sought feedback from a small number of people on the tone and content of this note, any errors herein are my own.↩︎\nSee the SEC website for details.↩︎\nIn some ways, the requirement for Matlab is unfortunate, as Matlab is proprietary software and offers less transparency than an implementation using one of the open-source alternatives, such as Python or R, would. Unfortunately there is no implementation of RUSBoost in the popular Python library, scikit-learn, though this library does have an implementation of AdaBoost. There is also no well-documented implementation of RUSBoost in R, though there are several implementations of the AdaBoost in R. Fortunately, it is possible to implement RUSBoost in R, and I include an implementation in the R package farr that I created as a complement to the course book found here. A chapter using RUSBoost will be forthcoming in the near future.↩︎\nThe one AAER in the Bao et al. (2020) sample connected to Enron actually covers the order for Citigroup to pay an amount in a settlement arising because “Citigroup assisted [Enron and Dynegy] in enhancing artificially their financial presentations through a series of complex structured transactions … to allow those companies to report proceeds of financings as cash from operating activities”.↩︎\nThe general result here being that statistical learning methods such as ensemble learning usually improve out-of-sample prediction performance relative to models—such as logistic regression—that tend to overfit the data used to train them.↩︎\nAAER dates are easily obtained from the SEC website, and included with the farr R package (see here). My own analysis suggests that AAERs are never released prior to the last affected period, so AAERs that affect test years are always in the “future” relative to that test year, and should never be coded as anything other than zero in the training sample.↩︎\nSee p.2 of Retraction Guidelines.↩︎\nThat this is the case is implicit in footnote 10 to BKLYZ1 and the discussion under “serial fraud” in BKLYZ1, which is not found in BKLYZ0.↩︎\nWhile this shorter period is definitely convenient for BKLYZ1, it seems less convenient that there is no evidence of the claim in Dyck, Morse, and Zingales (2010) itself. Perhaps the BKLYZ1 authors obtained underlying data from the authors of Dyck, Morse, and Zingales (2010).↩︎\nIn my own analysis, I found that 900 trees maximized performance in the validation sample, which is very close to the value found in W3.↩︎\nWe should not infer that the situation is better at journals without a data-sharing policy. If anything, we might expect it to be worse.↩︎\nInstall this using the command install.packages(\"tidyverse\") in R, if necessary.↩︎\nSee here for details. In short, the final data set used in Bao et al. (2020) contains AAERs not found in AAER_firm_year.csv.↩︎\nA “gap” is indicated by lag_missing being 1 and missing being zero.↩︎\nThe exact number depends on how one counts a line.↩︎\nIssues associated with this practice are explored in discussion questions here.↩︎"
  },
  {
    "objectID": "published/aus_ipos.html",
    "href": "published/aus_ipos.html",
    "title": "Analysis of IPOs on the ASX",
    "section": "",
    "text": "The purpose of this note is to compile some statistics about IPOs on the Australia Stock Exchange (ASX). The source code for this note can be found here.\n\n\n\n\n\n\n\n\n\nFigure 1: Distribution of IPO Day-1 returns (% over issue price)\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: IPOs by month (May 2017–April 2025)\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Average cumulative return on issue price with 95% confidence intervals"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Notes",
    "section": "",
    "text": "This site publishes a curated set of notes. Use the category filters to find notes by topic.\n\n\n\n\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\n\nDate\n\n\n\nCategories\n\n\n\n\n\n\n\n\nData management ideas for researchers\n\n\nFeb 25, 2026\n\n\nPython, Parquet, db2pq, WRDS\n\n\n\n\n\n\nData management ideas for researchers (R version)\n\n\nFeb 25, 2026\n\n\nR, Parquet, db2pq, WRDS\n\n\n\n\n\n\nData curation: The case of Call Reports\n\n\nFeb 18, 2026\n\n\nData curation, Polars, DuckDB\n\n\n\n\n\n\nData collection (with spreadsheets)\n\n\nFeb 5, 2026\n\n\nData curation, Spreadsheets\n\n\n\n\n\n\nData curation and the data science workflow\n\n\nJan 29, 2026\n\n\nData curation, Australia, ASX, SIRCA\n\n\n\n\n\n\nSome benchmarks with comp.g_secd\n\n\nJan 21, 2026\n\n\nSAS, WRDS, CRSP, Parquet, Python\n\n\n\n\n\n\nThe best of both worlds: Using modern data frame libraries to create pandas data\n\n\nJan 20, 2026\n\n\nWRDS, Polars, Ibis, pandas\n\n\n\n\n\n\nUsing SAS to create pandas data\n\n\nJan 20, 2026\n\n\nSAS, pandas, wrds2pg\n\n\n\n\n\n\nShared code\n\n\nJan 15, 2026\n\n\nresearch, web data\n\n\n\n\n\n\nReproducible data collection\n\n\nJan 5, 2026\n\n\nReproducibility, Research methods\n\n\n\n\n\n\nWriting better SQL without writing SQL\n\n\nDec 17, 2025\n\n\nSQL, dbplyr\n\n\n\n\n\n\nResponsive open-source software: Two examples from dbplyr\n\n\nDec 17, 2025\n\n\nData curation, dbplyr, SQL, DuckDB\n\n\n\n\n\n\nResponsive open-source software: Two examples from dbplyr\n\n\nDec 17, 2025\n\n\ndbplyr, SQL\n\n\n\n\n\n\nAnalysis of IPOs on the ASX\n\n\nSep 12, 2025\n\n\nAustralia, IPOs\n\n\n\n\n\n\nSIRCA ASX End of Day (EOD) collection\n\n\nSep 11, 2025\n\n\nAustralia, SIRCA, ASX, CSV, Parquet, dbplyr\n\n\n\n\n\n\nSIRCA Mergers and Acquisitions collection\n\n\nSep 11, 2025\n\n\nAustralia, M&A, ASX, SIRCA, Parquet\n\n\n\n\n\n\nDefining winter and summer in Oxford\n\n\nMar 10, 2025\n\n\nWeather, Oxford, Python\n\n\n\n\n\n\nStock returns on Yahoo Finance\n\n\nFeb 26, 2025\n\n\nYahoo, finance\n\n\n\n\n\n\nRetail sales\n\n\nFeb 14, 2025\n\n\n \n\n\n\n\n\n\nUsing DuckDB with WRDS: Python version\n\n\nJan 23, 2025\n\n\nWRDS, DuckDB, Python\n\n\n\n\n\n\nGetting SEC EDGAR XBRL data\n\n\nDec 2, 2024\n\n\n \n\n\n\n\n\n\nACNC Registry data: Arrow version\n\n\nOct 1, 2024\n\n\nAustralia, Arrow\n\n\n\n\n\n\nDoes @Beardsley_2021 show anything?\n\n\nOct 1, 2024\n\n\nResearch methods\n\n\n\n\n\n\nConsumer Price Index\n\n\nSep 25, 2024\n\n\nAustralia, Inflation\n\n\n\n\n\n\nA quick look at City of Melbourne bike data\n\n\nSep 19, 2024\n\n\n \n\n\n\n\n\n\nWorking with date and times\n\n\nAug 13, 2024\n\n\nDatetimes, DuckDB\n\n\n\n\n\n\nDefining winter and summer in Boston\n\n\nApr 20, 2024\n\n\nWeather, Boston\n\n\n\n\n\n\nSunrise and sunset times\n\n\nApr 20, 2024\n\n\nDatetimes, Weather, Australia, Melbourne, Boston\n\n\n\n\n\n\nDefining winter and summer in Sydney\n\n\nApr 20, 2024\n\n\nWeather, Australia, Sydney\n\n\n\n\n\n\nDefining winter and summer in Melbourne\n\n\nApr 20, 2024\n\n\nWeather, Australia, Melbourne\n\n\n\n\n\n\nTrading days per year (crsp.dsf)\n\n\nApr 10, 2024\n\n\nCRSP, WRDS\n\n\n\n\n\n\nData visualization challenge\n\n\nApr 5, 2024\n\n\nData visualization, ggplot2\n\n\n\n\n\n\nImproving performance of SQLite data\n\n\nDec 29, 2023\n\n\nTidy Finance, SQLite\n\n\n\n\n\n\nCalculating betas using DuckDB\n\n\nDec 23, 2023\n\n\nTidy Finance, DuckDB, WRDS, Finance\n\n\n\n\n\n\nThe Gino-Colada Affair\n\n\nOct 1, 2023\n\n\nReproducibility, Research methods\n\n\n\n\n\n\nThe elephant in the room: p-hacking and accounting research\n\n\nAug 8, 2023\n\n\nResearch methods, p-hacking\n\n\n\n\n\n\nAdding delisting returns to monthly data\n\n\nApr 7, 2023\n\n\nSAS, Stock returns, CRSP\n\n\n\n\n\n\nShould Bao et al. (2020) be retracted?\n\n\nOct 13, 2022\n\n\nResearch methods, Machine learning\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "published/acnc_registry_arrow.html",
    "href": "published/acnc_registry_arrow.html",
    "title": "ACNC Registry data: Arrow version",
    "section": "",
    "text": "This code shows how one can use list columns (e.g., in a parquet file) to provide a single-file (or single-table) representation of data that might naturally be stored as multiple tables in a more traditional relational database. The code to produce the parquet file used in the following analysis is provided here.\nIn the original registry data supplied by the ACNC, the data I have stored in list columns were spread over multiple columns. For example, “Operating locations (columns R-Z)” included columns such as “Operates in ACT” and “Operates in VIC” with values equal to either Y or blank. I converted these columns to a single column, states, with values such as VIC or VIC, NSW. While these look like simply comma-separated text values when viewing the data in software such as Tad, they are actually list columns.\nOther list columns include operating_countries (originally a single column, but as comma-separated text, not a list column), subtypes (originally “Subtypes (columns AA-AN)”), and beneficiaries (originally “Beneficiaries (columns AO-BN)”). Below I provide examples of working with such columns.\nIn writing this note, I use the packages listed below.1 This note was written using Quarto and compiled with RStudio, an integrated development environment (IDE) for working with R. The source code for this note is available here and the latest version of this PDF is here.\nlibrary(tidyverse)\nlibrary(tinytable)\nlibrary(arrow)\nlibrary(farr)\nWe start by downloading the data, which takes a few seconds.\nregistry &lt;-\n  read_parquet('https://go.unimelb.edu.au/5d78') |&gt;\n  collect() |&gt;\n  system_time()\n\n   user  system elapsed \n  0.427   0.059   3.897\nWe can construct the beneficiaries data frame by using unnest() with the list column beneficiaries.\nbeneficiaries &lt;-\n  registry |&gt;\n  select(abn, beneficiaries) |&gt;\n  unnest(beneficiaries) |&gt;\n  rename(beneficiary = beneficiaries)\nCharities vary in terms of the groups they serve, or beneficiaries. The results of the following code are shown in Table 1.\nregistry |&gt;\n  unnest(beneficiaries) |&gt;\n  count(beneficiaries, sort = TRUE) |&gt;\n  tt() |&gt;\n  style_tt(align = \"ld\") |&gt;\n  format_tt(escape = TRUE) \n\n\n\nTable 1: Number of charities serving each beneficiary type\n\n\n\n\n\n    \n\n    \n\n    \n    \n    \n      \n        \n        \n              \n                beneficiaries\n                n\n              \n        \n        \n        \n                \n                  Youth\n                  24633\n                \n                \n                  Adults\n                  23992\n                \n                \n                  Families\n                  23188\n                \n                \n                  General Community in Australia\n                  22638\n                \n                \n                  Children\n                  22130\n                \n                \n                  Aged Persons\n                  21763\n                \n                \n                  Females\n                  19027\n                \n                \n                  Males\n                  18012\n                \n                \n                  Financially Disadvantaged\n                  15826\n                \n                \n                  Early Childhood\n                  15184\n                \n                \n                  Rural Regional Remote Communities\n                  14757\n                \n                \n                  Ethnic Groups\n                  14384\n                \n                \n                  Aboriginal or TSI\n                  13528\n                \n                \n                  People with Disabilities\n                  13396\n                \n                \n                  People at risk of homelessness\n                  9493\n                \n                \n                  Unemployed Person\n                  9327\n                \n                \n                  People with Chronic Illness\n                  8082\n                \n                \n                  Other Charities\n                  6513\n                \n                \n                  Veterans or their families\n                  5656\n                \n                \n                  Victims of crime\n                  5220\n                \n                \n                  Victims of Disasters\n                  4856\n                \n                \n                  Communities Overseas\n                  4710\n                \n                \n                  Migrants Refugees or Asylum Seekers\n                  3735\n                \n                \n                  Pre Post Release Offenders\n                  3612\n                \n                \n                  Gay Lesbian Bisexual\n                  2890\nMany charities serve multiple beneficiary types. The most common pairs of beneficiary types are given in Table 2, which is produced using the following code.\nbeneficiaries |&gt;\n  inner_join(beneficiaries, by = \"abn\",\n             relationship = \"many-to-many\") |&gt;\n  filter(beneficiary.x &lt; beneficiary.y) |&gt;\n  count(beneficiary.x, beneficiary.y) |&gt;\n  arrange(desc(n)) |&gt;\n  head(n = 10) |&gt;\n  tt() |&gt;\n  style_tt(align = \"lld\") |&gt;\n  format_tt(escape = TRUE)\n\n\n\nTable 2: Most common beneficiary pairs\n\n\n\n\n\n    \n\n    \n\n    \n    \n    \n      \n        \n        \n              \n                beneficiary.x\n                beneficiary.y\n                n\n              \n        \n        \n        \n                \n                  Adults\n                  Aged Persons\n                  19277\n                \n                \n                  Adults\n                  Youth\n                  18429\n                \n                \n                  Children\n                  Youth\n                  17384\n                \n                \n                  Females\n                  Males\n                  17182\n                \n                \n                  Adults\n                  Families\n                  16247\n                \n                \n                  Aged Persons\n                  Youth\n                  15787\n                \n                \n                  Families\n                  Youth\n                  15381\n                \n                \n                  Aged Persons\n                  Families\n                  15048\n                \n                \n                  Adults\n                  Females\n                  14088\n                \n                \n                  Children\n                  Families\n                  13987\nThe results of the following code are shown in Table 3.\nregistry |&gt;\n  unnest(operating_countries) |&gt;\n  select(abn, operating_countries) |&gt;\n  filter(operating_countries != \"AUS\") |&gt;\n  count(operating_countries, sort = TRUE) |&gt;\n  head(n = 10) |&gt;\n  tt() |&gt;\n  format_tt(escape = TRUE)\n\n\n\nTable 3: Most common countries of operation\n\n\n\n\n\n    \n\n    \n\n    \n    \n    \n      \n        \n        \n              \n                operating_countries\n                n\n              \n        \n        \n        \n                \n                  IDN\n                  430\n                \n                \n                  PHL\n                  385\n                \n                \n                  PNG\n                  371\n                \n                \n                  KEN\n                  360\n                \n                \n                  UGA\n                  299\n                \n                \n                  NPL\n                  270\n                \n                \n                  FJI\n                  263\n                \n                \n                  IND\n                  247\n                \n                \n                  THA\n                  241\n                \n                \n                  VNM\n                  240\nThe results of the following code are shown in Table 4.\nregistry |&gt;\n  unnest(operating_countries) |&gt;\n  distinct(abn, operating_countries) |&gt;\n  filter(operating_countries != \"AUS\") |&gt;\n  count(abn, name = \"num_countries\", sort = TRUE) |&gt;\n  mutate(num_countries = if_else(num_countries &gt; 10, \"More than 10\", \n                                 as.character(num_countries)),\n         num_countries = fct_inorder(num_countries)) |&gt;\n  count(num_countries) |&gt;\n  arrange(desc(num_countries)) |&gt;\n  tt() |&gt;\n  style_tt(align = \"ld\") |&gt;\n  format_tt(escape = TRUE) \n\n\n\nTable 4: Number of countries of operation per charity\n\n\n\n\n\n    \n\n    \n\n    \n    \n    \n      \n        \n        \n              \n                num_countries\n                n\n              \n        \n        \n        \n                \n                  1\n                  1711\n                \n                \n                  2\n                  455\n                \n                \n                  3\n                  237\n                \n                \n                  4\n                  137\n                \n                \n                  5\n                  112\n                \n                \n                  6\n                  79\n                \n                \n                  7\n                  56\n                \n                \n                  8\n                  42\n                \n                \n                  9\n                  35\n                \n                \n                  10\n                  29\n                \n                \n                  More than 10\n                  187\nThe results of the following code are shown in Table 5.\nregistry |&gt;\n  unnest(subtypes) |&gt;\n  count(subtypes, sort = TRUE) |&gt;\n  head(n = 10) |&gt;\n  tt() |&gt;\n  style_tt(align = \"ld\") |&gt;\n  format_tt(escape = TRUE)\n\n\n\nTable 5: Most common charity subtypes\n\n\n\n\n\n    \n\n    \n\n    \n    \n    \n      \n        \n        \n              \n                subtypes\n                n\n              \n        \n        \n        \n                \n                  Advancing Religion\n                  16954\n                \n                \n                  Advancing social or public welfare\n                  12624\n                \n                \n                  Advancing Education\n                  11887\n                \n                \n                  PBI\n                  11696\n                \n                \n                  Purposes beneficial to ther general public and other analogous\n                  6674\n                \n                \n                  Advancing Health\n                  6305\n                \n                \n                  Advancing Culture\n                  5121\n                \n                \n                  HPC\n                  2463\n                \n                \n                  Advancing natual environment\n                  2153\n                \n                \n                  Promoting reconciliation  mutual respect and tolerance\n                  1440"
  },
  {
    "objectID": "published/acnc_registry_arrow.html#footnotes",
    "href": "published/acnc_registry_arrow.html#footnotes",
    "title": "ACNC Registry data: Arrow version",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nExecute install.packages(c(\"tidyverse\", \"arrow\", \"tinytable\", \"farr\")) within R to install all the packages you need to run the code in this note.↩︎"
  },
  {
    "objectID": "published/bikes.html",
    "href": "published/bikes.html",
    "title": "A quick look at City of Melbourne bike data",
    "section": "",
    "text": "In writing this note, I use the packages listed below.1 This note was written using Quarto and compiled with RStudio, an integrated development environment (IDE) for working with R. The source code for this note is available here and the latest version of this PDF is here.2\nlibrary(tidyverse)\nlibrary(duckdb)\nlibrary(tinytable)\nThe following code downloads the data and unzips the single file therein. It is cached (cache: true in chunk options) to save time with repeated runs of the code.\nt &lt;- tempfile(fileext = \".zip\")\nurl &lt;- str_c(\"https://opendatasoft-s3.s3.amazonaws.com/\",\n             \"downloads/archive/74id-aqj9.zip\")\ndownload.file(url, t)\nunzip(t)\nI create a database connection and load the icu extension, which contains time-zone information.3\ndb &lt;- dbConnect(duckdb::duckdb(), timezone_out = \"Australia/Melbourne\")\ndbExecute(db, \"INSTALL icu\")\ndbExecute(db, \"LOAD icu\")\nThe following SQL creates bikes_raw, which is fairly unprocessed data. Only RUNDATE is given a type, and this is TIMESTAMP because there is no time zone information in the data.\nCREATE OR REPLACE TABLE bikes_raw AS\n  SELECT *\n  FROM read_csv('74id-aqj9.csv',\n                timestampformat='%Y%m%d%H%M%S',\n                types={'RUNDATE': 'TIMESTAMP'});\nThe following SQL produces some information on the contents of bikes_raw that is shown in Table 1.\nSELECT column_name, column_type, max, null_percentage\nFROM (SUMMARIZE bikes_raw);\nTable 1: Information on unprocessed data (bikes_raw)\n\n\n\n\n\n    \n\n    \n\n    \n    \n    \n      \n        \n        \n              \n                column_name\n                column_type\n                max\n                null_percentage\n              \n        \n        \n        \n                \n                  ID\n                  BIGINT\n                  57\n                  0.00\n                \n                \n                  NAME\n                  VARCHAR\n                  Yorkshire Brewery - Wellington St - Collingwood\n                  0.00\n                \n                \n                  TERMINALNAME\n                  BIGINT\n                  60052\n                  0.00\n                \n                \n                  NBBIKES\n                  BIGINT\n                  39\n                  0.00\n                \n                \n                  NBEMPTYDOCKS\n                  BIGINT\n                  39\n                  0.00\n                \n                \n                  RUNDATE\n                  TIMESTAMP\n                  2018-09-04 10:00:10\n                  0.00\n                \n                \n                  INSTALLED\n                  BOOLEAN\n                  true\n                  0.00\n                \n                \n                  TEMPORARY\n                  BOOLEAN\n                  false\n                  0.00\n                \n                \n                  LOCKED\n                  BOOLEAN\n                  true\n                  0.00\n                \n                \n                  LASTCOMMWITHSERVER\n                  BIGINT\n                  1507119446229\n                  0.00\n                \n                \n                  LATESTUPDATETIME\n                  BIGINT\n                  1507119264599\n                  0.02\n                \n                \n                  REMOVALDATE\n                  VARCHAR\n                  NA\n                  100.00\n                \n                \n                  INSTALLDATE\n                  BIGINT\n                  1450061460000\n                  22.01\n                \n                \n                  LAT\n                  DOUBLE\n                  -37.79625\n                  0.00\n                \n                \n                  LONG\n                  DOUBLE\n                  144.988507\n                  0.00\n                \n                \n                  LOCATION\n                  VARCHAR\n                  (-37.867068, 144.976428)\n                  0.00\nThe following function is created in R, but generates SQL. The documentation for make_timestamptz() says that it returns “the TIMESTAMP WITH TIME ZONE for the given µs since the epoch.” But it seems the data we have are in milliseconds, not microseconds, so we need to multiply by 1000.\nepoch_to_ts &lt;- function(x) {\n  x &lt;- rlang::as_name(rlang::enquo(x))\n  dplyr::sql(stringr::str_c(\"make_timestamptz(\", x, \" * 1000)\"))\n}\nThe following code converts rundate to TIMESTAMPTZ assuming the original data are Melbourne times. It also converts lastcommwithserver, latestupdatetime, and installdate to TIMESTAMPTZ. Note that attention needs to be paid to time zones, because the epoch is defined as “the number of seconds since 1970-01-01 00:00:00 UTC”, which would be a different point in time from 1970-01-01 00:00:00 in Melbourne time.\nbikes &lt;-\n  tbl(db, \"bikes_raw\") |&gt;\n  rename_with(str_to_lower) |&gt;\n  select(-installed, -temporary, -removaldate) |&gt;\n  mutate(rundate = timezone(\"Australia/Melbourne\", rundate),\n         lastcommwithserver = !!epoch_to_ts(lastcommwithserver),\n         latestupdatetime = !!epoch_to_ts(latestupdatetime),\n         installdate = !!epoch_to_ts(installdate)) |&gt;\n  compute(name = \"bikes\", overwrite = TRUE)\nThe following SQL produces some information on the contents of bikes that is shown in Table 2.\nSELECT column_name, column_type, max, null_percentage\nFROM (SUMMARIZE bikes);\nTable 2: Information on processed data (bikes)\n\n\n\n\n\n    \n\n    \n\n    \n    \n    \n      \n        \n        \n              \n                column_name\n                column_type\n                max\n                null_percentage\n              \n        \n        \n        \n                \n                  id\n                  BIGINT\n                  57\n                  0.00\n                \n                \n                  name\n                  VARCHAR\n                  Yorkshire Brewery - Wellington St - Collingwood\n                  0.00\n                \n                \n                  terminalname\n                  BIGINT\n                  60052\n                  0.00\n                \n                \n                  nbbikes\n                  BIGINT\n                  39\n                  0.00\n                \n                \n                  nbemptydocks\n                  BIGINT\n                  39\n                  0.00\n                \n                \n                  rundate\n                  TIMESTAMP WITH TIME ZONE\n                  2018-09-03 20:00:10-04\n                  0.00\n                \n                \n                  locked\n                  BOOLEAN\n                  true\n                  0.00\n                \n                \n                  lastcommwithserver\n                  TIMESTAMP WITH TIME ZONE\n                  2017-10-04 08:17:26.229-04\n                  0.00\n                \n                \n                  latestupdatetime\n                  TIMESTAMP WITH TIME ZONE\n                  2017-10-04 08:14:24.599-04\n                  0.02\n                \n                \n                  installdate\n                  TIMESTAMP WITH TIME ZONE\n                  2015-12-13 21:51:00-05\n                  22.01\n                \n                \n                  lat\n                  DOUBLE\n                  -37.79625\n                  0.00\n                \n                \n                  long\n                  DOUBLE\n                  144.988507\n                  0.00\n                \n                \n                  location\n                  VARCHAR\n                  (-37.867068, 144.976428)\n                  0.00\nbikes |&gt; \n  select(lastcommwithserver, latestupdatetime, rundate, installdate) |&gt;\n  collect(n = 10)\n\n\n\nTable 3: Sample of date-time variables\n\n\n\n\n\n    \n\n    \n\n    \n    \n    \n      \n        \n        \n              \n                lastcommwithserver\n                latestupdatetime\n                rundate\n                installdate\n              \n        \n        \n        \n                \n                  2017-04-22 13:42:46.01\n                  2017-04-22 13:42:45.029\n                  2017-04-22 13:45:06\n                  2011-08-19 13:30:00\n                \n                \n                  2017-04-22 13:43:51.727\n                  2017-04-22 13:36:17.573\n                  2017-04-22 13:45:06\n                  NA\n                \n                \n                  2017-04-22 13:33:35.231\n                  2017-04-22 13:33:33.615\n                  2017-04-22 13:45:06\n                  NA\n                \n                \n                  2017-04-22 13:36:58.661\n                  2017-04-22 12:51:55.84\n                  2017-04-22 13:45:06\n                  NA\n                \n                \n                  2017-04-22 13:35:03.674\n                  2017-04-21 19:56:38.168\n                  2017-04-22 13:45:06\n                  NA\n                \n                \n                  2017-04-22 13:32:35.565\n                  2017-04-22 13:18:29.294\n                  2017-04-22 13:45:06\n                  2012-12-27 08:00:00\n                \n                \n                  2017-04-22 13:41:32.347\n                  2017-04-22 11:55:01.271\n                  2017-04-22 13:45:06\n                  NA\n                \n                \n                  2017-04-22 13:34:42.173\n                  2017-04-22 13:34:40.671\n                  2017-04-22 13:45:06\n                  NA\n                \n                \n                  2017-04-22 13:36:33.207\n                  2017-04-22 11:49:37.265\n                  2017-04-22 13:45:06\n                  NA\n                \n                \n                  2017-04-22 13:37:50.326\n                  2017-04-22 13:37:48.824\n                  2017-04-22 13:45:06\n                  2010-06-22 12:53:00\nIn making Figure 1, I convert the date component of runtime to the same date (2017-01-01). This facilitates plotting in R, as R has no native “time” type and thus things are easier using date-times. Unfortunately, it seems that all the timestamps in bikes are boring back-end times produced by systems, so there is nothing special about the distribution of these times. More interest plots might come from looking at when bikes are checked out and in (only net checkouts seem to be available) assuming that the data are sufficiently frequent.\nbikes |&gt;\n  mutate(runtime = make_timestamptz(2017L, 1L, 1L, \n                                    hour(rundate), minute(rundate), \n                                    second(rundate))) |&gt;\n  ggplot(aes(runtime)) +\n  geom_histogram(binwidth = 60 * 60) +\n  scale_x_datetime(date_breaks = \"1 hour\", date_labels = \"%H\")\n\n\n\n\n\n\n\nFigure 1: Distribution of times in runtime"
  },
  {
    "objectID": "published/bikes.html#footnotes",
    "href": "published/bikes.html#footnotes",
    "title": "A quick look at City of Melbourne bike data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nExecute install.packages(c(\"tidyverse\", duckdb\", \"tinytable\")) within R to install all the packages you need to run the code in this note.↩︎\nSome parts of the source code are ugly as I wrangled hurriedly with the output from SQL and LaTeX tables.↩︎\nYou may need to run INSTALL icu before LOAD icu depending on your DuckDB installation.↩︎"
  },
  {
    "objectID": "published/bos-seasons.html",
    "href": "published/bos-seasons.html",
    "title": "Defining winter and summer in Boston",
    "section": "",
    "text": "Figure 1: Average daily temperatures for 91 days following indicated date for period 2001–2023\n\n\n\n\n\nIn the United States, one often hears people speak of the “official” start of seasons. Ironically, there seems to be nothing that is official about these dates. However, there is consensus about the dates. The “official” start of summer is the summer solstice (for 2024: 20 June in Boston, 21 December in Melbourne) and the “official” start of winter is (for 2024: 21 June in Melbourne, 21 December in Boston).\nIn Australia, the usual convention is to divide seasons by months. On this basis, winter starts on 1 June and summer starts on 1 December.\nIs there a sense in which one approach is more correct than the other? Focusing on summer and winter, one definition for these seasons would be that winter starts on the first day of the 91-day period that is the coldest such period for a year averaged over a number of years. Similarly, summer should starts on the first day of the 91-day period that is the hottest such period for a year averaged over a number of years.\nWe answer this question focusing on Boston, USA (latitude of 42.3, longitude: -71.0).\nDaily temperature data from Open-Meteo comprise a maximum and minimum temperature. So immediately we have two possible definitions of each season according to the temperature we use (e.g., summer could be the 91-day period that has the highest average minimum temperature or it could be the period that has the highest average maximum temperature. Here we consider both.\nThe start of winter based on the 91-day period with the lowest average maximum temperature is 07 December. The start of winter based on the 91-day period with the lowest average minimum temperature is 08 December.\nThe start of summer based on the 91-day period with the highest average maximum temperature is 16 June. The start of summer based on the 91-day period with the highest average minimum temperature is 17 June. So using either, we get close to the Australian convention for winter and close to the US convention for summer.\nInterestingly, it seems that using average maximums for summer and winter gets closest to the current approach in Australia. However, even using these we have the issue that spring begins on 08 March and autumn begins on 15 September. This implies a spring of 100 days and an autumn of 83 days."
  },
  {
    "objectID": "published/ctes.html",
    "href": "published/ctes.html",
    "title": "Writing better SQL without writing SQL",
    "section": "",
    "text": "In this note, I use a query from Tanimura (2021) to illustrate first how one can re-write an SQL query using common table expressions (CTEs) and then how one can re-write that query again using dbplyr. I then do the analysis again from scratch, but using dbplyr expressions. I find that the SQL query contains inaccuracies, while the written-from-scratch dbplyr query does now. I conjecture that the “building blocks” approach to SQL facilitated by dbplyr may lead to more accurate of queries for many users.\nIn writing this note, I used the packages listed below.1 This note was written using Quarto and compiled with RStudio, an integrated development environment (IDE) for working with R. The source code for this note is available here and the latest version of this PDF is here.\nlibrary(DBI)\nlibrary(tidyverse)\nlibrary(dbplyr)"
  },
  {
    "objectID": "published/ctes.html#reproducing-figure-4-12-of-tanimura2021sql",
    "href": "published/ctes.html#reproducing-figure-4-12-of-tanimura2021sql",
    "title": "Writing better SQL without writing SQL",
    "section": "4.1 Reproducing Figure 4-12 of Tanimura (2021)",
    "text": "4.1 Reproducing Figure 4-12 of Tanimura (2021)\nFinally, I reproduce Figure 4-12 of Tanimura (2021, p. 163). This plot requires changes to the cohorts, to the cutoffs (now 10 and 20 years), and (it seems) to the population. From visual inspection of Figure 4-12 of Tanimura (2021), it seems that we now require a representative’s first term to have begun before 2000.\nNote that the query underlying Figure 4-12 of Tanimura (2021) is not provided in the book. Nor is the code for generating the plot itself. Because Tanimura (2021) only include SQL code, readers are on their own when it comes to code for generating the plots; this is an additional weakness of focusing on SQL code.\nThe first step I take is to make a new version of pct_rep_then_sen with cohort now based on decades and with the stricter filter on first_rep_term. Note that I simply overwrite whatever value for cohort was already in cohorts_revised. I also embed the calculation of reps (the number of members of each cohort) in the same pipeline as the other calculations.\n\npct_rep_then_sen &lt;-\n  cohorts_revised |&gt;\n  filter(first_rep_term &lt;= \"1999-12-31\") |&gt;\n  mutate(cohort = decade(first_rep_term),\n         event_time = age(first_sen_term, first_rep_term)) |&gt;\n  mutate(reps = n(), .by = cohort) |&gt;\n  mutate(cum_ids = cumsum(1), .by = cohort, .order = event_time) |&gt;\n  mutate(cum_ids = max(cum_ids, na.rm = TRUE),\n         .by = c(cohort, event_time)) |&gt;\n  mutate(pct = cum_ids / reps) \n\nI replace event_time_cutoffs with the new values (10 and 20 years):\n\nevent_time_cutoffs &lt;-\n  tibble(cutoff = c(10, 20)) |&gt;\n  copy_to(db, df = _, name = \"event_time_cutoffs\",\n          overwrite = TRUE) |&gt;\n  mutate(cutoff = years(cutoff))\n\nMaking the plot is now quite straightforward and the results of the following code can be seen in Figure 2:\n\npct_rep_then_sen |&gt;\n  cross_join(event_time_cutoffs) |&gt;\n  filter(event_time &lt;= cutoff) |&gt;\n  summarize(pct = max(pct, na.rm = TRUE),\n            .by = c(cohort, cutoff)) |&gt;\n  mutate(cutoff = as.character(year(cutoff))) |&gt;\n  ggplot(aes(x = cohort, y = pct, color = cutoff, group = cutoff)) +\n  geom_line()\n\n\n\n\n\n\n\nFigure 2: Share of representatives who become senators by decade"
  },
  {
    "objectID": "published/ctes.html#sec-fun",
    "href": "published/ctes.html#sec-fun",
    "title": "Writing better SQL without writing SQL",
    "section": "4.2 Making a function",
    "text": "4.2 Making a function\nCreating the code underlying Figure 4-12 of Tanimura (2021) likely involved a lot of copy-pasting and editing (e.g., to create new CASE statements for the new cutoffs and new WHERE clauses) even before moving the code (or data) to Python or Tableau to make the plot. The code above suggests that we might accomplish variants on the plot more programmatically and I pursue this idea in this section.\nOne benefit of doing this is that I can show how putting the survival data into a canonical structure can make it easier to run variants based on different populations and cohort definitions.\nFirst, I put the essence of the code in the following function with the only real edits being that I use more generic names for the tables and fields and put the cutoffs to be used in a variable cutoffs.\n\nmake_plot &lt;- function(cohorts, survival_data, cutoffs = c(10, 20)) {\n  plot_data &lt;-\n    survival_data |&gt;\n    mutate(event_time = age(event_date, entry_date)) |&gt;\n    inner_join(cohorts, by = \"id\") |&gt;\n    mutate(reps = n(), .by = cohort) |&gt;\n    mutate(cum_ids = cumsum(1), .by = cohort, .order = event_time) |&gt;\n    mutate(cum_ids = max(cum_ids, na.rm = TRUE),\n           .by = c(cohort, event_time)) |&gt;\n    mutate(pct = cum_ids / reps) \n  \n  event_time_cutoffs &lt;-\n    tibble(cutoff = cutoffs) |&gt;\n    copy_to(db, df = _, name = \"event_time_cutoffs\",\n            overwrite = TRUE) |&gt;\n    mutate(cutoff = years(cutoff))\n  \n  plot_data |&gt;\n    cross_join(event_time_cutoffs) |&gt;\n    filter(event_time &lt;= cutoff) |&gt;\n    summarize(pct = max(pct, na.rm = TRUE),\n              .by = c(cohort, cutoff)) |&gt;\n    mutate(cutoff = as.character(year(cutoff))) |&gt;\n    ggplot(aes(x = cohort, y = pct, color = cutoff, group = cutoff)) +\n    geom_line()\n}\n\nI next construct survival_data in a canonical form with id, entry_date, and event_date as the fields.\n\nsurvival_data &lt;-\n  first_terms |&gt;\n  filter(!is.na(first_rep_term), \n         first_term == first_rep_term) |&gt;\n  rename(id = id_bioguide,\n         entry_date = first_rep_term,\n         event_date = first_sen_term) |&gt;\n  select(id, entry_date, event_date)\n\nNow I can easily reproduce Figure 2 using the following code with the results being seen in Figure 3:\n\nsurvival_data |&gt;\n  filter(entry_date &lt;= \"1999-12-31\") |&gt;\n  mutate(cohort = decade(entry_date)) |&gt;\n  select(id, cohort) |&gt;\n  make_plot(survival_data = survival_data,\n            cutoffs = c(10, 20))\n\n\n\n\n\n\n\nFigure 3: Share of representatives who become senators by decade (encore)\n\n\n\n\n\nAnd with a few lines of code, I can make a plot version of Table 4, which can be seen in Figure 4:\n\nsurvival_data |&gt;\n  filter(entry_date &lt;= \"2009-12-31\") |&gt;\n  mutate(cohort = century(entry_date)) |&gt;\n  select(id, cohort) |&gt;\n  make_plot(survival_data = survival_data,\n            cutoffs = c(5, 10, 15))\n\n\n\n\n\n\n\nFigure 4: Share of representatives who become senators by century"
  },
  {
    "objectID": "published/ctes.html#footnotes",
    "href": "published/ctes.html#footnotes",
    "title": "Writing better SQL without writing SQL",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nExecute install.packages(c(\"tidyverse\", \"DBI\", \"duckdb\", \"dbplyr\") within R to install all the packages you need to run the code in this note.↩︎\nI made minor punctuation edits here.↩︎\nI put the ugly details of the db_get_csv() function that I use here in Listing 1.↩︎\nI edited the query slightly to reflect code style guidelines I use in later queries. I also rename the column cohort_century to cohort throughout. The value of using a more generic name will be seen in Section 4.2.↩︎\nMy view is that the choice made early in the development dplyr of a de facto default of .groups = \"drop_last\" is a rather unfortunate one.↩︎\nNote that if_else(age &lt;= years(5), id_bioguide, NA) would be an alternative way to get the same result as case_when(age &lt;= years(5) ~ id_bioguide) gives.↩︎\nAn alternative would’ve been to use pct_ in place of num_ in the age_cuts query.↩︎\nStrictly speaking, it should be “on or before 31 December 2009”, but legislators never start terms on 31 December.↩︎\nSee https://iangow.github.io/cohorts/intermezzo.html for some discussion on this point.↩︎\nThis is equivalent to count(DISTINCT id_bioguide) OVER (PARTITION BY cohort ORDER BY event_time), but we do not need the DISTINCT here because each value of id_bioguide is unique in this query.↩︎\nI explain why I think it is a more precise solution in Section 5.↩︎\nI made minor punctuation edits here.↩︎"
  },
  {
    "objectID": "published/data_mgt.html",
    "href": "published/data_mgt.html",
    "title": "Data management ideas for researchers",
    "section": "",
    "text": "My sense is that data management is a challenge for researchers. In an academic context, some fields may receive greater institutional support than others. My experience in business schools was that there was very little support for data curation and management. While many of the ideas I discuss here are general in nature, for concreteness, I focus on the special case of a WRDS user maintaining a local Parquet data library of the kind discussed in Appendix E of Empirical Research in Accounting: Tools and Methods and provide examples using my Python package db2pq."
  },
  {
    "objectID": "published/data_mgt.html#caveats",
    "href": "published/data_mgt.html#caveats",
    "title": "Data management ideas for researchers",
    "section": "2.1 Caveats",
    "text": "2.1 Caveats\nAt the outset, I should note some limitations to my discussion here.\nFirst, this note does not data at the scale of multiple terabytes. Researchers working with data at the scale that one starts thinking about Spark clusters and immense cloud storage will not find any answers here. That said, I think the approaches I cover here scale up to a “low terabyte” scale, at least for aggregate data.\nSecond, with very few exceptions, I have not really dealt with data with confidentiality issues. Readers dealing with sensitive data would need to overlay the necessary safeguards and protocols associated with such data onto the discussion I provide here."
  },
  {
    "objectID": "published/data_mgt.html#some-concepts-in-data-management",
    "href": "published/data_mgt.html#some-concepts-in-data-management",
    "title": "Data management ideas for researchers",
    "section": "2.2 Some concepts in data management",
    "text": "2.2 Some concepts in data management\n\n2.2.1 Scope\nMany datasets are project-specific datasets, meaning that only have use within a single project (e.g., paper). Examples of project-specific would include experimental data generated in a particular study.\nOther datasets are general-purpose datasets, meaning that they contain data that might be relevant to many studies. Classic examples in a business-school context would be the US stock price files offered by the Center for Research in Security Prices, LLC (CRSP) or financial statement data provided by Compustat, or economic time-series data provided by various statistical offices around the world.\nOther datasets are project-level datasets, meaning that the particular data sets are somehow frozen for a particular project, even though the nature of the data otherwise puts them in the category of general-purpose datasets. For example, I might want to fix on a particular version of comp.g_funda, Compustat’s global dataset for annual financial statements for my project, even though this dataset has relevance beyond a specific project.1\nThere are two reasons for having project-level that I can think of. The first reason arises in the context of reproducibility. If I have published a paper, then the replication package for that paper should ideally contain the data used to produce the exact results in the paper. For this purpose, if the paper used comp.g_funda data, then the ideal replication package would include the precise project-level version of that data set used to produce the paper. Of course, in reality, one cannot simply post the project-level version of comp.g_funda as part of a public replication package. Nonetheless, the authors themselves should have a project-level version of the dataset that they retain. This much aligns with the views of Welch (2019), who suggests that “the author should keep a private copy of the full data set with which the results were obtained.”\nThe second reason is related to the first, but in some ways opposite in spirit. Some authors do not want their results to be upset by updates to any of datasets used to produce them. On one research project, I was responsible for supplying a curated data set of significant scale and complexity. Unfortunately, my understanding of variable scoping in Perl meant that about 2% of the data were simply corrupted and I felt I had to fix this. At my end, I wanted to manage the data as a general-purpose data set, but my co-author wanted to stick to the earlier project-level data.2\nAs far as WRDS data are concerned, my db2pq package aims to facilitate managing WRDS data either as general-purpose data or as project-level data.3 On my computers, I have the environment variable DATA_DIR set to a location inside Dropbox. So, by default, new WRDS data will go in the matching schema (i.e., subdirectory) of the directory indicated by DATA_DIR. In Python, I can inspect the value in DATA_DIR:\n\nimport os\nos.environ['DATA_DIR']\n\n'/Users/igow/Dropbox/pq_data'\n\n\nFor this note, I’m using the very latest version of db2pq, which you can install by running pip install --upgrade db2pq from the command line. If you do not have pandas or paramiko installed, you should do pip install --upgrade \"db2pq[sas,pandas]\". See the db2pq PyPI homepage for more details.\nWithin Python, you can check the version using the following.4\n\nimport db2pq \ndb2pq.__version__\n\n'0.2.1.dev0'\n\n\nThe core function of db2pq is wrds_update_pq() If I ask wrds_update_pq() to update my general-purpose version crsp.dsi, I can see that the latest data on WRDS are no more recent than what I already have, so no update occurs.\n\nfrom db2pq import wrds_update_pq\nwrds_update_pq(\"dsi\", \"crsp\")\n\ncrsp.dsi already up to date.\n\n\nBut, if I wanted a project-level version of crsp.dsi, I can specify the project-level data directory (\"data\") and WRDS will update the data there. As I don’t have any data in that folder to begin with, an “update” occurs.\n\nfrom db2pq import wrds_update_pq\nwrds_update_pq(\"dsi\", \"crsp\", data_dir=\"data\")\n\nUpdated crsp.dsi is available.\nBeginning file download at 2026-02-26 14:28:45 UTC.\nCompleted file download at 2026-02-26 14:28:48 UTC.\n\n\n\n\n2.2.2 Version control\nVersion control is a thorny issue with data. As far as I know there is no equivalent of Git for datasets.5 While I am sure that version control of data is a big issue in many contexts (e.g., data for regulated bodies), many data providers, even commercial vendors, often do a poor job of version control.\nMany data sources will provide the current version of any given dataset and nothing else. For example, there is no way to get the version of the data you downloaded from WRDS two years ago if you want to understand why results have changed. In practice, researchers need to do any archiving of WRDS data sets themselves.\nMy db2pq Python package provides some functions that make it more convenient to maintain archives of previous version of tables from WRDS. The core function for maintaining a local repository of Parquet files based on WRDS data is wrds_update_pq(). This function has an archive argument that, if set to True, causes any existing data in the repository to be archived when an update is available and is applied:\n\nwrds_update_pq(\"company\", \"comp\", archive=True)\n\nUpdated comp.company is available.\nBeginning file download at 2026-02-26 14:28:48 UTC.\nCompleted file download at 2026-02-26 14:28:50 UTC.\n\n\nFor most tables on the WRDS database, it appears that “last updated” metadata is included in table comments. The wrds_update_pq() function will, by default, extract that metadata and embed it as metadata in the Parquet files.\nThe pq_last_modified() function, if given a table_name argument, will by default return the metadata embedded in the Parquet file.\n\nfrom db2pq import pq_last_modified\npq_last_modified(table_name=\"company\", schema=\"comp\")\n\n'Company (Updated 2026-02-26)'\n\n\nBut if I specify archive=True, then pq_last_modified() will instead return a pandas data frame containing information about (possibly several) files matching the specified table_name in the archive.6 Here, we see that I have four previous versions of comp.company in my archive.\n\npq_df = pq_last_modified(table_name=\"company\", schema=\"comp\", archive=True)\npq_df[[\"file_name\", \"last_mod\"]]\n\n\n\n\n\n\n\n\nfile_name\nlast_mod\n\n\n\n\n0\ncompany_20240614T062835Z\n2024-06-14 02:28:35-04:00\n\n\n1\ncompany_20260105T070000Z\n2026-01-05 02:00:00-05:00\n\n\n2\ncompany_20260107T070000Z\n2026-01-07 02:00:00-05:00\n\n\n3\ncompany_20260209T070000Z\n2026-02-09 02:00:00-05:00\n\n\n4\ncompany_20260218T070000Z\n2026-02-18 02:00:00-05:00\n\n\n5\ncompany_20260224T070000Z\n2026-02-24 02:00:00-05:00\n\n\n6\ncompany_20260225T000000Z\n2026-02-24 02:00:00-05:00\n\n\n7\ncompany_20260225T070000Z\n2026-02-25 02:00:00-05:00\n\n\n8\ncompany_20260226T070000Z\n2026-02-26 02:00:00-05:00\n\n\n\n\n\n\n\nI can use the function pq_restore() to make the one from 2024-06-14 the one that I am using for my data repository.\n\nfrom db2pq import pq_restore\npq_restore(\"company_20240614T062835Z\", \"comp\")\n\n'/Users/igow/Dropbox/pq_data/comp/company.parquet'\n\n\nI now see that this is the version used when I look for company in the comp schema:\n\npq_last_modified(table_name=\"company\", schema=\"comp\")\n\n'Last modified: 06/14/2024 02:28:35'\n\n\nOne thing you will notice is that the format of the “last modified” string has changed from the one above. This could be for one of three reasons:\n\nThe Parquet file that has been restored was created using my other Python package, wrds2pg, which extracts data from WRDS’s SAS data files. Naturally, it uses information returned by the SAS command PROC CONTENTS.\nThe Parquet file that has been restored was created using an earlier version of db2pq. Because WRDS did not initially store “last modified” information with its PostgreSQL tables, earlier version of db2pq retrieved information from the matching SAS file on the assumption that the SAS and PostgreSQL data sets would generally be aligned.\nThe Parquet file that has been restored was created using a recent version of db2pq, but with use_sas=True. In this case, wrds_update_pq() will retrieve the “last modified” information from the SAS file.7\n\nBy default, the pq_restore() function has archive=True, which means that any existing data file is archived.8 We can see that the file that we created just moments ago using wrds_update_pq() is now in the archive:\n\npq_df = pq_last_modified(table_name=\"company\", schema=\"comp\", archive=True)\npq_df[[\"file_name\", \"last_mod\"]]\n\n\n\n\n\n\n\n\nfile_name\nlast_mod\n\n\n\n\n0\ncompany_20260105T070000Z\n2026-01-05 02:00:00-05:00\n\n\n1\ncompany_20260107T070000Z\n2026-01-07 02:00:00-05:00\n\n\n2\ncompany_20260209T070000Z\n2026-02-09 02:00:00-05:00\n\n\n3\ncompany_20260218T070000Z\n2026-02-18 02:00:00-05:00\n\n\n4\ncompany_20260224T070000Z\n2026-02-24 02:00:00-05:00\n\n\n5\ncompany_20260225T000000Z\n2026-02-24 02:00:00-05:00\n\n\n6\ncompany_20260225T070000Z\n2026-02-25 02:00:00-05:00\n\n\n7\ncompany_20260226T070000Z\n2026-02-26 02:00:00-05:00\n\n\n\n\n\n\n\nIf we update again with archive=True, we will effectively put the 2024-06-14 back in the archive and replace it with the current version on WRDS.\n\nfrom db2pq import pq_archive\nwrds_update_pq(\"company\", \"comp\", archive=True)\n\nUpdated comp.company is available.\nBeginning file download at 2026-02-26 14:28:51 UTC.\nCompleted file download at 2026-02-26 14:28:53 UTC.\n\n\nSome WRDS PostgreSQL tables appear not (yet) to have “last modified” information. For example, some RavenPack data tables appear not to have this information. In the following, I set obs=100 and data_dir=\"data\", as I am doing this “update” purely for the purposes of this note, so only get 100 observations to keep things fast.\n\nwrds_update_pq(\"rpa_entity_mappings\", \"ravenpack_common\", obs=100, \n               data_dir=\"data\")\n\nNo comment found for ravenpack_common.rpa_entity_mappings.\nravenpack_common.rpa_entity_mappings already up to date.\n\n\nWe can confirm this using pq_last_modified():\n\npq_last_modified(table_name=\"rpa_entity_mappings\", schema=\"ravenpack_common\",\n                 data_dir=\"data\")\n\n''\n\n\nIn such cases, any subsequent call to wrds_update_pq() will not trigger an “update” because there is effectively nothing to allow it to confirm that the local data are not current.9\n\nwrds_update_pq(\"rpa_entity_mappings\", \"ravenpack_common\", obs=100, \n               data_dir=\"data\")\n\nNo comment found for ravenpack_common.rpa_entity_mappings.\nravenpack_common.rpa_entity_mappings already up to date.\n\n\nIn such cases, it makes sense to use the SAS data to determine the vintage of the data. However, a wrinkle in this case is that there is no SAS library called ravenpack_common. Instead the data are stored in the SAS library named rpa. So we also need to tell wrds_update_pq() where to get the SAS data.\n\nwrds_update_pq(\"rpa_entity_mappings\", \"ravenpack_common\", obs=100, \n               data_dir=\"data\", use_sas=True, sas_schema=\"rpa\")\n\nUpdated ravenpack_common.rpa_entity_mappings is available.\nBeginning file download at 2026-02-26 14:29:00 UTC.\nCompleted file download at 2026-02-26 14:29:02 UTC.\n\n\nNow we have valid “last modified” data:\n\npq_last_modified(table_name=\"rpa_entity_mappings\", schema=\"ravenpack_common\",\n                 data_dir=\"data\")\n\n'Last modified: 02/09/2026 16:11:10'\n\n\nSo a subsequent call to wrds_update_pq() does not trigger an update, but for the correct reasons.\n\nwrds_update_pq(\"rpa_entity_mappings\", \"ravenpack_common\", obs=100, \n               data_dir=\"data\", use_sas=True, sas_schema=\"rpa\")\n\nravenpack_common.rpa_entity_mappings already up to date.\n\n\n\n\n2.2.3 Storage formats\nWhile there are many storage formats available for data, I think a strong case can be made for Parquet being the default storage format for many users. If you use R or Python, I think the case is easy to make. Many software packages can read Parquet data and some of them (e.g., Polars or DuckDB) will absolutely fly with Parquet data.\nI believe that recent editions of Stata can read Parquet files, though the way Stata operates means that Stata users are unlikely to see the performance benefits Parquet offers.10 SAS users might find the case for Parquet less compelling, though there are probably benefits in moving to a storage medium that is more compact, less proprietary, and more likely to be supported in a few years’ time.\nOf course, an alternative to using Parquet files would be using a database, such as PostgreSQL. I think such systems have a lot of merit (and I have used PostgreSQL to store WRDS data since 2011), but I think they are more complicated for most users’ needs and their benefits (e.g., shared access to data and rock-solid assurance) are less meaningful for most.\nAnother alternative is the CSV file, perhaps compressed. I think if one were sending data on the next Voyager mission, then CSV might be the chosen format.11 Or if you really, really wanted data novices to inspect your data in Excel or Word, then CSV might be the go-to option. Or perhaps you want to put your data in a written form in a book for users to type in. For any other purpose with serious data needs, I think Parquet dominates.\nOne issue with CSV is that one is always dealing with type inference (string, integer, timestamp) and I think that type inference is one of those problems you want to solve once for any given dataset. For the WRDS data that is the focus of this note, I think CSV is to be avoided.\n\n\n2.2.4 Timestamps\nSpeaking of type inference, one bane of the existence of any data analyst might be timestamps. The usual purpose of timestamps is to identify a moment in time. For example, I want to know the precise time at which an earnings conference call happened, so I can turn to a dataset with intra-day data on quotes and trades to see how the market reacted. If the data on earnings conference call use UTC and the trade-and-quote data use US Eastern time and I ignore these differences, then I will be looking at times that are off by four or five hours (depending on the time of year).\nTo examine this issue, I’m going to look revisit the Call Report data I wrote about recently. I have these data in my (general-purpose) data repository and I can use the following function to load it into Polars.\n\nimport polars as pl\nfrom pathlib import Path\nimport os\n\ndef ffiec_scan_pqs(schedule=None, *, \n                   schema=\"ffiec\", data_dir=None):\n    if data_dir is None:\n        data_dir = Path(os.environ[\"DATA_DIR\"]).expanduser()\n\n    path = data_dir / schema if schema else data_dir\n\n    if schedule is None:\n        raise ValueError(\"You must supply `schedule`.\")\n    files = list(path.glob(f\"{schedule}_*.parquet\"))\n    if not files:\n        raise FileNotFoundError(\n          f\"No Parquet files found for schedule '{schedule}' in {path}\"\n        )\n    \n    return pl.concat([pl.scan_parquet(f) for f in files])\n\nIn my earlier note, I discussed how I managed to infer that the timestamps on that dataset are in US Eastern time (America/New_York). We can inspect the data I have using the function above and focused on a single observation:\n\n(\n    ffiec_scan_pqs(\"por\")\n    .select(\"IDRSSD\", \"date\", \"last_date_time_submission_updated_on\")\n    .filter(pl.col(\"IDRSSD\") == 37, \n            pl.col(\"date\") == pl.date(2023 , 12, 31))\n    .collect()\n)\n\n\nshape: (1, 3)\n\n\n\nIDRSSD\ndate\nlast_date_time_submission_updated_on\n\n\ni32\ndate\ndatetime[μs, UTC]\n\n\n\n\n37\n2023-12-31\n2024-01-10 18:43:43 UTC\n\n\n\n\n\n\nWRDS offers essentially the same data in its bank schema. We can use wrds_update_pq() to get a sample of these data.12\n\nwrds_update_pq(\"wrds_call_rcfa_1\", \"bank\", data_dir = \"data\", obs=100)\n\nUpdated bank.wrds_call_rcfa_1 is available.\nBeginning file download at 2026-02-26 14:29:07 UTC.\nCompleted file download at 2026-02-26 14:29:09 UTC.\n\n\nTo load the data into Polars, I will use the following small function:\n\ndef load_parquet(table, schema, *, data_dir=None):\n  if data_dir is None:\n    data_dir = Path(os.environ[\"DATA_DIR\"]).expanduser()\n  else:\n    data_dir = Path(data_dir)\n  \n  path = data_dir / schema / f\"{table}.parquet\"\n  return pl.scan_parquet(path)\n\nAs can be seen from the output below, the timestamp is off by five hours. That is because wrds_update_pq() assumes that timestamps are in UTC, which is a correct assumption for some data sets on WRDS, but is incorrect in this case.\n\nrcfa_1 = load_parquet(\"wrds_call_rcfa_1\", \"bank\", data_dir = \"data\")\n(\n    rcfa_1\n    .select(\"rssd9001\", \"wrdsreportdate\", \"rssdsubmissiondate\")\n    .filter(pl.col(\"rssd9001\") == 37, \n            pl.col(\"wrdsreportdate\") == pl.date(2023, 12, 31))\n    .collect()\n)\n\n\nshape: (1, 3)\n\n\n\nrssd9001\nwrdsreportdate\nrssdsubmissiondate\n\n\ni32\ndate\ndatetime[μs, UTC]\n\n\n\n\n37\n2023-12-31\n2024-01-10 13:43:43 UTC\n\n\n\n\n\n\nWRDS generally stores timestamps in PostgreSQL with type TIMESTAMP WITHOUT TIME ZONE, which is equivalent to saying “you figure out the time zone, user.”13 Because we know that the timestamps provided by the FFIEC are expressed in US Eastern time, we can tell wrds_update_pq() this using the tz argument:\n\n# cache: true\nwrds_update_pq(\"wrds_call_rcfa_1\", \"bank\", \n               data_dir = \"data\", obs=100, \n               force=True, tz=\"America/New_York\")\n\nForcing update based on user request.\nCompleted file download at 2026-02-26 14:29:12 UTC.\n\n\nNow, we see that the data are correct.\n\nrcfa_1 = load_parquet(\"wrds_call_rcfa_1\", \"bank\")\n(\n    rcfa_1\n    .select(\"rssd9001\", \"wrdsreportdate\", \"rssdsubmissiondate\")\n    .filter(pl.col(\"rssd9001\") == 37, \n            pl.col(\"wrdsreportdate\") == pl.date(2023, 12, 31))\n    .collect()\n)\n\n\nshape: (1, 3)\n\n\n\nrssd9001\nwrdsreportdate\nrssdsubmissiondate\n\n\ni32\ndate\ndatetime[μs]\n\n\n\n\n37\n2023-12-31\n2024-01-10 13:43:43\n\n\n\n\n\n\nIn other cases, WRDS doesn’t even bother to store the data as TIMESTAMP WITHOUT TIME ZONE, but instead the data are stored as strings. Here’s one example.14\n\n# cache: true\nwrds_update_pq(\"feed03_audit_fees\", \"audit\", \n               keep=[\"auditor_fkey\", \"file_accepted\"],\n               obs=5, data_dir=\"data\")\n\nUpdated audit.feed03_audit_fees is available.\nBeginning file download at 2026-02-26 14:29:12 UTC.\nCompleted file download at 2026-02-26 14:29:14 UTC.\n\n\nBut here we see that file_accepted is stored as a string (and auditor_fkey is a floating-point value).\n\nload_parquet(\"feed03_audit_fees\", \"audit\", \n             data_dir=\"data\").collect()\n\n\nshape: (5, 2)\n\n\n\nauditor_fkey\nfile_accepted\n\n\nf64\nstr\n\n\n\n\n5.0\n\"2001-03-28 00:00:00\"\n\n\n5.0\n\"2002-03-25 00:00:00\"\n\n\n4.0\n\"2003-03-31 09:19:45\"\n\n\n6.0\n\"2004-04-06 14:34:14\"\n\n\n6.0\n\"2005-04-04 11:55:05\"\n\n\n\n\n\n\nFortunately, with wrds_update_pq(), I can specify the (Arrow) data types for selected columns and, in the case of timestamp, the time zone.\n\nwrds_update_pq(\"feed03_audit_fees\", \"audit\", \n               col_types={\"auditor_fkey\": \"int32\",\n                          \"file_accepted\": \"timestamp\"},\n               tz=\"America/New_York\", force=True,\n               keep=[\"auditor_fkey\", \"file_accepted\"],\n               data_dir=\"data\")\n\nForcing update based on user request.\nCompleted file download at 2026-02-26 14:29:17 UTC.\n\n\nNow things look much better.\n\nload_parquet(\"feed03_audit_fees\", \"audit\", \n             data_dir=\"data\").head().collect()\n\n\nshape: (5, 2)\n\n\n\nauditor_fkey\nfile_accepted\n\n\ni32\ndatetime[μs, UTC]\n\n\n\n\n5\n2001-03-28 05:00:00 UTC\n\n\n5\n2002-03-25 05:00:00 UTC\n\n\n4\n2003-03-31 14:19:45 UTC\n\n\n6\n2004-04-06 18:34:14 UTC\n\n\n6\n2005-04-04 15:55:05 UTC"
  },
  {
    "objectID": "published/data_mgt.html#other-ideas",
    "href": "published/data_mgt.html#other-ideas",
    "title": "Data management ideas for researchers",
    "section": "2.3 Other ideas",
    "text": "2.3 Other ideas\nThere are several ideas not covered by this note currently, but that might be added later:\n\nBack up your data\nModification of raw data files\nThe WRDS web query\n\nIn the last case, don’t use it! (I will explain why, but one issue is reproducibility.)"
  },
  {
    "objectID": "published/data_mgt.html#footnotes",
    "href": "published/data_mgt.html#footnotes",
    "title": "Data management ideas for researchers",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAs academic researchers generally get Compustat data through Wharton Research Data Services (WRDS), I refer to this dataset using the nomenclature used by WRDS. Here “global” means “not the United States (or Canada)”.↩︎\nThre can be reasonable explanations for my co-author’s stance. From some discussions I’ve had, it seems that many authors are worried about reviewers querying any change in any number in any table. While I do not understand why “because I updated Compustat” wouldn’t be an adequate response to “why did the coefficients change?” query, I guess many authors put a high priority on triggering as few questions as possible in a far-from-perfect review process. Another reason for this stance is that many researchers have a very manual research process, so changing an input data set means changing many other things, including re-typing the coefficients in the Word document containing the paper or re-exporting the data to Excel to make any plots.↩︎\nWRDS data are not project-specific data sets pretty much by definition.↩︎\nThis will not work if you have a version of db2pq older than 0.2.0, in which case you should reinstall it.↩︎\nI’d guess that such a thing would amount to the equivalent of SQL’s INSERT, UPDATE, and DELETE commands.↩︎\nIf table_name is omitted and schema is specified, then the function will return a data frame with information on the files in the data directory for the schema (if archive is False, as is the default) or in the archive directory (if archive is True).↩︎\nThe information returned by PROC CONTENTS is assumed to be expressed in US Eastern local time (i.e., America/New_York). The PostgreSQL comments generally only indicate a date, and the db2pq assumes that the update occurred at 02:00 US Eastern time.↩︎\nIn addition to pg_restore(), the db2pq package also offers pq_archive() and pq_remove() functions.↩︎\nAn update can always be forced using force=True.↩︎\nOf course, if a user cared about performance with data manipulation, he probably wouldn’t be using Stata to begin with.↩︎\nEach of the two Voyager spacecraft, launched by NASA in 1977, carry the Voyager Golden Record, a gold-plated copper phonograph record intended as a message to any intelligent extraterrestrial lifeforms that might encounter the probes. If we wanted to give data to such lifeforms, I think it would be (quoted) CSV data and written on paper.↩︎\nBecause the data appear to be sorted by bank ID, I should retrieve the observation above, even though I’m only getting 100 rows of data.↩︎\nThe only option that should be used with PostgreSQL is TIMESTAMP WITH TIME ZONE.↩︎\nHere I use keep to limit my download to the fields of interest in this case.↩︎"
  },
  {
    "objectID": "published/data_viz_problem.html",
    "href": "published/data_viz_problem.html",
    "title": "Data visualization challenge",
    "section": "",
    "text": "library(tidyverse)\nThe data we will use in this note are as follows:\ndf &lt;-\n  tribble(\n    ~date, ~cumret, ~is_event,\n    \"2024-01-01\", 1, FALSE,\n    \"2024-01-02\", 1.15, FALSE,\n    \"2024-01-03\", 1.20, TRUE,\n    \"2024-01-04\", 1.06, TRUE,\n    \"2024-01-05\", 1.01, FALSE,\n    \"2024-01-06\", 1.06, FALSE,\n    \"2024-01-07\", 0.95, TRUE,\n    \"2024-01-08\", 0.99, TRUE\n  ) |&gt;\n  mutate(date = as.Date(date))\nThe data in cumret are intended to represent the “cumulative return” of a stock over time. You might think of cumret as a stock price with an initial value of 1. The most common way to represent the cumulative returns over time in a plot uses a line plot (geom_line()), often with points representing discrete observations (e.g., end-of-day stock prices), which we can show using geom_point().\nIn this note, we will discuss the situation where some trading days are “event days” and some are not. A question naturally arises as to how depict the performance of a stock over time in such as case while presenting information about events.\nFigure 1 is an initial attempt at depicting this situation. We have cumulative returns depicted using both lines (geom_line()) and points (geom_point()) and the events are depicted as different colours in the columns (geom_col()).\nHowever, Figure 1 has some aesthetic deficiencies or inconsistencies of presentation. First, the column graph represents data shown as points as wide bars that are flat on top. Second, using columns to represent events suggest that the points go in the middle of the events.\nThis second feature is inconsistent with the notion that the points are day-end prices and the events happen during the trading days. Thus a better representation would seem to be to have the bars line up between the points.\nHowever, merely moving the bars horizontally would not address the inconsistency introduced by the first feature. In a sense, the line plot presents an idealized notion of the stock price moving in a linear fashion between two points. It would seem desirable for the representation of the events to follow this notion.\nIn a sense, we want a plot that simply fills the area under the line plot and uses colours to distinguish event periods from non-event periods.\ndf |&gt;\n  arrange(date) |&gt;\n  ggplot(aes(x = date, y = cumret)) +\n  geom_col(aes(fill = is_event)) +\n  geom_line() +\n  geom_point() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nFigure 1: An initial plot\nI argue that Figure 2 is the plot we are looking for. Note that viewing the “events” as occurring between points means that we lose one “column” that we have in Figure 1. Of course, there is inherently a loss in “columns” with the new perspective because the number of lines between points is always one less than the number of consecutive points.\nIf we accept Figure 2 as the goal, our challenge here is, in a nutshell, to transform the data in df into Figure 2. In words, we are looking to “fill” the area under the line with shading that is coloured according to the value of is_event. We want the shading to be continuous with the colours “switching” immediately after the value for is_event changes.\nWarning: Removed 3 rows containing missing values or values outside the scale range\n(`geom_ribbon()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_ribbon()`).\n\n\n\n\n\n\n\n\nFigure 2: Desired plot\nOur first attempt uses the code below, which produces Figure 3. One issue with Figure 3 is that it doesn’t satisfy the requirement that the colours “switch” immediately after the value for is_event changes. Instead, we have the plot ramping up and down for “event” periods between “non-event” periods, rather than going up and down vertically as in Figure 2.\ndf |&gt;\n  arrange(date) |&gt;\n  ggplot(aes(x = date, y = cumret)) +\n  geom_area(mapping = aes(y = if_else(!is_event, cumret, 0),\n                          fill = FALSE)) +\n  geom_area(mapping = aes(y = if_else(is_event, cumret, 0),\n                          fill = TRUE)) +\n  geom_line() +\n  geom_point() +\n  labs(fill = \"is_event\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nFigure 3: First attempt\nOur second attempt uses the code below, which produces Figure 4. While Figure 4 satisfies the requirement that the colours between “switch” immediately after the value for is_event changes, we still have the issue of the plot gradually ramping up and down between dates rather than going up and down vertically.\ndf |&gt;\n  arrange(date) |&gt;\n  mutate(switch = coalesce(is_event != lead(is_event), FALSE)) |&gt;\n  ggplot(aes(x = date, y = cumret)) +\n  geom_area(mapping = aes(y = if_else(!is_event | switch, cumret, 0),\n                          fill = FALSE)) +\n  geom_area(mapping = aes(y = if_else(is_event | switch, cumret, 0),\n                          fill = TRUE)) +\n  geom_line() +\n  geom_point() +\n  labs(fill = \"is_event\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nFigure 4: Second attempt"
  },
  {
    "objectID": "published/data_viz_problem.html#hints",
    "href": "published/data_viz_problem.html#hints",
    "title": "Data visualization challenge",
    "section": "Hints",
    "text": "Hints\n\nThe variable switch as defined in the second attempt could be useful.\nSetting the “false” value in the if_else() calls to NA rather than 0 might help.\nIf you call ? geom_area, you will see discussion of the closely related geom_ribbon() function. This could be useful."
  },
  {
    "objectID": "published/dbplyr-news.html",
    "href": "published/dbplyr-news.html",
    "title": "Responsive open-source software: Two examples from dbplyr",
    "section": "",
    "text": "In this note, I explore some recent changes in the open-source R package dbplyr to illustrate some of the beauty of how open-source software evolves in practice. In particular, I offer two case studies where features requested by users became reality in dbplyr, which may be my favourite R package.\n\n\n\n\n\n\nTip\n\n\n\nIn writing this note, I used the packages listed below.1 At the time of writing, you also need to install the development version of dbplyr, which you can do using the remotes::install_github() command below. This note was written and compiled using Quarto with RStudio, an integrated development environment (IDE) for working with R. The source code for this note is available here and the latest version of this PDF is here.\n\n\n\nlibrary(farr)\nlibrary(dplyr, warn.conflicts = FALSE)\nlibrary(DBI)\n\n\nremotes::install_github(\"tidyverse/dbplyr\", ref = \"main\")"
  },
  {
    "objectID": "published/dbplyr-news.html#sec-retail-data",
    "href": "published/dbplyr-news.html#sec-retail-data",
    "title": "Responsive open-source software: Two examples from dbplyr",
    "section": "3.1 The retail sales data set",
    "text": "3.1 The retail sales data set\nThe first query I will use to illustrate the use of window functions with mutate() comes from Chapter 3 of SQL for Data Analysis, which uses data on retail sales by industry in the United States to explore ideas on time-series analysis.\nI will use DuckDB as my database engine. Creating a DuckDB database requires just one line of code:\n\ndb &lt;- dbConnect(duckdb::duckdb())\n\nI then call get_data() to load the data into the database. I name the table (\"retail_sales\") so that I can refer to it when using SQL.5\n\nretail_sales &lt;- get_data(db, \n                         dir = \"Chapter 3: Time Series Analysis\",\n                         file = \"us_retail_sales.csv\",\n                         name = \"retail_sales\")\n\nThe SQL version of the query provided in SQL for Data Analysis is as follows:\n\nSELECT sales_month,\n  avg(sales) OVER w AS moving_avg,\n  count(sales) OVER w AS records_count\nFROM retail_sales\nWHERE kind_of_business = 'Women''s clothing stores'\nWINDOW w AS (ORDER BY sales_month \n             ROWS BETWEEN 11 PRECEDING AND CURRENT ROW)\nORDER BY sales_month DESC;\n\n\nDisplaying records 1 - 10\n\n\nsales_month\nmoving_avg\nrecords_count\n\n\n\n\n2020-12-01\n2210.500\n12\n\n\n2020-11-01\n2301.917\n12\n\n\n2020-10-01\n2395.583\n12\n\n\n2020-09-01\n2458.583\n12\n\n\n2020-08-01\n2507.417\n12\n\n\n2020-07-01\n2585.667\n12\n\n\n2020-06-01\n2659.667\n12\n\n\n2020-05-01\n2763.417\n12\n\n\n2020-04-01\n2989.083\n12\n\n\n2020-03-01\n3248.167\n12\n\n\n\n\n\nTo do the same using dbplyr, I can just specify .order and .frame in the call to mutate().\n\nmvg_avg &lt;-\n  retail_sales |&gt;\n  filter(kind_of_business == \"Women's clothing stores\") |&gt;\n  mutate(moving_avg = mean(sales, na.rm = TRUE),\n         records_count = n(),\n         .order = sales_month,\n         .frame = c(-11, 0)) |&gt;\n  select(sales_month, moving_avg, records_count) |&gt;\n  arrange(desc(sales_month))\n\nAs can be seen in Table 1, the resulting data set is the same.6\n\nmvg_avg |&gt;\n  collect(n = 10)\n\n\n\nTable 1: Moving average sales for women’s clothing store (first 10 records)\n\n\n\n\n\n\nsales_month\nmoving_avg\nrecords_count\n\n\n\n\n2020-12-01\n2210.500\n12\n\n\n2020-11-01\n2301.917\n12\n\n\n2020-10-01\n2395.583\n12\n\n\n2020-09-01\n2458.583\n12\n\n\n2020-08-01\n2507.417\n12\n\n\n2020-07-01\n2585.667\n12\n\n\n2020-06-01\n2659.667\n12\n\n\n2020-05-01\n2763.417\n12\n\n\n2020-04-01\n2989.083\n12\n\n\n2020-03-01\n3248.167\n12"
  },
  {
    "objectID": "published/dbplyr-news.html#the-legislators-data",
    "href": "published/dbplyr-news.html#the-legislators-data",
    "title": "Responsive open-source software: Two examples from dbplyr",
    "section": "3.2 The legislators data",
    "text": "3.2 The legislators data\nAnother data set I will use to illustrate the use of mutate() is the legislators data set used in Chapter 4 of SQL for Data Analysis to explore cohort analysis. The legislators data set comprises two tables, which I read into my DuckDB database using the following code.\n\nlegislators_terms &lt;- get_data(db, \n                         dir = \"Chapter 4: Cohorts\",\n                         file = \"legislators_terms.csv\",\n                         name = \"legislators_terms\")\n\nlegislators &lt;- get_data(db, \n                         dir = \"Chapter 4: Cohorts\",\n                         file = \"legislators.csv\",\n                         name = \"legislators\")\n\nA third data set used in Chapter 4 of SQL for Data Analysis is the year_ends table, which I construct in R and copy to my DuckDB database using the following code.7\n\nyear_ends &lt;-\n  tibble(date = seq(as.Date('1770-12-31'), \n                    as.Date('2030-12-31'), \n                    by = \"1 year\")) |&gt;\n  copy_to(db, df = _, overwrite = TRUE, name = \"year_ends\")\n\nFinally, I add a minor tweak to original query by adding an enumerated data type that ensures the tables are ordered meaningfully.8\n\nCREATE TYPE band AS ENUM ('1 to 4', '5 to 10', '11 to 20', '21+')\n\nThe following is a modified version of the SQL query found on page 173 of Chapter 4 of SQL for Data Analysis.9 As can be seen, because of how we defined the band data type, it is meaningful to sort by tenure (check what happens if you omit the casting of tenure to type band using ::band).\n\nWITH term_dates AS (\n  SELECT DISTINCT a.id_bioguide, b.date\n  FROM legislators_terms a\n  JOIN year_ends b \n  ON b.date BETWEEN a.term_start AND a.term_end \n    AND b.date &lt;= '2020-01-01'),\n\ncum_term_dates AS (\n  SELECT id_bioguide, date,\n    count(date) OVER w AS cume_years\n  FROM term_dates\n  WINDOW w AS (PARTITION BY id_bioguide \n               ORDER BY date \n               ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)),\n  \ncum_term_bands AS (\n  SELECT date,\n    CASE WHEN cume_years &lt;= 4 THEN '1 to 4'\n         WHEN cume_years &lt;= 10 THEN '5 to 10'\n         WHEN cume_years &lt;= 20 THEN '11 to 20'\n         ELSE '21+' END AS tenure,\n    COUNT(DISTINCT id_bioguide) AS legislators\n  FROM cum_term_dates\n  GROUP BY 1,2)\n  \nSELECT date, tenure::band AS tenure,\n  legislators * 100.0 / sum(legislators) OVER w AS pct_legislators \nFROM cum_term_bands\nWINDOW w AS (partition by date)\nORDER BY date DESC, tenure;\n\n\nDisplaying records 1 - 10\n\n\ndate\ntenure\npct_legislators\n\n\n\n\n2019-12-31\n1 to 4\n29.98138\n\n\n2019-12-31\n5 to 10\n32.02980\n\n\n2019-12-31\n11 to 20\n20.11173\n\n\n2019-12-31\n21+\n17.87710\n\n\n2018-12-31\n1 to 4\n25.60297\n\n\n2018-12-31\n5 to 10\n33.76623\n\n\n2018-12-31\n11 to 20\n21.33581\n\n\n2018-12-31\n21+\n19.29499\n\n\n2017-12-31\n1 to 4\n24.53532\n\n\n2017-12-31\n5 to 10\n34.75836\n\n\n\n\n\nTranslating the query to dbplyr is greatly facilitated by the use of CTEs, as each CTE can be constructed as a separate remote or lazy data frame. Here remote means that the data are in a database. In this case, “remote” does not mean “far away”, but the data could be physically distant as in the case of the dsf data frame examined above. The term “lazy” refers to the fact that the underlying SQL query for the data frame is not executed until we ask it to be evaluated using functions like collect() (to bring the data into R) or compute() (to create a temporary table in the database).\n\nterm_dates &lt;-\n  legislators_terms |&gt;\n  inner_join(year_ends |&gt; filter(date &lt;= '2020-01-01'),\n             join_by(between(y$date, x$term_start, x$term_end))) |&gt;\n  distinct(id_bioguide, date) \n\nHere I use .order, .frame, and .by to get the same window used in the SQL above.\n\ncum_term_dates &lt;-\n  term_dates |&gt;\n  mutate(cume_years = n(),\n         .by = id_bioguide,\n         .order = date,\n         .frame = c(-Inf, 0)) |&gt;\n  select(id_bioguide, date, cume_years) \n\nThe following query aggregates the data into different ranges of tenure. Note that a glitch in n_distinct() in the duckdb package (presumably something that will be fixed soon enough) means that I need to directly call SQL COUNT(DISTINCT id_bioguide) as can be seen in the code below.\n\ncum_term_bands &lt;-\n  cum_term_dates |&gt; \n  mutate(tenure = case_when(cume_years &lt;= 4 ~ '1 to 4',\n                            cume_years &lt;= 10 ~ '5 to 10',\n                            cume_years &lt;= 20 ~ '11 to 20',\n                            TRUE ~ '21+')) |&gt;\n  mutate(tenure = sql(\"tenure::band\")) |&gt;\n  summarize(legislators = sql(\"COUNT(DISTINCT id_bioguide)\"),\n            .by = c(date, tenure))\n\nThe final query pulls everything together and uses a window function to count the number of legislators in the denominator of pct_legislators. As can be seen in Table 2, the resulting data set is the same as that produced by the SQL above.\n\ncum_term_bands |&gt;\n  mutate(sum_legislators = sum(legislators),\n         pct_legislators = legislators * 100.0 / sum_legislators,\n         .by = date) |&gt;\n  select(date, tenure, pct_legislators) |&gt;\n  arrange(desc(date), tenure) |&gt;\n  collect(n = 10)\n\n\n\nTable 2: Percentage of legislators by tenure (first 10 records)\n\n\n\n\n\n\ndate\ntenure\npct_legislators\n\n\n\n\n2019-12-31\n1 to 4\n29.981\n\n\n2019-12-31\n5 to 10\n32.030\n\n\n2019-12-31\n11 to 20\n20.112\n\n\n2019-12-31\n21+\n17.877\n\n\n2018-12-31\n1 to 4\n25.603\n\n\n2018-12-31\n5 to 10\n33.766\n\n\n2018-12-31\n11 to 20\n21.336\n\n\n2018-12-31\n21+\n19.295\n\n\n2017-12-31\n1 to 4\n24.535\n\n\n2017-12-31\n5 to 10\n34.758\n\n\n\n\n\n\n\n\nAgain, the new functionality supports powerful analyses with clean, easy-to-read code."
  },
  {
    "objectID": "published/dbplyr-news.html#footnotes",
    "href": "published/dbplyr-news.html#footnotes",
    "title": "Responsive open-source software: Two examples from dbplyr",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nRun install.packages(c(\"farr\", \"dplyr\", \"DBI\", \"duckdb\")) within R to install all the packages you need to run the code in this note.↩︎\nThe “go-ahead” seems to be implied by Hadley’s reopening of the issue on 24 June 2021.↩︎\nSee Chapter 17 of Empirical Research in Accounting: Tools and Methods for more on the michels_2017 data. Of course, a more careful approach would probably use trading days before and after events; see Chapter 12 of Empirical Research in Accounting: Tools and Methods for discussion of how to do this.↩︎\nI have written about this book here.↩︎\nIt is not necessary to specify a table name if we are just using dbplyr to analyse the data.↩︎\nThis makes sense as the dplyr/dbplyr code is translated into SQL behind the scenes.↩︎\nIn Chapter 4 of SQL for Data Analysis, year_ends is created using SQL in the relevant dialect, which is PostgreSQL in the book.↩︎\nThis data type is similar to factors in R, a topic covered in Chapter 2 of Empirical Research in Accounting: Tools and Methods.↩︎\nApart from formatting changes, the main modification I made to the query was the use of common-table expressions (CTEs) in place of subqueries. I discuss the merits of CTEs (and of using dbplyr to write SQL) here.↩︎"
  },
  {
    "objectID": "published/dsf_polars.html",
    "href": "published/dsf_polars.html",
    "title": "The best of both worlds: Using modern data frame libraries to create pandas data",
    "section": "",
    "text": "A strong point of pandas is its expressiveness. Its API allows users to explore data using succinct and (generally) intuitive code. However, some of this expressiveness relies on data being in forms (for example, with dates ready to serve as an index) that often differ from the data we have, and pandas can struggle to manipulate the data into those forms, especially with larger data sets.\nA number of modern data frame libraries have emerged that address weaknesses of pandas. In this note, I use polars and Ibis to show how one can use these libraries to get the data into a form in which pandas can shine.\nWhile the underlying data would occupy over 10 GB in memory, the polars variant below runs in about half a second. This approach may have particular appeal to pandas experts because the code is likely more familiar to experienced analysts of data frames.\nI consider two Ibis alternatives. The first uses the same underlying parquet files used in the polars variant, but perhaps takes even less time than polars does. The second tweaks just a small portion of the earlier Ibis code to source the underlying data directly from the WRDS PostgreSQL database. Even so, it takes less than a second to run.\nSo we get the best of both worlds. We can use modern, lazy libraries to perform the heavy data manipulation, and then hand off a compact result to pandas for exploration and visualization.\n\n\n\n\n\n\nTip\n\n\n\nThis note uses several Python packages and parts of it rely on the existence of a local data repository of parquet files for WRDS data of the kind described in Appendix E of Empirical Research in Accounting: Tools and Methods. The following command (run in the terminal on your computer) installs the packages you need.\n\npip install 'ibis-framework[duckdb, postgres]' pandas polars db2pq\n\nThe code assumes you have set the environment variables DATA_DIR and WRDS_ID to point to the location of the parquet repository on your computer and your WRDS ID, respectively.\nThe necessary files for the parquet repository for this note can be created using the db2pq package using the following code. Note that wrds_update_pq(\"dsf\", \"crsp\") will take about 12 minutes with a fast connection to WRDS, but only runs if the file on your computer is not current with the data on the WRDS server.\n\nfrom db2pq import wrds_update_pq\n\nwrds_update_pq(\"dsf\", \"crsp\")  \nwrds_update_pq(\"stocknames\", \"crsp\");\n\ncrsp.dsf already up to date.\ncrsp.stocknames already up to date.\n\n\nThis note was written using Quarto. The source code for this note is available here and the latest version of this PDF is here."
  },
  {
    "objectID": "published/dsf_polars.html#sec-ibis",
    "href": "published/dsf_polars.html#sec-ibis",
    "title": "The best of both worlds: Using modern data frame libraries to create pandas data",
    "section": "3.1 Generating Figure 1 using Ibis and DuckDB",
    "text": "3.1 Generating Figure 1 using Ibis and DuckDB\nAn alternative to using polars would be to use Ibis and its default backend, DuckDB.\nI import Ibis and the _ placeholder, as the latter facilitates more succinct code. I also turn on interactive mode to make it easier to inspect the data in tables if I need to do so.\n\nimport ibis\nfrom ibis import _\nibis.options.interactive = True\n\nI next make a load_parquet_ibis() function that I can use to load data from my parquet repository.\n\ndef load_parquet_ibis(con, table, schema, *, data_dir=None):\n    if data_dir is None:\n        data_dir = Path(os.environ[\"DATA_DIR\"]).expanduser()\n\n    path = data_dir / schema / f\"{table}.parquet\"\n    # register the parquet file as an Ibis table\n    return con.read_parquet(str(path), table_name=f\"{schema}_{table}\")\n\nThe short time taken to “load” the data below suggests that, analogous to the results of pl.scan_parquet(), the tables created here are lazy Ibis expressions rather than materialized data frames.\n\n%%ptime\ncon = ibis.duckdb.connect()\ndsf = load_parquet_ibis(con, \"dsf\", \"crsp\")\nstocknames = load_parquet_ibis(con, \"stocknames\", \"crsp\")\n\nWall time: 67.27 ms\n\n\nA lot of the remaining code is a largely a straightforward translation of the polars code into Ibis equivalents. We start by creating tickers.\n\n%%ptime\nend_date = ibis.literal(data.index.max())\n\ntickers = (\n    stocknames\n    .filter(_.ticker.isin(clean_tickers))\n    .filter((_.namedt &lt;= end_date) & (end_date &lt;= _.nameenddt))\n    .select(\"permno\", \"ticker\")\n)\n\nWall time: 1.15 ms\n\n\nThen the Ibis version of dsf_sub from above.\n\n%%ptime\nstart_date = ibis.literal(data.index.min())\n\nnum_cols = [\"prc\", \"ret\", \"retx\"]\n\ndsf_sub = (\n    dsf\n    .inner_join(tickers, predicates=[dsf.permno == tickers.permno])\n    .filter(_.date.between(start_date, end_date))\n    .select(\"ticker\", \"date\", *num_cols)\n    .mutate(**{c: getattr(_, c).cast(\"float64\") for c in num_cols})\n)\n\nWall time: 1.78 ms\n\n\nNote that I am using some Python tricks here. First, *num_cols unpacks the list num_cols into positional arguments. So, .select(\"ticker\", \"date\", *num_cols) is equivalent to .select(\"ticker\", \"date\", \"prc\", \"ret\", \"retx\").\nSecond, ** unpacks a dictionary into keyword arguments. So .mutate(**{c: getattr(_, c).cast(\"float64\") for c in num_cols}) unpacks into the following:\n\n{\n    \"prc\": _.prc.cast(\"float64\"),\n    \"ret\": _.ret.cast(\"float64\"),\n    \"retx\": _.retx.cast(\"float64\"),\n}\n\nAnd finally, the equivalent of dsf_adj. Here I am using window functions, which are well-supported by DuckDB. Originally, I had attempted to use Ibis with a polars backend, but the polars backend does not support window functions with Ibis. It is not clear to me whether this is an inherent limitation of polars, or is simply a gap in the implementation of the polars backend for Ibis that may be addressed in future versions.\nThe code here seems pretty transparent (once you understand the prc = prc_last * growth / growth_last logic discussed above).\n\n%%ptime\n# window for cumulative calculations (up to current row)\nw = ibis.window(group_by=_.ticker, order_by=_.date)\n\ndef cumprod1p(x):\n    # cumprod(1 + x) = exp(cumsum(log(1 + x)))\n    return (1 + x).ln().cumsum().over(w).exp().cast(\"float64\")\n\ndsf_adj = (\n    dsf_sub\n    .mutate(growth = cumprod1p(_.retx))\n    .mutate(\n        prc_last    = _.prc.last().over(w),\n        growth_last = _.growth.last().over(w),\n    )\n    .mutate(prc = _.prc_last * _.growth / _.growth_last)\n    .drop(\"growth\", \"prc_last\", \"growth_last\")\n)\n\nWall time: 1.83 ms\n\n\nAnd the final step is very similar to what we saw with polars. We simply pivot_wider(), sort by date (important for the pandas index) and then execute() to create a pandas date frame, which we can apply .set_index(\"date\") to just like above. Note that pivot_wider() is translated into a group-by aggregate query in SQL. If we don’t specify values_agg=\"max\", it seems that values_agg=\"first\" by default. This works with DuckDB, which has a first() aggregate, but will not work with PostgreSQL, which does not. Because there should be just one prc for each (date, ticker) combination, the use of max() should not affect the output.\n\n%%ptime\ndata_alt = (\n    dsf_adj\n    .select(\"date\", \"ticker\", \"prc\")  \n    .pivot_wider(\n        names_from=\"ticker\",\n        values_from=\"prc\",\n        values_agg=\"max\", \n    )\n    .order_by(\"date\")\n    .execute()\n    .set_index(\"date\")\n)\n\nWall time: 140.06 ms\n\n\nAs before, we .reindex() to make the plot look better.\n\n%%ptime\ndata_alt = data_alt.reindex(data.index)\n\nWall time: 0.42 ms\n\n\nAnd the resulting plot in Figure 3 suggests that again we have successfully recreated the pandas data frame seen in Hilpisch (2019).\n\ndata_alt.plot(figsize=(8, 8), subplots=True);\n\n\n\n\n\n\n\nFigure 3: Stock prices for five firms: Using CRSP, Ibis and DuckDB"
  },
  {
    "objectID": "published/dsf_polars.html#sec-wrds",
    "href": "published/dsf_polars.html#sec-wrds",
    "title": "The best of both worlds: Using modern data frame libraries to create pandas data",
    "section": "3.2 Generating Figure 1 using Ibis and WRDS PostgreSQL",
    "text": "3.2 Generating Figure 1 using Ibis and WRDS PostgreSQL\nNow that we have Ibis code, it is a trivial matter to replace the DuckDB backend with a PostgreSQL backend using the WRDS PostgreSQL database. I have my WRDS ID in the environment variable WRDS_ID. Only the first chunk of code below is changed from the Ibis-with-DuckDB version in Section 3.1. This first chunk takes a (relatively!) long time to run because Ibis needs to connect to the WRDS database and query it for metadata about the two tables it is using.\nLooking at Figure 4, it seems that this code also effectively reproduces the data frame created by the pandas code in Section 2.\n\n%%ptime\nwrds_id = os.environ['WRDS_ID']\n\ncon = ibis.postgres.connect(\n    host='wrds-pgdata.wharton.upenn.edu',\n    database='wrds',\n    user=wrds_id,\n    port=9737\n)\n\ndsf = con.table('dsf', database='crsp')\nstocknames = con.table('stocknames', database='crsp')\n\nWall time: 960.32 ms\n\n\nThe remaining code is identical to that above for Ibis.\n\n%%ptime\nend_date = ibis.literal(data.index.max())\n\ntickers = (\n    stocknames\n    .filter(_.ticker.isin(clean_tickers))\n    .filter((_.namedt &lt;= end_date) & (end_date &lt;= _.nameenddt))\n    .select(\"permno\", \"ticker\")\n)\n\nWall time: 2.76 ms\n\n\n\n%%ptime\nstart_date = ibis.literal(data.index.min())\n\nnum_cols = [\"prc\", \"ret\", \"retx\"]\n\ndsf_sub = (\n    dsf\n    .inner_join(tickers, predicates=[dsf.permno == tickers.permno])\n    .filter(_.date.between(start_date, end_date))\n    .select(\"ticker\", \"date\", *num_cols)\n    .mutate(**{c: getattr(_, c).cast(\"float64\") for c in num_cols})\n)\n\nWall time: 4.20 ms\n\n\n\n%%ptime\n# window for cumulative calculations (up to current row)\nw = ibis.window(group_by=_.ticker, order_by=_.date)\n\ndef cumprod1p(x):\n    # cumprod(1 + x) = exp(cumsum(log(1 + x)))\n    return (1 + x).ln().cumsum().over(w).exp().cast(\"float64\")\n\ndsf_adj = (\n    dsf_sub\n    .mutate(growth = cumprod1p(_.retx))\n    .mutate(\n        prc_last    = _.prc.last().over(w),\n        growth_last = _.growth.last().over(w),\n    )\n    .mutate(prc = _.prc_last * _.growth / _.growth_last)\n    .drop(\"growth\", \"prc_last\", \"growth_last\")\n)\n\nWall time: 4.34 ms\n\n\nThe next chunk takes a little longer than the same code chunk took in Section 3.1, because the data need to be transported from the WRDS PostgreSQL server to my computer.\n\n%%ptime\ndata_alt = (\n    dsf_adj\n    .select(\"date\", \"ticker\", \"prc\")  \n    .pivot_wider(\n        names_from=\"ticker\",\n        values_from=\"prc\",\n        values_agg=\"max\",   \n    )\n    .order_by(\"date\")\n    .execute()\n    .set_index(\"date\")\n)\n\nWall time: 338.39 ms\n\n\n\n%%ptime\ndata_alt = data_alt.reindex(data.index)\n\nWall time: 1.23 ms\n\n\n\ndata_alt.plot(figsize=(8, 8), subplots=True);\n\n\n\n\n\n\n\nFigure 4: Stock prices for five firms: Using CRSP, Ibis and PostgreSQL"
  },
  {
    "objectID": "published/dsf_polars.html#footnotes",
    "href": "published/dsf_polars.html#footnotes",
    "title": "The best of both worlds: Using modern data frame libraries to create pandas data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThough in Section 3.2, I effectively run SQL against the WRDS PostgreSQL database, but using the Ibis library to generate the SQL.↩︎\nThis was on an M4 Pro Mac mini with 24 GB of RAM.↩︎\nThe term “framework libraries” is one I made up. If you go to the Ibis website https://ibis-project.org, Ibis is described as “an open source dataframe library that works with any data system”. But Ibis relies on some other system to provide the execution engine, which it calls the backend. Most data frame libraries provide their own execution engines.↩︎\nNote, some of these data frame libraries are not limited to Python. For example, the polars documentation states that polars “is written in Rust, and available for Python, R and NodeJS [sic].” https://docs.pola.rs↩︎\nNote that I use a little custom “cell magic” %%ptime to produce execution times in a way that prints nicely in the PDF output. See the source code for this document for details.↩︎\nSee the source code for details.↩︎"
  },
  {
    "objectID": "published/g_secd.html",
    "href": "published/g_secd.html",
    "title": "Some benchmarks with comp.g_secd",
    "section": "",
    "text": "In this note, I use a simple query to benchmark performance of various approaches to accessing data in comp.g_secd. Many international researchers will know that comp.g_secd holds daily security-level data for non-US firms covered by Compustat Global. It is the global version of the North America daily security file, comp.secd, and, I guess, the closest analogue of crsp.dsf for finance and accounting researchers studying non-US firms.\nI use this data set to do some benchmarking. I find that a query using comp.g_secd that takes 6 minutes using SAS on the WRDS servers, takes about 1 minute using the WRDS PostgreSQL server and about 0.2 seconds using a local Parquet file. The Parquet file occupies less than 4 GB on my hard drive, which compares with about 145 GB for the SAS file on the WRDS server. While creating the Parquet file takes 45 minutes, this may be a reasonable trade-off for a researcher who is analysing comp.g_secd frequently and does not need the very latest iteration of comp.g_secd for research purposes."
  },
  {
    "objectID": "published/g_secd.html#the-benchmark-query",
    "href": "published/g_secd.html#the-benchmark-query",
    "title": "Some benchmarks with comp.g_secd",
    "section": "2.1 The benchmark query",
    "text": "2.1 The benchmark query\nNow that we understand a little about comp.g_secd and its SAS manifestation, I will introduce my benchmark query. The idea of the benchmark is that it is somewhat representative of the kinds of things a researcher might want to do with comp.g_secd without being overly complicated. Additionally, the benchmark should require actually looking at a significant part of the data (in this case all records) and, to keep it interesting, it should not be able to use a short cut of simply looking at an index.11\nThe SAS version of my benchmark query simply counts the number of rows associated with each value of curcdd, which Listing 2 tells us represents “ISO Currency Code - Daily”:\nproc sql;\n    CREATE TABLE curcdd_counts AS\n    SELECT curcdd, count(*) AS n\n    FROM comp.g_secd\n    GROUP BY curcdd\n    ORDER BY n DESC;\nquit;\n\nproc print data=curcdd_counts;\nrun;\nI put this SAS code into a file g_secd.sas in my home directory on the WRDS server and ran qsas g_secd.sas. Inspecting g_secd.log a few minutes later, I see The SAS System used: real time 6:08.66. So SAS needs more than 5 minutes to run this query. And here is a sample of the output in g_secd.lst:\n                                 The SAS System                                1\n                                        Wednesday, February 18, 2026 01:55:00 PM\n\n                           Obs    curcdd           n\n\n                             1     EUR      48130467\n                             2     JPY      31841749\n                             3     CNY      25687260\n                             4     INR      20676444\n                             5     GBP      20636852\n                             6     KRW      15657569\n                             7     HKD      14717583\n                             8     AUD      13812258"
  },
  {
    "objectID": "published/g_secd.html#footnotes",
    "href": "published/g_secd.html#footnotes",
    "title": "Some benchmarks with comp.g_secd",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nExecute install.packages(c(\"tidyverse\", \"DBI\", \"duckdb\", \"dbplyr\", \"farr\", \"RPostgres\") within R to install all the packages you need to run the code in this note.↩︎\nMy original code was in Perl, but by 2015 I had Python code to do the same job, and by 2019 there was an installable Python package, created with the help of Jingyu Zhang. Early on, I considered SQLite and MySQL as well as PostgreSQL, but I decided on PostgreSQL because of its rich type system, powerful SQL, and support for server programming (though I ended up not using the last one much). Though I had zero expertise, it seems that I somehow made the right choice. I was also an early adopter of RStudio in 2010 or 2011.↩︎\nThe only recent change was I recently exposed the (previously internal) sas_to_pandas() function that I used in my recent note on SAS and pandas.↩︎\nTrying to do Gow and Ding (2024) using pandas and SQL would be painful and slow.↩︎\nSee the Apache Arrow website at https://arrow.apache.org/overview/.↩︎\nA lot of early work involved using poorly documented libraries to keep the memory impact to a minimum and to facilitate the wrds_update_pq() conditional-update functionality.↩︎\nNote that the “using R” part is relatively unimportant. For example, it would be easy to do everything with essentially identical performance using Python.↩︎\nIf you look at Listing 1, you may notice “135GB”; this is incorrect and should be “135 GiB” (a GiB is a “binary” unit in which 1 GiB = \\(2^{30}\\) = 1,073,741,824 bytes).↩︎\nNote that the following Python code use the environment variable WRDS_ID. Call proc_contents(\"g_secd\", \"comp\", wrds_id=\"your_wrds_id\") if you don’t have this set.↩︎\nFor example, analogues of funda_mod as described in Chapter 6 of Gow and Ding (2024).↩︎\nWhether queries can use such shortcuts obviously depends on the specifics of the available indexes (in this case, there is no index to use for curcdd), but also on whether the backend system will use them for the query.↩︎\nNote that system_time() comes from the farr package.↩︎\nNote that the PostgreSQL server, like SAS, also has one or more indexes, which occupy an additional 15.75 GB.↩︎\nA recent update to db2pq means that the “last updated” string comes from the comments appended to the PostgreSQL table.↩︎"
  },
  {
    "objectID": "published/gino-colada.html",
    "href": "published/gino-colada.html",
    "title": "The Gino-Colada Affair",
    "section": "",
    "text": "We can reproduce the equivalent of the published results of an ANOVA with the three conditions as categorical predictor variable and deductions as outcome variable using linear regression. Results are reported in column (1) of Table 1. In addition, the original article reported that each difference between the experimental “signature-on-top” and the two control conditions (“signature-on-bottom”, “no signature”) was significant. This is confirmed in columns (2) and (3) of Table 1.\nNext, we can repeat the analysis without rows 67 to 72. Results are reported in Table 2. Without the six contested cases, the results are no longer statistically significant, \\(F( 2, 92) = 2.96\\), \\(p = 0.057\\), as seen in column (1) of Table 2. The comparisons of the experimental group with the two control groups were also statistically significant (see columns (2) and (3) of Table 2). Combining the two control groups into one and comparing it to the experimental group and presenting the results as a planned contrast would also have produced a significant result (see column (4) of Table 2).\nOf course, the accusation is that she switched rows with low values to the experimental condition and rows with high values to the control condition. To attempt to reverse this manipulation, we can recode the contested rows 67–69 as signature-at-the-bottom and 70–72 as signature-at-the-top and repeat the analysis. In this case, there was no evidence that the group means differed from each other, \\(F( 2, 98) = 0.454\\), \\(p = 0.637\\). Results are presented in Column (1) of Table 3. Neither comparison of the experimental group with each of the two control groups was statistically significant (see columns (2) and (3) of Table 3).\n\n\n\n\nTable 1: Reproduction of results from the paper\n\n\n\n\n\n    \n\n    \n\n    \n    \n    \n      \n        \n        \n              \n                 \n                (1)\n                (2)\n                (3)\n              \n        \n        \n        \n                \n                  (Intercept)\n                  8.445\n                  5.271\n                  8.445\n                \n                \n                  \n                  (0.966)\n                  (0.906)\n                  (0.906)\n                \n                \n                  Cond1\n                  -3.174\n                  \n                  -3.174\n                \n                \n                  \n                  (1.346)\n                  \n                  (1.263)\n                \n                \n                  Cond2\n                  1.179\n                  4.353\n                  \n                \n                \n                  \n                  (1.366)\n                  (1.300)\n                  \n                \n                \n                  F\n                  5.633\n                  11.203\n                  6.312\n                \n                \n                  p\n                  0.005\n                  0.001\n                  0.014\n                \n        \n      \n    \n\n\n\n\n\n\n\n\n\n\nTable 2: Reproduction of results without disputed observations\n\n\n\n\n\n    \n\n    \n\n    \n    \n    \n      \n        \n        \n              \n                 \n                (1)\n                (2)\n                (3)\n                (4)\n              \n        \n        \n        \n                \n                  (Intercept)\n                  8.445\n                  5.703\n                  8.445\n                  8.417\n                \n                \n                  \n                  (0.895)\n                  (0.827)\n                  (0.909)\n                  (0.645)\n                \n                \n                  Cond1\n                  -2.742\n                  \n                  -2.742\n                  \n                \n                \n                  \n                  (1.276)\n                  \n                  (1.296)\n                  \n                \n                \n                  Cond2\n                  -0.059\n                  2.684\n                  \n                  \n                \n                \n                  \n                  (1.297)\n                  (1.189)\n                  \n                  \n                \n                \n                  Cond == 1TRUE\n                  \n                  \n                  \n                  -2.714\n                \n                \n                  \n                  \n                  \n                  \n                  (1.110)\n                \n                \n                  F\n                  2.956\n                  5.098\n                  4.478\n                  5.975\n                \n                \n                  p\n                  0.057\n                  0.028\n                  0.038\n                  0.016\n                \n        \n      \n    \n\n\n\n\n\n\n\n\n\n\nTable 3: Reproduction of results with corrected data\n\n\n\n\n\n    \n\n    \n\n    \n    \n    \n      \n        \n        \n              \n                 \n                (1)\n                (2)\n                (3)\n              \n        \n        \n        \n                \n                  (Intercept)\n                  8.445\n                  7.100\n                  8.445\n                \n                \n                  \n                  (1.015)\n                  (0.979)\n                  (1.062)\n                \n                \n                  Cond1\n                  -1.345\n                  \n                  -1.345\n                \n                \n                  \n                  (1.415)\n                  \n                  (1.480)\n                \n                \n                  Cond2\n                  -0.761\n                  0.585\n                  \n                \n                \n                  \n                  (1.436)\n                  (1.405)\n                  \n                \n                \n                  F\n                  0.454\n                  0.173\n                  0.826\n                \n                \n                  p\n                  0.637\n                  0.679\n                  0.367"
  },
  {
    "objectID": "published/gino-colada.html#study-1",
    "href": "published/gino-colada.html#study-1",
    "title": "The Gino-Colada Affair",
    "section": "",
    "text": "We can reproduce the equivalent of the published results of an ANOVA with the three conditions as categorical predictor variable and deductions as outcome variable using linear regression. Results are reported in column (1) of Table 1. In addition, the original article reported that each difference between the experimental “signature-on-top” and the two control conditions (“signature-on-bottom”, “no signature”) was significant. This is confirmed in columns (2) and (3) of Table 1.\nNext, we can repeat the analysis without rows 67 to 72. Results are reported in Table 2. Without the six contested cases, the results are no longer statistically significant, \\(F( 2, 92) = 2.96\\), \\(p = 0.057\\), as seen in column (1) of Table 2. The comparisons of the experimental group with the two control groups were also statistically significant (see columns (2) and (3) of Table 2). Combining the two control groups into one and comparing it to the experimental group and presenting the results as a planned contrast would also have produced a significant result (see column (4) of Table 2).\nOf course, the accusation is that she switched rows with low values to the experimental condition and rows with high values to the control condition. To attempt to reverse this manipulation, we can recode the contested rows 67–69 as signature-at-the-bottom and 70–72 as signature-at-the-top and repeat the analysis. In this case, there was no evidence that the group means differed from each other, \\(F( 2, 98) = 0.454\\), \\(p = 0.637\\). Results are presented in Column (1) of Table 3. Neither comparison of the experimental group with each of the two control groups was statistically significant (see columns (2) and (3) of Table 3).\n\n\n\n\nTable 1: Reproduction of results from the paper\n\n\n\n\n\n    \n\n    \n\n    \n    \n    \n      \n        \n        \n              \n                 \n                (1)\n                (2)\n                (3)\n              \n        \n        \n        \n                \n                  (Intercept)\n                  8.445\n                  5.271\n                  8.445\n                \n                \n                  \n                  (0.966)\n                  (0.906)\n                  (0.906)\n                \n                \n                  Cond1\n                  -3.174\n                  \n                  -3.174\n                \n                \n                  \n                  (1.346)\n                  \n                  (1.263)\n                \n                \n                  Cond2\n                  1.179\n                  4.353\n                  \n                \n                \n                  \n                  (1.366)\n                  (1.300)\n                  \n                \n                \n                  F\n                  5.633\n                  11.203\n                  6.312\n                \n                \n                  p\n                  0.005\n                  0.001\n                  0.014\n                \n        \n      \n    \n\n\n\n\n\n\n\n\n\n\nTable 2: Reproduction of results without disputed observations\n\n\n\n\n\n    \n\n    \n\n    \n    \n    \n      \n        \n        \n              \n                 \n                (1)\n                (2)\n                (3)\n                (4)\n              \n        \n        \n        \n                \n                  (Intercept)\n                  8.445\n                  5.703\n                  8.445\n                  8.417\n                \n                \n                  \n                  (0.895)\n                  (0.827)\n                  (0.909)\n                  (0.645)\n                \n                \n                  Cond1\n                  -2.742\n                  \n                  -2.742\n                  \n                \n                \n                  \n                  (1.276)\n                  \n                  (1.296)\n                  \n                \n                \n                  Cond2\n                  -0.059\n                  2.684\n                  \n                  \n                \n                \n                  \n                  (1.297)\n                  (1.189)\n                  \n                  \n                \n                \n                  Cond == 1TRUE\n                  \n                  \n                  \n                  -2.714\n                \n                \n                  \n                  \n                  \n                  \n                  (1.110)\n                \n                \n                  F\n                  2.956\n                  5.098\n                  4.478\n                  5.975\n                \n                \n                  p\n                  0.057\n                  0.028\n                  0.038\n                  0.016\n                \n        \n      \n    \n\n\n\n\n\n\n\n\n\n\nTable 3: Reproduction of results with corrected data\n\n\n\n\n\n    \n\n    \n\n    \n    \n    \n      \n        \n        \n              \n                 \n                (1)\n                (2)\n                (3)\n              \n        \n        \n        \n                \n                  (Intercept)\n                  8.445\n                  7.100\n                  8.445\n                \n                \n                  \n                  (1.015)\n                  (0.979)\n                  (1.062)\n                \n                \n                  Cond1\n                  -1.345\n                  \n                  -1.345\n                \n                \n                  \n                  (1.415)\n                  \n                  (1.480)\n                \n                \n                  Cond2\n                  -0.761\n                  0.585\n                  \n                \n                \n                  \n                  (1.436)\n                  (1.405)\n                  \n                \n                \n                  F\n                  0.454\n                  0.173\n                  0.826\n                \n                \n                  p\n                  0.637\n                  0.679\n                  0.367"
  },
  {
    "objectID": "published/gino-colada.html#study-2",
    "href": "published/gino-colada.html#study-2",
    "title": "The Gino-Colada Affair",
    "section": "2 Study #2",
    "text": "2 Study #2\n\n\n\n\nTable 4: Study 2: Reproduction of results with OSF data\n\n\n\n\n\n    \n\n    \n\n    \n    \n    \n      \n        \n        \n              \n                 \n                Cheating\n                SumDeductions\n                SumEthicsWords\n              \n        \n        \n        \n                \n                  (Intercept)\n                  3.567\n                  7.063\n                  0.867\n                \n                \n                  \n                  (5.491)\n                  (7.266)\n                  (4.720)\n                \n                \n                  SignAtTop\n                  -1.900\n                  -3.830\n                  0.533\n                \n                \n                  \n                  (-2.068)\n                  (-2.786)\n                  (2.054)\n                \n                \n                  F\n                  4.279\n                  7.761\n                  4.218\n                \n                \n                  p\n                  0.043\n                  0.007\n                  0.045\n                \n        \n      \n    \n\n\n\n\n\n\nThe original results from the paper are reported in Table 4.\nFrom the calcChain.xml file, it appears that just three observations (P# values 1, 59, 61) have been moved “out of order” from “sign at the bottom” to “sign at the top”. These observations are (now) in rows 2, 60, and 61 of the OSF spreadsheet. It seems these changes involved moving a row from the bottom to the top and two rows from the top to the bottom.\n&lt;c r=\"I58\" i=\"1\"/&gt;\n&lt;c r=\"K58\" i=\"1\"/&gt;\n&lt;c r=\"I59\" i=\"1\"/&gt;\n&lt;c r=\"K59\" i=\"1\"/&gt;\n&lt;c r=\"I2\" i=\"1\"/&gt;\n&lt;c r=\"K2\" i=\"1\"/&gt;\nand\n&lt;c r=\"I60\" i=\"1\"/&gt;\n&lt;c r=\"K60\" i=\"1\"/&gt;\n&lt;c r=\"I61\" i=\"1\"/&gt;\n&lt;c r=\"K61\" i=\"1\"/&gt;\n&lt;c r=\"I3\" i=\"1\"/&gt;\n&lt;c r=\"K3\" i=\"1\"/&gt;\n&lt;c r=\"I4\" i=\"1\"/&gt;\nLet’s see what happens if we move it back? Results are reported in Table 5.\n\n\n\n\nTable 5: Study 2: Reproduction of results with corrected data\n\n\n\n\n\n    \n\n    \n\n    \n    \n    \n      \n        \n        \n              \n                 \n                Cheating\n                SumDeductions\n                SumEthicsWords\n              \n        \n        \n        \n                \n                  (Intercept)\n                  3.375\n                  6.950\n                  0.906\n                \n                \n                  \n                  (5.314)\n                  (7.390)\n                  (5.066)\n                \n                \n                  SignAtTop\n                  -1.625\n                  -3.861\n                  0.487\n                \n                \n                  \n                  (-1.748)\n                  (-2.804)\n                  (1.858)\n                \n                \n                  F\n                  3.055\n                  7.864\n                  3.453\n                \n                \n                  p\n                  0.086\n                  0.007\n                  0.068"
  },
  {
    "objectID": "published/oxon_weather.html",
    "href": "published/oxon_weather.html",
    "title": "Defining winter and summer in Oxford",
    "section": "",
    "text": "Figure 1: Average daily temperatures for 91 days following indicated date for period 2001–2024\nIn the United States, one often hears people speak of the “official” start of seasons. Ironically, there seems to be nothing that is official about these dates. However, there is consensus about the dates in the US. The “official” start of summer is the summer solstice (for 2024: 20 June in Oxford and Boston) and the “official” start of winter is the winter solstice (for 2024: 21 December in Oxford and Boston).1\nIn Australia, the usual convention is to divide seasons by months. On this basis, winter starts on 1 June and summer starts on 1 December.2\nIs there a sense in which one approach is more correct than the other? Focusing on summer and winter, one definition for these seasons would be that winter starts on the first day of the 91-day period that is the coldest such period for a year averaged over a number of years. Similarly, summer should start on the first day of the 91-day period that is the hottest such period for a year averaged over a number of years.\nWe answer this question focusing on Oxford, England (latitude of 51.75222, longitude: -1.25596).\nDaily temperature data from Open-Meteo comprise a maximum and minimum temperature. So immediately we have two possible definitions of each season according to the temperature we use (e.g., summer could be the 91-day period that has the highest average minimum temperature or it could be the period that has the highest average maximum temperature). Here we consider both.\nThe start of winter based on the 91-day period with the lowest average maximum temperature is 29 November. The start of winter based on the 91-day period with the lowest average minimum temperature is 08 December.\nThe start of summer based on the 91-day period with the highest average maximum temperature is 14 June. The start of summer based on the 91-day period with the highest average minimum temperature is 16 June. So using maximums, we get close to the Australian convention for winter and close to the US convention for summer.\nInterestingly, it seems that using average maximums for summer and winter gets closest to the current approach in Australia. However, even using these we have the issue that spring begins on 28 February and autumn begins on 13 September. This implies a spring of 106 days and an autumn of 77 days."
  },
  {
    "objectID": "published/oxon_weather.html#footnotes",
    "href": "published/oxon_weather.html#footnotes",
    "title": "Defining winter and summer in Oxford",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSeasons reckoned in this way are known as astronomical seasons. See here.↩︎\nSeasons reckoned in this way are known as meteorological seasons. See here.↩︎"
  },
  {
    "objectID": "published/retail-sales.html",
    "href": "published/retail-sales.html",
    "title": "Retail sales",
    "section": "",
    "text": "Tip 1\n\n\n\nThe code in this chapter uses the packages listed below. For instructions on how to set up your computer to use the code found in this book, see [TBD].1 Quarto templates for the exercises below are available on GitHub.2\nlibrary(tidyverse)\nlibrary(modelsummary)\nlibrary(dbplyr)\nlibrary(DBI)\nThe data we focus on in this chapter come from the Australian Bureau of Statistics (ABS). The ABS describes itself as “Australia’s national statistical agency and an official source of independent, reliable information.” We discuss the source of the data in more detail in Section 2.3.5.\nThe data are stored as a comma-separated value (CSV) file format in data/retail_sales.csv in the repository you downloaded for this course.3 There is a good chance that, if Microsoft Excel is installed on your computer, double-clicking on this file on your computer will open it in Excel. To understand the contents of the file, it’s actually better to open it in a text editor, such as Notepad (Windows) or TextEdit (MacOS). CSV files are a common format for sharing data, as it is a simple format that many software packages can handle. Chapter 7 of R for Data Science discusses CSVs in more detail.\nHere we use read_csv() from the Tidyverse to load the data into R.\nretail_sales &lt;- read_csv(\"data/retail_sales.csv\")\n\nRows: 96957 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (3): industry, state, parent_industry\ndbl  (2): sales, ind_level\ndate (1): date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nAs can be seen in the output above, read_csv() will attempt to infer a number of features of the data. For example, the first row of the CSV file is assumed to provide the names the columns of the resulting data frame:\nAdditionally, read_csv() will guess the appropriate type for each column. From the output above, we can see that industry, state, and parent_industry are imported as character columns, that date has type date, and that sales and ind_level have type dbl (“double” being the R type for storing real numbers).\nThe data in retail_sales are monthly estimates of turnover [sales in dollars] and volumes for retail businesses, including store and online sales, by industry group and industry subgroup. According to the ABS, “estimates of turnover are compiled from the monthly Retail Business Survey. About 700 ‘large’ businesses are included in the survey every month, while a sample of about 2,700 ‘smaller’ businesses is selected.”\nWe can take a peek at the first few rows of data by typing retail_sales in the R console:\nretail_sales\n\n# A tibble: 96,957 × 6\n   industry                     state date       sales ind_level parent_industry\n   &lt;chr&gt;                        &lt;chr&gt; &lt;date&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;          \n 1 Cafes, restaurants and cate… Aust… 1982-04-01   4.4         2 Cafes, restaur…\n 2 Cafes, restaurants and cate… New … 1982-04-01  61.8         2 Cafes, restaur…\n 3 Cafes, restaurants and cate… Nort… 1982-04-01  NA           2 Cafes, restaur…\n 4 Cafes, restaurants and cate… Quee… 1982-04-01  18.7         2 Cafes, restaur…\n 5 Cafes, restaurants and cate… Sout… 1982-04-01  14.3         2 Cafes, restaur…\n 6 Cafes, restaurants and cate… Tasm… 1982-04-01   1.9         2 Cafes, restaur…\n 7 Cafes, restaurants and cate… Tota… 1982-04-01 146.          2 Cafes, restaur…\n 8 Cafes, restaurants and cate… Vict… 1982-04-01  36.4         2 Cafes, restaur…\n 9 Cafes, restaurants and cate… West… 1982-04-01   8           2 Cafes, restaur…\n10 Cafes, restaurants and take… Aust… 1982-04-01   7.6         1 Total (Industr…\n# ℹ 96,947 more rows\nWe can also get a succinct take on the data using summary():\nsummary(retail_sales)\n\n   industry            state                date                sales        \n Length:96957       Length:96957       Min.   :1982-04-01   Min.   :    0.4  \n Class :character   Class :character   1st Qu.:1992-12-01   1st Qu.:   27.5  \n Mode  :character   Mode  :character   Median :2003-08-01   Median :  107.8  \n                                       Mean   :2003-08-01   Mean   :  533.4  \n                                       3rd Qu.:2014-04-01   3rd Qu.:  371.1  \n                                       Max.   :2024-12-01   Max.   :45943.1  \n                                                            NA's   :6970     \n   ind_level     parent_industry   \n Min.   :0.000   Length:96957      \n 1st Qu.:1.000   Class :character  \n Median :2.000   Mode  :character  \n Mean   :1.619                     \n 3rd Qu.:2.000                     \n Max.   :2.000"
  },
  {
    "objectID": "published/retail-sales.html#exercises",
    "href": "published/retail-sales.html#exercises",
    "title": "Retail sales",
    "section": "1.1 Exercises",
    "text": "1.1 Exercises\n\nDescribe in words what the query above using anti_join() is doing.\nWhat do functions like inner_join() and union_all() have in common? How do they differ?\nUsing the documentation provided for them, explain how union_all() and union() differ. Would you expect different results if we had used union() instead of union_all() in the code above?\nSuppose we wanted to focus our analysis on industry subgroups, but wanted to retain “Department stores” as part of this. Write code to create ind_sub_groups, a data set comprising data from retail_sales that we could use for this purpose.\nIn creating Table 1, I used left_join() and then coalesce(). Why did I need to use left_join() rather than inner_join(). (Hint: Replace left_join() with inner_join() and see what happens.)\nWhat industry group has missing values at level 2 for Queensland, as seen in Table 2? Which subgroups are affected?\nProduce a version of Table 1 but focused on Queensland instead of all of Australia (Total (State)). Do you see an issue related to the issue flagged in the previous question?\nProduce a version of Table 1 but focused on Tasmania instead of all of Australia (Total (State)). Do you see an issue related to the issue flagged in the previous question?"
  },
  {
    "objectID": "published/retail-sales.html#plotting-panel-data",
    "href": "published/retail-sales.html#plotting-panel-data",
    "title": "Retail sales",
    "section": "2.1 Plotting panel data",
    "text": "2.1 Plotting panel data\nThe retail_sales data set is a panel data set with monthly data on retail sales for several “states” and several industries.7 For a given state and industry, the monthly retail sales series is a time-series. For an initial exploration of retail_sales, I construct df_group_plot, a data set focused on a subset of the available dates and the data for all states combined (state == \"Total (State)\") and the industry groups (ind_level == 1).\n\nstart_date &lt;- as.Date(\"2010-01-01\")\nend_date &lt;- as.Date(\"2024-07-01\")\n\ndf_group_plot &lt;-\n  retail_sales |&gt;\n  left_join(short_names, by = \"industry\") |&gt;\n  mutate(industry = coalesce(short_ind, industry)) |&gt;\n  filter(state == \"Total (State)\", ind_level == 1,\n         between(date, start_date, end_date))\n\nFigures 1 and 2 plot sales from January 2010 until July 2024 by industry group. While Figures 1 and 2 use the same underlying data, they present the information in two different ways.\n\ndf_group_plot |&gt;\n  ggplot(aes(x = date, y = sales / 1e3, colour = industry)) +\n  ylab(\"Sales ($ billion)\") +\n  xlab(\"Month\") +\n  geom_line() +\n  facet_wrap(industry ~ ., ncol = 2, scales = \"free_y\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nFigure 1: Retail sales by industry using facet_wrap()\n\n\n\n\n\n\ndf_group_plot |&gt;\n  ggplot(aes(x = date, y = sales / 1e3, colour = industry)) +\n  ylab(\"Sales ($ billion)\") +\n  xlab(\"Month\") +\n  geom_line() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nFigure 2: Retail sales by industry\n\n\n\n\n\n\n2.1.1 Exercises\n\nIdentify three patterns in the data observed in Figures 1 and 2. For each pattern, provide a conjecture for what explains the pattern.\nDescribe in words how Figure 1 differs from Figure 2.\nDescribe in words how the code used to create Figure 1 was modified to create Figure 2.\nIn your view, which plot—Figure 1 or Figure 2—best presents the information? Is one better than the other for identifying the patterns you identified in the previous question?"
  },
  {
    "objectID": "published/retail-sales.html#seasonality",
    "href": "published/retail-sales.html#seasonality",
    "title": "Retail sales",
    "section": "2.2 Seasonality",
    "text": "2.2 Seasonality\nOne thing you likely noticed about Figures 1 and 2 is that retail sales appear to be highly seasonal. Seasonality refers to the tendency of some time-series to exhibit recurring patterns over time. For example, ice cream sales tend to be higher in summer, while sales of ski boots probably peak in winter months. To see how we can better understand seasonality, consider Figure 3, which plots sales of Cafes, restaurants and takeaway food services for all states combined in a way that allows us to consider each year separately.\n\nretail_sales |&gt;\n  filter(industry == \"Cafes, restaurants and takeaway food services\",\n         state == \"Total (State)\",\n         between(date, start_date, end_date)) |&gt;\n  mutate(month = month(date, label = TRUE),\n         year = year(date)) |&gt;\n  ggplot(aes(x = month, y = sales / 1e3, fill = month)) +\n  ylab(\"Sales ($ billion)\") +\n  xlab(\"Month\") +\n  geom_col() +\n  facet_wrap(year ~ ., ncol = 3) +\n  theme(legend.position = \"none\") +\n  scale_x_discrete(labels = \\(x) str_sub(x, 1, 1))\n\n\n\n\n\n\n\nFigure 3: Seasonality\n\n\n\n\n\n\n2.2.1 Exercises\n\nLooking at Figure 3, which month (if any) generally has the highest sales in each year? Provide a conjecture for the underlying forces causing this month to have the highest sales in most or all years?\nLooking at Figure 3, which month (if any) generally has the lowest sales in each year? Provide a conjecture for the underlying forces causing this month to have the lowest sales in most or all years?\nWhat happens if you omit “, fill = month” from the code creating Figure 3? Does the use of colour add useful information to this plot?\nWhat is the line of code “scale_x_discrete(labels = \\(x) str_sub(x, 1, 1))” doing here? (Hint: Omit this line and compare the plot produced with Figure 3. Also look at the documentation for str_sub().)\nLooking again at Figures 1 and 2, are all industries equally seasonal? Which industry do you consider to be the most seasonal? The least seasonal? How might you measure the seasonality of each industry using data in that plot? What complications do you anticipate in measuring seasonality?\n\n\n\n\n\n\n\nTip 2: Alternative engines\n\n\n\nSo far, we have done many calculations using mutate() and summarize(), combined data frames using functions like inner_join(), or focused on certain rows or columns using filter() or select(). We have generally applied the functions to data in data frames, which we understand to comprise vectors of data stored in the memory (RAM) of our computer. However, as you work more with data, you are likely to encounter situations where this approach has limitations.\nFor example, you might have the data stored on disk, but it either would take too long to load or simply cannot fit into RAM. It turns out that there are packages that provide alternative engines that can help us work with this kind data with little change to how we have done things so far:\n\nOne popular option is data.table, a package that provides a “high-performance version of base R’s data.frame with syntax and feature enhancements for ease of use, convenience and programming speed.”\nAnother option is Apache Arrow, “a multi-language toolbox designed for efficient analysis and transport of large datasets.” The arrow R package provides a dplyr backend allowing users to analyze larger-than-memory datasets using familiar syntax.\nAnother possibility is that the data you want to work with are stored on a remote computer, such as a database server or a Spark cluster. If the data or interest are stored in a database, the DBI package provides an interface allowing users to connect to most popular databases, execute SQL, and retrieve results.\n\nThe dbplyr package allows users to create remote data frames and execute queries using a familiar interface often identical to that offered by dplyr. Using a remote database server offers a number of potential advantages:\n\nMoving processing to where the data are located\nMoving processing to a more powerful computer\nAvoiding loading of data into RAM\n\nIn this book, in addition to using dplyr with data frames, we explore the use of DuckDB as an analytical engine. DuckDB provides an SQL interface, so working with it is much like working with PostgreSQL or MySQL but, unlike most database server products, DuckDB requires almost no setup and offers significant performance benefits. While we use DuckDB largely for these performance benefits, we also get the happy side-effect of learning how to work with remote databases.\nAnother advantage offered by DuckDB is access to more powerful window functions than are offered by dplyr itself. We see these in use in Section 2.3."
  },
  {
    "objectID": "published/retail-sales.html#sec-windows",
    "href": "published/retail-sales.html#sec-windows",
    "title": "Retail sales",
    "section": "2.3 Time windows",
    "text": "2.3 Time windows\nMany analyses in business and economics involve calculations over time windows. Some examples:\n\nReturns on stocks over the days or months after their initial public offerings (IPOs)\nComparing sales or profits for the year to date with previous years\nSmoothing sales over time using moving averages\n\nWe will see that working with time windows is greatly facilitated by the use of window functions and that the SQL engine offered by DuckDB provides the more-powerful window functions that we will need here.\nTo create a DuckDB engine, we simply create a connection.\n\ndb &lt;- dbConnect(duckdb::duckdb())\n\nWe can then use the function copy_to() to move the data from R into the in-memory DuckDB database we have created.8\n\nretail_sales_db &lt;-\n  retail_sales |&gt;\n  copy_to(db, df = _, overwrite = TRUE)\n\nWe can inspect retail_sales_db at the R console much as we did with retail_sales above.\n\nretail_sales_db\n\n# A query:  ?? x 6\n# Database: DuckDB 1.4.4 [igow@Darwin 25.4.0:R 4.5.2/:memory:]\n   industry                     state date       sales ind_level parent_industry\n   &lt;chr&gt;                        &lt;chr&gt; &lt;date&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;chr&gt;          \n 1 Cafes, restaurants and cate… Aust… 1982-04-01   4.4         2 Cafes, restaur…\n 2 Cafes, restaurants and cate… New … 1982-04-01  61.8         2 Cafes, restaur…\n 3 Cafes, restaurants and cate… Nort… 1982-04-01  NA           2 Cafes, restaur…\n 4 Cafes, restaurants and cate… Quee… 1982-04-01  18.7         2 Cafes, restaur…\n 5 Cafes, restaurants and cate… Sout… 1982-04-01  14.3         2 Cafes, restaur…\n 6 Cafes, restaurants and cate… Tasm… 1982-04-01   1.9         2 Cafes, restaur…\n 7 Cafes, restaurants and cate… Tota… 1982-04-01 146.          2 Cafes, restaur…\n 8 Cafes, restaurants and cate… Vict… 1982-04-01  36.4         2 Cafes, restaur…\n 9 Cafes, restaurants and cate… West… 1982-04-01   8           2 Cafes, restaur…\n10 Cafes, restaurants and take… Aust… 1982-04-01   7.6         1 Total (Industr…\n# ℹ more rows\n\n\nThe output is almost identical to the output shown above when we typed retail_sales at the R console. The two differences are at the top and bottom of the output. At the top, we now see two lines indicating that the data come from a table inside an in-memory (:memory) DuckDB database and thus represent a remote data frame rather than a local data frame.9 At the bottom, we no longer see the numeber of rows. This partly reflects the lazy nature of remote data frames: we have to execute a specific query to determine the number of rows.10\n\n2.3.1 Year-to-date (YTD)\nOften we will want to compare performance for the current period with that in earlier periods. If we were in the middle of August 2025, it would not make sense to compare total sales for 2025 with total sales for 2024, as this would be comparing seven months (January through July) with twelve months. One standard approach is to calculate year-to-date performance and compare that with performance for a comparable period from earlier years.\nFor this exercise, I will focus on industry subgroups of Cafes, restaurants and takeaway food services and just two states—Victoria and Western Australia. I create cafes_vic_wa_db, a data frame focused on these observations.\n\ncafes_vic_wa_db &lt;-\n  retail_sales_db |&gt; \n  filter(str_detect(parent_industry, \"^Cafes\"),\n         state %in% c(\"Victoria\", \"Western Australia\"))\n\nThe next step is to create ytd_plot_df, the data frame that will contain the data we need to plot year-to-date sales. Here we first calculate year and then group by industry, state, and year. Grouping by industry and state makes sense as it does not make sense to aggregate data across industries and states in this context. Grouping additionally by year reflects the nature of the YTD calculation we are making: only values for a given year should be consider.\nWe next use window_order(date) so that the following mutate() command puts the data in the correct order. It is helpful to think of the windows that are being created.11\nThe following line invokes cumsum(sum) to calculate the cumulate sales for each (industry, state, year) up to the current row (remember that the rows are order by date) before invoking ungroup() and then filter() to focus on the years we want to plot.\nThe collect() command brings the data into R, converting the remote data frame to a local data frame. The last line converts year to a character variable and calculates month using the month() function.\n\nytd_plot_df &lt;-\n  cafes_vic_wa_db |&gt;\n  mutate(year = year(date)) |&gt;\n  group_by(industry, state, year) |&gt;\n  window_order(date) |&gt;\n  mutate(sales_ytd = cumsum(sales)) |&gt;\n  ungroup() |&gt;\n  filter(between(year, 2018, 2021)) |&gt;\n  collect() |&gt;\n  mutate(year = as.character(year),\n         month = month(date, label = TRUE))\n\nWith ytd_plot_df in hand, creating Figure 4 is quite straightforward:12\n\nytd_plot_df |&gt;\n  ggplot(aes(x = month, y = sales_ytd / 1e3,\n             color = year, group = year)) +\n  ylab(\"Sales, year-to-date ($ billion)\") +\n  xlab(\"Month\") +\n  geom_line() +\n  facet_wrap(industry ~ state, ncol = 2, scales = \"free_y\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nFigure 4: Year-to-date sales for cafes\n\n\n\n\n\n\n2.3.1.1 Exercises\n\nWhat patterns do you observe in Figure 4? Can you suggest explanations for these patterns?\nIs cafes_vic_wa_db (created by the code above) a remote data frame or a local data frame? How can you tell?\nDoes filter(str_detect(parent_industry, \"^Cafes\")) produce the same result as filter(parent_industry == \"Cafes, restaurants and takeaway food services\")? What benefit is there from using the former option rather than the latter?\nWhat happens if you replace the function window_order() with function arrange().\nAs an analogue to cafes_vic_wa_db (created using retail_sales_db), create cafes_vic_wa using retail_sales. Use cafes_vic_wa to create a version of ytd_plot_df. (Hint: You will need to use arrange() in place of window_order().) Does the plot created using this version of ytd_plot_df look the same as Figure 4?\nDo we need both color = year and group = year in creating Figure 4? How can you tell?\nWhat happens if we omit ungroup() from the code creating ytd_plot_df? Is there any effect on ytd_plot_df? Is there any effect on the appearance of Figure 4?\nWhat is the data type for month in ytd_plot_df? (Hint: You may find it helpful to type ytd_plot_df$month at the R console or to use the function class().)\nWhat happens if we move the mutate() code creating year and month in ytd_plot_df before the collect() call? Does the code still run? Does the plot look fine? Can you explain what’s going on? (Hint: Go back to the previous question with the new version of ytd_plot_df.)\nIn creating ytd_plot_df, can we move the application of as.character() to the line mutate(year = year(date))? (That is, can we use as.character(year(date)) in that line?) What other code do we need to modify to take this approach? After making the needed modification, does the plot produced look the same as Figure 4? Why did we not need to use the same type for year as we did for month? (Hint: You may find it helpful to type ytd_plot_df$year at the R console or to use the function class().)\n\n\n\n\n2.3.2 Fiscal year-to-date (YTD)\nAbove we calculated YTD numbers using calendar years. However, for many purposes, most companies in Australia reckon fiscal years from 1 July to 30 June and we might want to plot fiscal YTD numbers.\nTo make this happen, I do two things. First, I create a function fiscal_month() that behaves a lot like month(), but with an extra argument that allows me to specify the first month of the fiscal year (e.g., first_month = 7 would make July the first month of the fiscal year). For present purposes, you don’t need to understand the details of fiscal_month(), but note that this actually borrows elements from the original month() function itself.\n\nfiscal_month &lt;- function(x, first_month = 7, label = TRUE, abbr = TRUE) {\n  new_order &lt;- c(first_month:12, 1:(first_month - 1))\n  labels &lt;- levels(month(x, label = label, abbr = abbr))\n  new_labels &lt;- labels[new_order]\n  ordered(month(x), levels = new_order, labels = new_labels)\n}\n\nThe second thing I do is create fyear to represent fiscal year based on fiscal years starting in July. Below is code creating fytd_plot_df using fyear in place of year used in ytd_plot_df and using fiscal_month() in place of month().\n\nfytd_plot_df &lt;-\n  cafes_vic_wa_db |&gt;\n  mutate(year = year(date),\n         fyear = year + as.integer(month(date) &gt;= 7)) |&gt;\n  group_by(industry, state, fyear) |&gt;\n  window_order(date) |&gt;\n  mutate(sales_ytd = cumsum(sales)) |&gt;\n  ungroup() |&gt;\n  filter(between(fyear, 2018, 2021)) |&gt;\n  collect() |&gt;\n  mutate(fyear = as.character(fyear),\n         month = fiscal_month(date))\n\nThe following code creates Figure 5 and is essentially identical to that used to create Figure 4 except for changes to reflect use of fyear in place of year.\n\nfytd_plot_df |&gt;\n  ggplot(aes(x = month, y = sales_ytd / 1e3,\n             color = fyear, group = fyear)) +\n  ylab(\"Sales, fiscal year-to-date ($ billion)\") +\n  xlab(\"Month\") +\n  geom_line() +\n  facet_wrap(industry ~ state, ncol = 2, scales = \"free_y\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nFigure 5: Fiscal year-to-date sales for cafes\n\n\n\n\n\n\n2.3.2.1 Exercises\n\nIn words, what is the code creating fyear in the creation of fytd_plot_df doing? (Hint: You might find it helpful to append count(fyear, date) |&gt; arrange(date) to the first two lines of the code creating fytd_plot_df.)\nWhy is important to filter() using fyear rather than year (i.e., between(fyear, 2018, 2021)))? (Hint: Change the filter() to use year instead and look at the resulting plot.)\nWhat happens if the call to fiscal_month() in the creation of fytd_plot_df is replaced by a call to month = month(date, label = TRUE)? (Hint: Change the code and look at the resulting plot.)\nWhat features of the variable month in the data frame fytd_plot_df ensure that we get the plot we are looking for?\nCould we use cafes_vic_wa in place of cafes_vic_wa_db to create fytd_plot_df? What changes do we need to make to the code? Does the resulting plot look the same as Figure 5?\n\n\n\n\n2.3.3 Moving averages\nAnother calculation using time windows that is commonly used is the moving average. A moving average can be used to smooth out a volatile time series by replacing the value for period \\(t\\) with the average values for the periods \\(t - k\\) through \\(t\\), where \\(k \\geq 1\\). With seasonal data, it seems natural to consider twelve-month periods (i.e., \\(k = 11\\)).\nTo indicate that we want to use windows from \\(t - 11\\) to \\(t\\), we can use window_frame(-11, 0). This causes aggregate functions such as mean() and sum() to be applied as window functions to the resulting data frame.13\n\ngroup_ma_plot_df &lt;-\n  retail_sales_db |&gt;\n  left_join(short_names, by = \"industry\", copy = TRUE) |&gt;\n  mutate(industry = coalesce(short_ind, industry)) |&gt;\n  group_by(industry, state) |&gt;\n  window_order(date) |&gt;\n  window_frame(-11, 0) |&gt;\n  mutate(sales_ma = mean(sales, na.rm = TRUE),\n         count_ma = sum(as.integer(!is.na(sales)), na.rm = TRUE)) |&gt;\n  ungroup() |&gt;\n  filter(state == \"Total (State)\", ind_level == 1,\n         between(date, start_date, end_date),\n         count_ma == 12)\n\nFigure 6 plots both the underlying time-series and the twelve-month moving average.\n\ngroup_ma_plot_df |&gt;\n  ggplot(aes(x = date)) +\n  geom_line(aes(y = sales / 1e3, colour = \"Sales\")) +\n  geom_line(aes(y = sales_ma / 1e3, colour = \"12-month moving average\")) +\n  ylab(\"Sales ($ billion)\") +\n  xlab(\"Month\") +\n  facet_wrap(industry ~ ., ncol = 2, scales = \"free_y\") +\n  theme(legend.title = element_blank(),\n        legend.position = \"bottom\")\n\n\n\n\n\n\n\nFigure 6: Retail sales by industry with moving average\n\n\n\n\n\nAn alternative approach to constructing the data frame for the plot above would use pivot_longer:\n\ngroup_ma_plot_df_alt &lt;-\n  group_ma_plot_df |&gt;\n  pivot_longer(cols = starts_with(\"sales\"), \n               names_to = \"series\", \n               values_to = \"sales\")\n\n\n2.3.3.1 Exercises\n\nConstruct a plot like Figure 6 using group_ma_plot_df_alt instead of group_ma_plot_df. (Hint: These code snippets may help: ggplot(aes(x = date, y = sales, colour = series)) and geom_line().) What differences do you see between your plot and Figure 6? Can you remove these differences by modifying group_ma_plot_df_alt? (Hint: Try using mutate() with case_when().)\nDoes the moving average help you to discern trends or patterns that are more difficult to see with the original time-series? If so, discuss how it helps. If not, why not?\nWhy do you think I included count_ma == 12 in the filter() arguments? What happens if you omit this? Suppose I wanted to include the moving average in the plot only when count_ma == 12 but wanted to include all values of underlying time-series. How could I achieve this? Construct a plot applying this approach. Do you think that this plot is more informative? (Hint: Try using mutate() with if_else().)\nHow helpful is the use of the moving average in Figure 6? Are there some features of the data that are highlighted or obscured?\n\n\n\n\n2.3.4 Economic shocks\nI guess that one of the observations you had about Figures 1 and 2 is the sharp decrease in sales, especially to cafes and restaurants, in early 2020. Assuming you are more than ten years old, you probably attributed this to the Covid-19 pandemic of 2020–2022.\nOn 25 February 2020, the federal government of Australia activated an emergency response plan in response to the rising number of cases of Covid-19 around the world. During 2020 and 2021, Australia largely pursued a strategy of eliminating Covid-19.14 Even entertaining such a strategy was only possible due to special features of Australia, including the absence of any land borders and the fact that the majority of the population is located in urban areas that are largely isolated from each other. Most Australians live in capital cities of Australia’s six states and these cities are far apart from each other. The two state capitals on the Australian mainland that are closest to each other are Melbourne (Victoria) and Adelaide (South Australia), which are 654km apart.15\nOne consequence of Australia’s approach to Covid-19 is that Australians spent a lot more time in lockdowns than did citizens of other developed countries. That said, not all states had the same experience. Victoria had the worst experience with Covid-19, in terms of both the number of cases and total days spent in lockdown. Melbourne spent 262 days in lockdown with the last lockdown ending on 21 October 2021.16 Western Australia, isolated even by Australian standards, had perhaps the best experience; it had just 13 days in lockdown. These lockdowns were much more rigorous than their equivalents in many other countries, as these were not “advisories” but strictly enforced requirements for people to stay at home with few exceptions.\nThere is plenty of anecdotal evidence that alcohol consumption increased during Covid-19.17 One question might be whether there are differences in consumption patterns between Victoria and Western Australia given their radically different experiences with lockdowns.\nI begin by constructing liquor_df, a data frame with sales for Liquor retailing from 1 January 2018 onwards for Victoria and Western Australia\n\nliquor_df &lt;-\n  retail_sales |&gt;\n  filter(state %in% c(\"Victoria\", \"Western Australia\"),\n         industry == \"Liquor retailing\") |&gt;\n  filter(date &gt;= \"2018-01-01\")\n\nI use the data in liquor_df to construct liquor_plot_df, which I will use for a plot. I first construct scale_factor for each state based on average sales in 2018 and then use that to scale sales to calculate sales_normalised.\n\nliquor_plot_df &lt;-\n  liquor_df |&gt;\n  filter(year(date) == 2018) |&gt;\n  group_by(state) |&gt;\n  summarize(scale_factor = mean(sales, na.rm = TRUE), .groups = \"drop\") |&gt;\n  inner_join(liquor_df, by = \"state\") |&gt;\n  mutate(year = year(date),\n         month = month(date, label = TRUE),\n         sales_normalised = sales / scale_factor)\n\nFigure 7 plots the data in liquor_plot_df by month and state by year.\n\nliquor_plot_df |&gt;\n  ggplot(aes(x = month, y = sales_normalised, fill = state)) +\n  geom_col(position = \"dodge\") +\n  facet_wrap(year ~ ., ncol = 1) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nFigure 7: Liquor retailing sales (normalised by 2018 average monthly sales)\n\n\n\n\n\nLooking at Figure 7, we can see that Victoria and Western Australia look very similar to each other in 2018. This is to be expected as the mean value of sales_normalised in 2018 is exactly one by construction for both states. However, there is some evidence of greater seasonality in sales in Victoria, with the value sales_normalised in December 2018 being higher there, while values in months in the middle of the year are lower.\nIn 2019, we see a pretty similar pattern to that of 2018 even though there is no mechanical requirement for average sales_normalised to be the same across the two states to be the same in that year.\nBut from 2020 onwards, we start to see some divergence that is explored in the exercises.\n\n2.3.4.1 Exercises\n\nWhat is position =\"dodge\" doing in Figure 7? (Hint: Take this out and compare the plot created with Figure 7.)\nWhat patterns do you observe in Figure 7 over time? What do you conjecture explains these patterns?\nWhat alternative explanations (other than the one you just gave) could explain the patterns observed in Figure 7? What data would you need to get to determine which explanation for these patterns is the most plausible?\nWhat do you find most surprising about Figure 7? Does this support your conjectured explanations or raise questions about it?\n\n\n\n\n2.3.5 Where can I get the data?\n\n\n\n\n\n\nCaution\n\n\n\n\n\nThe material in this section is relatively advanced. You could skip this on a first reading and come back to this material when your skills have developed more or when you are interested in getting and working with other data from ABS.\n\n\n\nAs discussed, I got the data used to create retail_sales.csv from the ABS. While the ABS provides an application programming interface (API), this API is difficult to use relative to using community-supplied R packages that extract data from the older spreadsheet formats provided by the ABS. I used the readabs package (Cowgill et al. 2025) to download the data. The readabs package downloads spreadsheets into a directory identified using the environment variable R_READABS_PATH, which I set to \"~/Downloads/\", a location on my computer.\n\nSys.setenv(R_READABS_PATH = \"~/Downloads/\")\nlibrary(readabs)\n\nI get data for code 8501.0, which is covers retail trade data in Australia. I focus on table 11 from that series.18 I use regular expressions to split the series column into three columns: label, state, and industry.19\n\ntable &lt;- \"TABLE 11. Retail Turnover, State by Industry Subgroup, Original\"\n\nabs_data &lt;-\n  read_abs(\"8501.0\") |&gt;\n  filter(table_title == table) |&gt;\n  separate_wider_regex(series,\n                       c(label = \"^.*\", \";\\\\s+\", \n                         state = \".*?\", \"\\\\s*;\\\\s*\",\n                         industry = \".*?\", \"\\\\s*;$\")) |&gt;\n  mutate(label = str_trim(label)) |&gt;\n  select(industry, state, date, value) |&gt;\n  rename(sales = value) |&gt;\n  arrange(date, industry, state)\n\nabs_data\n\n# A tibble: 96,957 × 4\n   industry                                      state          date       sales\n   &lt;chr&gt;                                         &lt;chr&gt;          &lt;date&gt;     &lt;dbl&gt;\n 1 Cafes, restaurants and catering services      Australian Ca… 1982-04-01   4.4\n 2 Cafes, restaurants and catering services      New South Wal… 1982-04-01  61.8\n 3 Cafes, restaurants and catering services      Northern Terr… 1982-04-01  NA  \n 4 Cafes, restaurants and catering services      Queensland     1982-04-01  18.7\n 5 Cafes, restaurants and catering services      South Austral… 1982-04-01  14.3\n 6 Cafes, restaurants and catering services      Tasmania       1982-04-01   1.9\n 7 Cafes, restaurants and catering services      Total (State)  1982-04-01 146. \n 8 Cafes, restaurants and catering services      Victoria       1982-04-01  36.4\n 9 Cafes, restaurants and catering services      Western Austr… 1982-04-01   8  \n10 Cafes, restaurants and takeaway food services Australian Ca… 1982-04-01   7.6\n# ℹ 96,947 more rows\n\n\nOne thing that can be observed in the output above is that there is no indication of which industries are groups and which are subgroups. Also there is no indication of how the subgroups map to groups. To handle this, I created the following data frame to map industry to ind_level.\n\nretail_industries &lt;- tribble(\n  ~industry, ~ind_level,\n  \"Supermarket and grocery stores\", 2,\n  \"Liquor retailing\", 2,\n  \"Other specialised food retailing\", 2,\n  \"Food retailing\", 1,\n  \"Furniture, floor coverings, houseware and textile goods retailing\", 2,\n  \"Electrical and electronic goods retailing\", 2,\n  \"Hardware, building and garden supplies retailing\", 2,\n  \"Household goods retailing\", 1,\n  \"Clothing retailing\", 2,\n  \"Footwear and other personal accessory retailing\", 2,\n  \"Clothing, footwear and personal accessory retailing\", 1,\n  \"Department stores\", 1,\n  \"Newspaper and book retailing\", 2,\n  \"Other recreational goods retailing\", 2,\n  \"Pharmaceutical, cosmetic and toiletry goods retailing\", 2,\n  \"Other retailing n.e.c.\", 2,\n  \"Other retailing\", 1,\n  \"Cafes, restaurants and catering services\", 2,\n  \"Takeaway food services\", 2,\n  \"Cafes, restaurants and takeaway food services\", 1,\n  \"Total (Industry)\", 0) |&gt;\n  mutate(parent_industry = if_else(ind_level == 1, industry, NA)) |&gt;\n  fill(parent_industry, .direction = \"up\") |&gt;\n  mutate(parent_industry = if_else(ind_level == 1, \"Total (Industry)\", \n                                   parent_industry)) \n\n\nretail_sales &lt;-\n  abs_data |&gt;\n  left_join(retail_industries, join_by(industry)) |&gt;\n  write_csv(\"data/retail_sales.csv\") \n\n\n2.3.5.1 Exercises\n\nDescribe in words what the mutate()-fill()-mutate() sequence is doing in the creation of retail_industries above? Why do I first populate parent_industry with the value of industry before overwriting that wtih \"Total (Industry)\" two lines later? What assumptions does the code make about how the data provided by preceding steps?"
  },
  {
    "objectID": "published/retail-sales.html#footnotes",
    "href": "published/retail-sales.html#footnotes",
    "title": "Retail sales",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTODO: Add instructions for setting up R. For now, Steps 1 and 2 from here would probably work. Then run install.packages(c(\"tidyverse\", \"modelsummary\", \"dbplyr\", \"DBI\", \"readabs\")).↩︎\nTODO: Add templates. Current link goes to templates for a different book.↩︎\nTODO: This repository has not yet been set up. For now you can get the data by running the code provided in Section 2.3.5.↩︎\nTODO: Discuss implicit missing values in an earlier section on left_join().↩︎\nOnce we recognize that Department stores is its own parent industry, it is trivial that its own sales will “add up” in the sense here, so we don’t need to consider that industry for present purposes.↩︎\nOf course, Northern Territory is not a state, but a territory.↩︎\nI put “states” in scare quotes because it includes territories and also a series for all states combined (Total (State)).↩︎\nTODO: Consider alternative approaches to getting data into DuckDB. Probably choose whichever is simplest pedagogically.↩︎\nThe terminology “remote” is used even though the data are on the same computer, hence local in a sense. In principle, the data could be on a computer on the other side of the planet (e.g., in a remote PostgreSQL database).↩︎\nTODO: This statement could be tightened up and clarified.↩︎\nTODO: Put a fleshed-out example of a window here.↩︎\nTODO: Should I scale by 1e3 throughout?↩︎\nFor some reason, cumsum() does not behave correctly if used with window_frame(). This much can be seen if you read the documentation shown when you type ? window_order at the R console. I don’t see the reason for this behaviour.↩︎\nUsing the term “elimination” to describe Australia’s approach was controversial. Nonetheless, it is clear that the approach in Australia differed radically from that in much of Europe and the United States.↩︎\nTODO: Tighten up the material here, including fact-checking of the data used herein.↩︎\nSee Wikipedia for more details.↩︎\nSee “Australia’s COVID-19 relationship with booze” in Pursuit for some illustrative statistics from the early part of the pandemic.↩︎\nFor some reason, supplying tables = 11 to read_abs() does not work for me, so I use filter() to focus on the data of interest.↩︎\nTODO: Add material to an earlier chapter on regular expression. For now, see the chapter on Importing data at https://iangow.github.io/far_book/web-data.html.↩︎"
  },
  {
    "objectID": "published/shared_code.html",
    "href": "published/shared_code.html",
    "title": "Shared code",
    "section": "",
    "text": "Open-source software dominates in certain areas. Probably most internet sites run on Linux machines. Python and R are huge in data science and statistics. All of these systems rely on thousands of open-source packages that are continually being improved in part because anyone can see how they work.\nAcademic research is quite different. Most research papers are really software development projects in disguise. While the final output is a PDF rather than an app, a tremendous amount of coding is often involved and multiple authors can be involved.\nYet the open-source model has not taken off in academia. While some journals are including data-and-code repositories as part of their process, these do not yet dominate. Authors are generally reluctant to share their code and data. There are multiple reasons for this in my view. First, the code is often a “trade secret” of sorts; future papers may be produced and authors may want to retain a competitive edge in what is essentially a zero-sum game.1 Second, having code available risks making it easy to show how fragile results are. It is difficult to replicate most research papers without access to the code and data, and many papers’ results are “fragile” at best. Third, a lot of authors likely fear embarrassment if others could see their code. Academics’ code is often inefficient, difficult to read, perhaps even wrong.\nFor some reason, a lot of the publicly available code in accounting research relates to two seemingly obscure topics: Fama-French industries and winsorization. As we cover both topics in Empirical Research in Accounting: Tools and Methods, I discuss these a little below.\n\n\n\n\n\n\nTip\n\n\n\nIn writing this note, I use several packages including those listed below.2 This note was written using Quarto and compiled with RStudio, an integrated development environment (IDE) for working with R. The source code for this note is available here and the latest version of this PDF is here.\n\nlibrary(dplyr)\nlibrary(farr)"
  },
  {
    "objectID": "published/shared_code.html#footnotes",
    "href": "published/shared_code.html#footnotes",
    "title": "Shared code",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nZero-sum because papers need to be published in a finite list of journals, which publish a finite number of papers. Publishing your paper on some topic often means not publishing mine.↩︎\nExecute install.packages(c(\"dplyr\", \"farr\", \"haven\")) within R to install all the packages you need to run the code in this note.↩︎\nAccording to https://siccode.com, “Standard Industrial Classification Codes (SIC Codes) identify the primary line of business of a company. It is the most widely used system by the US Government, public, and private organizations.”↩︎\nThe choices relate to handling of ties and interpolating values. See ? quantile in R for details.↩︎\nWhile the inclusion of prob = 0.01 is not strictly necessary given that that’s the default used by the function, this code does illustrate how you could choose prob = 0.02 to get winsorization at 2% and 98% levels, a popular alternative choice.↩︎"
  },
  {
    "objectID": "published/sirca_ma.html",
    "href": "published/sirca_ma.html",
    "title": "SIRCA Mergers and Acquisitions collection",
    "section": "",
    "text": "SIRCA’s Mergers and Acquisitions collection provides data on Australian M&A transactions.\nThe code in this note uses the packages listed below, plus the duckdb package.1 This note was written using Quarto and compiled with RStudio, an integrated development environment (IDE) for working with R. The source code for this note is available here.\n\nlibrary(tidyverse)\nlibrary(arrow)"
  },
  {
    "objectID": "published/sirca_ma.html#footnotes",
    "href": "published/sirca_ma.html#footnotes",
    "title": "SIRCA Mergers and Acquisitions collection",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nExecute install.packages(c(\"tidyverse\", \"arrow\", \"duckdb\", \"DBI\")) within R to install all the packages you need to run the code in this note.↩︎\nSee SIRCA’s documentation for details on getting the data.↩︎"
  },
  {
    "objectID": "published/sunrise.html",
    "href": "published/sunrise.html",
    "title": "Sunrise and sunset times",
    "section": "",
    "text": "make_local &lt;- function(time, tz) {\n  as.character(with_tz(time, tz)) |&gt;\n    as.POSIXct() |&gt;\n    strftime(x = _, format = \"%H:%M:%S\")\n}\n\n\nthe_date &lt;- \"2019-04-05\"\n\nbig_cities &lt;- \n  world.cities |&gt;\n  filter(pop &gt; 2e5) |&gt;\n  mutate(date = as.Date(the_date)) |&gt;\n  rename(lon = long) |&gt;\n  mutate(tz = tz_lookup_coords(lat, lon, warn = FALSE)) |&gt;\n  as_tibble()\n\n\nsunrise_times &lt;-\n  big_cities |&gt;\n  getSunlightTimes(data = _, keep = \"sunrise\") |&gt;\n  as_tibble()\n\nsunrise &lt;-\n  big_cities |&gt;\n  inner_join(sunrise_times, by = join_by(lat, lon, date)) |&gt;\n  select(name, country.etc, sunrise, date, tz, pop) |&gt;\n  rowwise() %&gt;%\n  mutate(local_time = make_local(sunrise, tz)) |&gt;\n  mutate(local_time = as.POSIXct(paste(date, local_time)))\n\n\nsunrise |&gt;\n  ggplot(aes(x = local_time)) +\n  geom_histogram(binwidth = 5 * 60) +\n  geom_text_repel(data = \n                    subset(sunrise, \n                           name == \"Boston\" | (pop &gt; 1e6 &\n                           (local_time &gt; as.POSIXct(paste(date, \"07:50.00\")) |\n                                  country.etc %in% c(\"Australia\")))),\n                  mapping = aes(y = 10, label = name, color = country.etc),\n                  angle = 90, vjust = 1, hjust = 1) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nFigure 1: Sunrise on 2019-04-05 (UTC)\n\n\n\n\n\n\nthe_year &lt;- year(the_date)\n\nsample_cities &lt;-\n    crossing(\n      world.cities %&gt;%\n        filter(name %in% \"Tokyo\" |\n                 (name %in% c(\"Boston\", \"Sydney\") \n                  & country.etc %in% c(\"USA\", \"Australia\")) |\n                 (name == \"Melbourne\" & country.etc == \"Australia\")) %&gt;%\n        rename(lon = long) %&gt;%\n        mutate(tz = tz_lookup_coords(lat, lon, warn = FALSE)) %&gt;%\n        as_tibble(),\n      date = seq(as.Date(str_glue(\"{the_year}-01-01\")), \n                 to = as.Date(str_glue(\"{the_year}-12-31\")),\n                 by = 1))\n\nmake_time &lt;- function(a_time, tz) {\n  res &lt;- strftime(a_time, format=\"%H:%M:%S\", tz = tz)\n  res &lt;- as.POSIXct(paste(\"2019-01-01\", res))\n  res\n}\n\nsample_cities_times &lt;-\n  sample_cities |&gt;\n  getSunlightTimes(data = _, keep = c(\"sunset\", \"sunrise\")) |&gt;\n  as_tibble() |&gt;\n  inner_join(sample_cities, by = join_by(date, lat, lon)) |&gt;\n  rowwise() |&gt;\n  mutate(sunrise = make_time(sunrise, tz),\n         sunset = make_time(sunset, tz))\n\n\nsample_cities_times |&gt;\n  gather(key = \"event\", value = \"time\", sunrise, sunset) |&gt;\n  ggplot(aes(x = date, y = time, group = event, color = event)) +\n  geom_line() +\n  facet_wrap(~ name) +\n  scale_x_date(date_labels = \"%b\") +\n  scale_y_datetime(labels = function(x) strftime(x, format=\"%H:%M\"),\n                   date_breaks = \"1 hour\") +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nFigure 2: Sunrise and sunset times over 2019"
  },
  {
    "objectID": "published/tidy_finance_betas.html",
    "href": "published/tidy_finance_betas.html",
    "title": "Calculating betas using DuckDB",
    "section": "",
    "text": "This note illustrates how DuckDB can be used to perform calculations with impressively high performance. The focus here is on the calculation of betas following the basic approach used in Tidy Finance.\nWhile the code here is surprisingly elegant and fast given the amount of work it is doing, there are limitations. For example, calculating betas is easy because univariate regression is supported by DuckDB (and PostgreSQL), but multivariate regression (e.g., to calculate loadings for multi-factor models) would require a different approach.\nIt seems that some of the benefits seen here could be achieved if we could use user-defined functions (UDFs). While there is no support for UDFs in the R API, support for scalar UDFs is provided in the Python API and support for aggregate UDFs, which is what would be needed here, is on the DuckDB roadmap.1\nThe code in this note uses the following packages. Use install.packages() to install any that are not already on your system.\nlibrary(farr)\nlibrary(dbplyr, warn.conflicts = FALSE)\nlibrary(tidyverse)\nlibrary(DBI)\nlibrary(frenchdata)\nlibrary(arrow)\nlibrary(duckdb)\nWe will use the following small function to calculate the time taken for steps below.2\nsystem_time &lt;- function(x) {\n  print(system.time(x))\n  x\n}"
  },
  {
    "objectID": "published/tidy_finance_betas.html#fama-french-factor-data",
    "href": "published/tidy_finance_betas.html#fama-french-factor-data",
    "title": "Calculating betas using DuckDB",
    "section": "1.1 Fama-French factor data",
    "text": "1.1 Fama-French factor data\nThe following code is adapted from code in Tidy Finance.\n\nfix_date &lt;- function(date) {\n  date_c &lt;- as.character(date)\n  ymd(if_else(nchar(date_c) == 6, str_c(date_c, \"01\"), date_c))\n}\n\nget_ff_factors &lt;- function(id, \n                           start_date = \"1960-01-01\",\n                           end_date = \"2021-12-31\") {\n  ff_vars &lt;- c(\"RF\", \"Mkt-RF\", \"SMB\", \"HML\", \"RMW\", \"CMA\")\n  \n  download_french_data(id)$subsets$data[[1]] |&gt;\n    mutate(across(any_of(ff_vars), \\(x) as.numeric(x) / 100),\n           date = fix_date(date)) |&gt;\n    rename_with(str_to_lower) |&gt;\n    rename(mkt_excess = `mkt-rf`) |&gt;\n    filter(date &gt;= ymd(start_date),\n           date &lt;= ymd(end_date)) |&gt;\n    mutate(month = floor_date(date, \"month\"))\n}\n\nCollecting the Fama-French factor data (from the web) takes just a few seconds.\n\nfactors_ff3_monthly &lt;- get_ff_factors(\"Fama/French 3 Factors\")\nfactors_ff3_daily &lt;- \n  get_ff_factors(\"Fama/French 3 Factors [Daily]\") |&gt;\n  system_time()\n\n   user  system elapsed \n  0.142   0.024   1.170 \n\n\nNote that the get_ff_factors() function is flexible enough to handle \"Fama/French 5 Factors (2x3)\" as an argument."
  },
  {
    "objectID": "published/tidy_finance_betas.html#crsp-data",
    "href": "published/tidy_finance_betas.html#crsp-data",
    "title": "Calculating betas using DuckDB",
    "section": "1.2 CRSP data",
    "text": "1.2 CRSP data\nWe will need data from two tables: crsp.msf and crsp.dsf. One option would be to connect to the WRDS PostgreSQL database for these:\n\ndb &lt;- dbConnect(RPostgres::Postgres())\nmsf &lt;- tbl(db, Id(table = \"msf\", schema = \"crsp\"))\ndsf &lt;- tbl(db, Id(table = \"dsf\", schema = \"crsp\"))\n\nHowever,it turns out that there is no efficient way to merge the Fama-French factor data we have above with those tables on the WRDS server due our inability to create temporary tables there.3 In fact, even with a local PostgreSQL database, we can do better if we have local parquet versions of crsp.msf and crsp.dsf (see here), and we assume that we have those in the rest of this note.\nIf we have our library set up, we can create an in-memory DuckDB database, and use load_parquet() from the farr package to load these tables in the database. Note that nothing is actually being read into memory at this point, so these steps take a few milliseconds.\n\ndb &lt;- dbConnect(duckdb::duckdb())\nmsf &lt;- load_parquet(db, \"msf\", \"crsp\")\ndsf &lt;- load_parquet(db, \"dsf\", \"crsp\") |&gt; system_time()\n\n   user  system elapsed \n  0.006   0.000   0.007"
  },
  {
    "objectID": "published/tidy_finance_betas.html#combining-crsp-and-fama-french-data",
    "href": "published/tidy_finance_betas.html#combining-crsp-and-fama-french-data",
    "title": "Calculating betas using DuckDB",
    "section": "1.3 Combining CRSP and Fama-French data",
    "text": "1.3 Combining CRSP and Fama-French data\nNow we can construct the tables used in Tidy Finance from merging CRSP and Fama-French factor data: crsp_monthly and crsp_daily. Again, because these queries are lazy, no material time is taken with these steps.\n\ncrsp_monthly &lt;- \n  msf |&gt;\n  filter(!is.na(ret)) |&gt;\n  mutate(month = floor_date(date, \"month\")) |&gt;\n  inner_join(factors_ff3_monthly, by = \"month\",\n             copy = TRUE,\n             suffix = c(\"\", \"_ff\")) |&gt;\n  mutate(\n    ret_excess = ret - rf,\n    ret_excess = pmax(ret_excess, -1, na.rm = TRUE)\n  ) |&gt;\n  select(permno, date, month, ret_excess, mkt_excess) |&gt;\n  system_time()\n\n   user  system elapsed \n  0.041   0.003   0.051 \n\n\n\ncrsp_daily &lt;- \n  dsf |&gt;\n  filter(!is.na(ret)) |&gt;\n  inner_join(factors_ff3_daily, by = \"date\",\n             copy = TRUE) |&gt;\n  mutate(ret_excess = ret - rf,\n         ret_excess = pmax(ret_excess, -1, na.rm = TRUE)) |&gt;\n  system_time()\n\n   user  system elapsed \n  0.034   0.001   0.036"
  },
  {
    "objectID": "published/tidy_finance_betas.html#monthly-betas",
    "href": "published/tidy_finance_betas.html#monthly-betas",
    "title": "Calculating betas using DuckDB",
    "section": "2.1 Monthly betas",
    "text": "2.1 Monthly betas\nIn Tidy Finance, the slide_period_vec() function from the slider package is used to create 60-month windows with a minimum of 48 months being required. With either DuckDB or PostgreSQL, we can use window functions to achieve the same thing.\nThe Tidy Finance code uses lm() from R to calculate the slope of a regression of a stock’s excess returns on market excess returns. Both DuckDB and PostgreSQL offer the aggregate function regr_slope(y, x) that we can use as a window function. However, regr_slope(y, x) is not recognized as an aggregate function by dbplyr, so we need to write the SQL for the window ourselves.\nThere is some complexity in the construction of the window frame, but the documentation provided by PostgreSQL can help us to construct the right frame. In this case, it seems we want the range starting 60 months before the current row and the current row. We partition by permno and order by date. In dplyr, this would be group_by(permno) |&gt; window_order(date), but in SQL, we use PARTITION BY and ORDER BY. In SQL, we define a window (w) and then write SELECT regr_slope(y, x) OVER w; we include the OVER in our variable w.\n\nw &lt;- paste(\"OVER (PARTITION BY permno\",\n           \"ORDER BY date\",\n           \"RANGE BETWEEN INTERVAL '60 MONTHS' PRECEDING AND\",\n           \"CURRENT ROW)\")\n\nHaving constructed our window, we can append the window to SQL for the aggregates we want to use. To impose the requirement minimum number of months, we calculate n_rets, which is the number of return values in our window w and impose a filter accordingly. Finally, we select just the variables we want.4\nAccording to the Tidy Finance website, “using eight cores, the estimation for our sample of around 25k stocks takes around 20 minutes.” How long does our approach take? Less than a second! Note that DuckDB does multi-core processing out of the box, with no need to set up workers or such like.5\n\nbeta_monthly &lt;-\n  crsp_monthly |&gt;\n  mutate(beta = sql(paste(\"regr_slope(ret_excess, mkt_excess)\", w)),\n         n_rets = sql(paste(\"count(ret_excess)\", w))) |&gt;\n  filter(n_rets &gt;= 48) |&gt;\n  select(permno, month, date, beta) |&gt;\n  collect() |&gt;\n  system_time()\n\n   user  system elapsed \n  2.381   0.130   0.407"
  },
  {
    "objectID": "published/tidy_finance_betas.html#daily-betas",
    "href": "published/tidy_finance_betas.html#daily-betas",
    "title": "Calculating betas using DuckDB",
    "section": "2.2 Daily betas",
    "text": "2.2 Daily betas\nNow, let’s do daily betas. Here Tidy Finance uses a three-month window and requires at last fifty observations. This requires only a minor tweak to our window definition.\n\nw &lt;- paste(\"OVER (PARTITION BY permno\",\n            \"ORDER BY date\",\n            \"RANGE BETWEEN INTERVAL '3 MONTHS' PRECEDING AND CURRENT ROW)\")\n\nWhile daily data are used to calculate the betas, only one beta per PERMNO-month is returned. There are some ambiguities in my reading of Tidy Finance in terms of exactly which dates are used for each month, but I assume that daily returns for the three months up to the last day of the month are used. The code below actually calculates betas using three months of data for every stock for every day, but only keeps data for the last day of each month.\nFor Tidy Finance, “the whole estimation takes around 30 minutes using eight cores and 16GB memory.” With the approach here, it takes about 8 seconds. This is relatively worse than the monthly beta calculations, which I suspect is due to our calculations of betas for every day even though we don’t keep these.\n\nbeta_daily &lt;-\n  crsp_daily |&gt;\n  mutate(beta = sql(paste(\"regr_slope(ret_excess, mkt_excess)\", w)),\n         n_rets = sql(paste(\"count(ret_excess)\", w))) |&gt;\n  filter(n_rets &gt;= 50) |&gt;\n  mutate(month = floor_date(date, \"month\")) |&gt;\n  group_by(permno, month) |&gt;\n  filter(date == max(date, na.rm = TRUE)) |&gt;\n  ungroup() |&gt;\n  select(permno, month, date, beta) |&gt;\n  collect() |&gt;\n  system_time()\n\n   user  system elapsed \n 58.516   6.514   6.669 \n\n\nThis is pretty impressive given that we have calculated 4,251,711 betas!\nAt this point, DuckDB has done its work and we have the data in local data frames. So we can disconnect from our in-memory database.\n\ndbDisconnect(db, shutdown = TRUE)"
  },
  {
    "objectID": "published/tidy_finance_betas.html#comparing-beta-estimates",
    "href": "published/tidy_finance_betas.html#comparing-beta-estimates",
    "title": "Calculating betas using DuckDB",
    "section": "2.3 Comparing beta estimates",
    "text": "2.3 Comparing beta estimates\nThe remaining code is largely copy-pasted from Tidy Finance. Our focus here is Figure 1, a version of Figure 4 from Tidy Finance that compares monthly and daily beta estimates for four stocks.\n\nexamples &lt;- tribble(\n  ~permno, ~company,\n  14593, \"Apple\",\n  10107, \"Microsoft\",\n  93436, \"Tesla\",\n  17778, \"Berkshire Hathaway\")\n\nBecause we have retained date, we merge on that variable too when constructing beta.\n\nbeta &lt;- \n  beta_monthly |&gt;\n  full_join(beta_daily, \n            by = c(\"permno\", \"month\", \"date\"),\n            suffix = c(\"_monthly\", \"_daily\")) |&gt;\n  arrange(permno, date)\n\nI have made no effort to do a detailed reconciliation of my calculations with those in Tidy Finance, but Figure 1 looks quite similar to the counterpart in Tidy Finance.\n\nbeta |&gt;\n  inner_join(examples, by = \"permno\") |&gt;\n  pivot_longer(cols = c(beta_monthly, beta_daily)) |&gt;\n  filter(!is.na(value)) |&gt;\n  ggplot(aes(\n    x = date, \n    y = value, \n    color = name, \n    linetype = name\n    )) +\n  geom_line() +\n  facet_wrap(~ company, ncol = 1) +\n  labs(x = NULL, y = NULL, color = NULL, linetype = NULL) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nFigure 1: Comparison of beta estimates using monthly and daily data"
  },
  {
    "objectID": "published/tidy_finance_betas.html#footnotes",
    "href": "published/tidy_finance_betas.html#footnotes",
    "title": "Calculating betas using DuckDB",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee here for an example of using a scalar Python UDF.↩︎\nUnlike the base R system.time(), this function works with assignment. If we put system.time() at the end of a pipe, then the value returned by system.time() would be stored rather than the result of the pipeline preceding it. Hadley Wickham explained to me that this function works because of lazy evaluation, which is discussed in “Advanced R” here. Essentially, x is evaluated just once—inside system.time()—and its value is returned in the next line.↩︎\nAs discussed here, the copy_inline() function can often address this gap, but the complexity of the queries below seems too high for this to be a viable approach.↩︎\nWhile the date variable is omitted in Tidy Finance, we retain it, as in principle it facilitates merging of beta data with monthly CRSP data.↩︎\nMy understanding is that this may be a function of writing DuckDB with C++11.↩︎"
  },
  {
    "objectID": "published/weather-syd.html",
    "href": "published/weather-syd.html",
    "title": "Defining winter and summer in Sydney",
    "section": "",
    "text": "Figure 1: Average daily temperatures for 91 days following indicated date for period 2001–2023\n\n\n\n\n\nIn the United States, one often hears people speak of the “official” start of seasons. Ironically, there seems to be nothing that is official about these dates. However, there is consensus about the dates. The “official” start of summer is the summer solstice (for 2024: 20 June in Boston, 21 December in Sydney) and the “official” start of winter is (for 2024: 21 December in Boston, 21 June in Sydney).\nIn Australia, the usual convention is to divide seasons by months. On this basis, winter starts on 1 June and summer starts on 1 December.\nIs there a sense in which one approach is more correct than the other? Focusing on summer and winter, one definition for these seasons would be that winter starts on the first day of the 91-day period that is the coldest such period for a year averaged over a number of years. Similarly, summer should starts on the first day of the 91-day period that is the hottest such period for a year averaged over a number of years.\nWe answer this question focusing on Sydney, Australia (latitude of -33.9, longitude: 151).\nDaily temperature data from Open-Meteo comprise a maximum and minimum temperature. So immediately we have two possible definitions of each season according to the temperature we use (e.g., summer could be the 91-day period that has the highest average minimum temperature or it could be the period that has the highest average maximum temperature. Here we consider both.\nThe start of winter based on the 91-day period with the lowest average maximum temperature is 27 May. The start of winter based on the 91-day period with the lowest average minimum temperature is 03 June. So whether we use maximums or minimums, we get close to the Australian convention for winter.\nThe start of summer based on the 91-day period with the highest average maximum temperature is 30 November. The start of summer based on the 91-day period with the highest average minimum temperature is 16 December. With summer, we get close to the US convention for summer using minimums, but close to the Australian convention using maximums.\nInterestingly, it seems that using average maximums for summer and winter gets closest to the current approach in Australia. However, even using these we have the issue that spring begins on 26 August and autumn begins on 01 March. This implies a spring of 96 days and an autumn of 87 days."
  },
  {
    "objectID": "published/window-functions.html",
    "href": "published/window-functions.html",
    "title": "Responsive open-source software: Two examples from dbplyr",
    "section": "",
    "text": "In this note, I explore some recent changes in the open-source R package dbplyr to illustrate some of the beauty of how open-source software evolves in practice. In particular, I offer two case studies where features requested by users became reality in dbplyr, which may be my favourite R package.\n\n\n\n\n\n\nTip\n\n\n\nIn writing this note, I used the packages listed below.1 At the time of writing, you also need to install the development version of dbplyr, which you can do using the remotes::install_github() command below. This note was written and compiled using Quarto with RStudio, an integrated development environment (IDE) for working with R. The source code for this note is available here and the latest version of this PDF is here.\n\n\n\nlibrary(farr)\nlibrary(dplyr, warn.conflicts = FALSE)\nlibrary(DBI)\n\n\nremotes::install_github(\"tidyverse/dbplyr\", ref = \"main\")"
  },
  {
    "objectID": "published/window-functions.html#sec-retail-data",
    "href": "published/window-functions.html#sec-retail-data",
    "title": "Responsive open-source software: Two examples from dbplyr",
    "section": "3.1 The retail sales data set",
    "text": "3.1 The retail sales data set\nThe first query I will use to illustrate the use of window functions with mutate() comes from Chapter 3 of SQL for Data Analysis, which uses data on retail sales by industry in the United States to explore ideas on time-series analysis.\nI will use DuckDB as my database engine. Creating a DuckDB database requires just one line of code:\n\ndb &lt;- dbConnect(duckdb::duckdb())\n\nI then call get_data() to load the data into the database. I name the table (\"retail_sales\") so that I can refer to it when using SQL.5\n\nretail_sales &lt;- get_data(db, \n                         dir = \"Chapter 3: Time Series Analysis\",\n                         file = \"us_retail_sales.csv\",\n                         name = \"retail_sales\")\n\nThe SQL version of the query provided in SQL for Data Analysis is as follows:\n\nSELECT sales_month,\n  avg(sales) OVER w AS moving_avg,\n  count(sales) OVER w AS records_count\nFROM retail_sales\nWHERE kind_of_business = 'Women''s clothing stores'\nWINDOW w AS (ORDER BY sales_month \n             ROWS BETWEEN 11 PRECEDING AND CURRENT ROW)\nORDER BY sales_month DESC;\n\n\nDisplaying records 1 - 10\n\n\nsales_month\nmoving_avg\nrecords_count\n\n\n\n\n2020-12-01\n2210.500\n12\n\n\n2020-11-01\n2301.917\n12\n\n\n2020-10-01\n2395.583\n12\n\n\n2020-09-01\n2458.583\n12\n\n\n2020-08-01\n2507.417\n12\n\n\n2020-07-01\n2585.667\n12\n\n\n2020-06-01\n2659.667\n12\n\n\n2020-05-01\n2763.417\n12\n\n\n2020-04-01\n2989.083\n12\n\n\n2020-03-01\n3248.167\n12\n\n\n\n\n\nTo do the same using dbplyr, I can just specify .order and .frame in the call to mutate().\n\nmvg_avg &lt;-\n  retail_sales |&gt;\n  filter(kind_of_business == \"Women's clothing stores\") |&gt;\n  mutate(moving_avg = mean(sales, na.rm = TRUE),\n         records_count = n(),\n         .order = sales_month,\n         .frame = c(-11, 0)) |&gt;\n  select(sales_month, moving_avg, records_count) |&gt;\n  arrange(desc(sales_month))\n\nAs can be seen in Table 1, the resulting data set is the same.6\n\nmvg_avg |&gt;\n  collect(n = 10)\n\n\n\nTable 1: Moving average sales for women’s clothing store (first 10 records)\n\n\n\n\n\n\nsales_month\nmoving_avg\nrecords_count\n\n\n\n\n2020-12-01\n2210.500\n12\n\n\n2020-11-01\n2301.917\n12\n\n\n2020-10-01\n2395.583\n12\n\n\n2020-09-01\n2458.583\n12\n\n\n2020-08-01\n2507.417\n12\n\n\n2020-07-01\n2585.667\n12\n\n\n2020-06-01\n2659.667\n12\n\n\n2020-05-01\n2763.417\n12\n\n\n2020-04-01\n2989.083\n12\n\n\n2020-03-01\n3248.167\n12"
  },
  {
    "objectID": "published/window-functions.html#the-legislators-data",
    "href": "published/window-functions.html#the-legislators-data",
    "title": "Responsive open-source software: Two examples from dbplyr",
    "section": "3.2 The legislators data",
    "text": "3.2 The legislators data\nAnother data set I will use to illustrate the use of mutate() is the legislators data set used in Chapter 4 of SQL for Data Analysis to explore cohort analysis. The legislators data set comprises two tables, which I read into my DuckDB database using the following code.\n\nlegislators_terms &lt;- get_data(db, \n                         dir = \"Chapter 4: Cohorts\",\n                         file = \"legislators_terms.csv\",\n                         name = \"legislators_terms\")\n\nlegislators &lt;- get_data(db, \n                         dir = \"Chapter 4: Cohorts\",\n                         file = \"legislators.csv\",\n                         name = \"legislators\")\n\nA third data set used in Chapter 4 of SQL for Data Analysis is the year_ends table, which I construct in R and copy to my DuckDB database using the following code.7\n\nyear_ends &lt;-\n  tibble(date = seq(as.Date('1770-12-31'), \n                    as.Date('2030-12-31'), \n                    by = \"1 year\")) |&gt;\n  copy_to(db, df = _, overwrite = TRUE, name = \"year_ends\")\n\nFinally, I add a minor tweak to original query by adding an enumerated data type that ensures the tables are ordered meaningfully.8\n\nCREATE TYPE band AS ENUM ('1 to 4', '5 to 10', '11 to 20', '21+')\n\nThe following is a modified version of the SQL query found on page 173 of Chapter 4 of SQL for Data Analysis.9 As can be seen, because of how we defined the band data type, it is meaningful to sort by tenure (check what happens if you omit the casting of tenure to type band using ::band).\n\nWITH term_dates AS (\n  SELECT DISTINCT a.id_bioguide, b.date\n  FROM legislators_terms a\n  JOIN year_ends b \n  ON b.date BETWEEN a.term_start AND a.term_end \n    AND b.date &lt;= '2020-01-01'),\n\ncum_term_dates AS (\n  SELECT id_bioguide, date,\n    count(date) OVER w AS cume_years\n  FROM term_dates\n  WINDOW w AS (PARTITION BY id_bioguide \n               ORDER BY date \n               ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)),\n  \ncum_term_bands AS (\n  SELECT date,\n    CASE WHEN cume_years &lt;= 4 THEN '1 to 4'\n         WHEN cume_years &lt;= 10 THEN '5 to 10'\n         WHEN cume_years &lt;= 20 THEN '11 to 20'\n         ELSE '21+' END AS tenure,\n    COUNT(DISTINCT id_bioguide) AS legislators\n  FROM cum_term_dates\n  GROUP BY 1,2)\n  \nSELECT date, tenure::band AS tenure,\n  legislators * 100.0 / sum(legislators) OVER w AS pct_legislators \nFROM cum_term_bands\nWINDOW w AS (partition by date)\nORDER BY date DESC, tenure;\n\n\nDisplaying records 1 - 10\n\n\ndate\ntenure\npct_legislators\n\n\n\n\n2019-12-31\n1 to 4\n29.98138\n\n\n2019-12-31\n5 to 10\n32.02980\n\n\n2019-12-31\n11 to 20\n20.11173\n\n\n2019-12-31\n21+\n17.87710\n\n\n2018-12-31\n1 to 4\n25.60297\n\n\n2018-12-31\n5 to 10\n33.76623\n\n\n2018-12-31\n11 to 20\n21.33581\n\n\n2018-12-31\n21+\n19.29499\n\n\n2017-12-31\n1 to 4\n24.53532\n\n\n2017-12-31\n5 to 10\n34.75836\n\n\n\n\n\nTranslating the query to dbplyr is greatly facilitated by the use of CTEs, as each CTE can be constructed as a separate remote or lazy data frame. Here remote means that the data are in a database. In this case, “remote” does not mean “far away”, but the data could be physically distant as in the case of the dsf data frame examined above. The term “lazy” refers to the fact that the underlying SQL query for the data frame is not executed until we ask it to be evaluated using functions like collect() (to bring the data into R) or compute() (to create a temporary table in the database).\n\nterm_dates &lt;-\n  legislators_terms |&gt;\n  inner_join(year_ends |&gt; filter(date &lt;= '2020-01-01'),\n             join_by(between(y$date, x$term_start, x$term_end))) |&gt;\n  distinct(id_bioguide, date) \n\nHere I use .order, .frame, and .by to get the same window used in the SQL above.\n\ncum_term_dates &lt;-\n  term_dates |&gt;\n  mutate(cume_years = n(),\n         .by = id_bioguide,\n         .order = date,\n         .frame = c(-Inf, 0)) |&gt;\n  select(id_bioguide, date, cume_years) \n\nThe following query aggregates the data into different ranges of tenure. Note that a glitch in n_distinct() in the duckdb package (presumably something that will be fixed soon enough) means that I need to directly call SQL COUNT(DISTINCT id_bioguide) as can be seen in the code below.\n\ncum_term_bands &lt;-\n  cum_term_dates |&gt; \n  mutate(tenure = case_when(cume_years &lt;= 4 ~ '1 to 4',\n                            cume_years &lt;= 10 ~ '5 to 10',\n                            cume_years &lt;= 20 ~ '11 to 20',\n                            TRUE ~ '21+')) |&gt;\n  mutate(tenure = sql(\"tenure::band\")) |&gt;\n  summarize(legislators = sql(\"COUNT(DISTINCT id_bioguide)\"),\n            .by = c(date, tenure))\n\nThe final query pulls everything together and uses a window function to count the number of legislators in the denominator of pct_legislators. As can be seen in Table 2, the resulting data set is the same as that produced by the SQL above.\n\ncum_term_bands |&gt;\n  mutate(sum_legislators = sum(legislators),\n         pct_legislators = legislators * 100.0 / sum_legislators,\n         .by = date) |&gt;\n  select(date, tenure, pct_legislators) |&gt;\n  arrange(desc(date), tenure) |&gt;\n  collect(n = 10)\n\n\n\nTable 2: Percentage of legislators by tenure (first 10 records)\n\n\n\n\n\n\ndate\ntenure\npct_legislators\n\n\n\n\n2019-12-31\n1 to 4\n29.981\n\n\n2019-12-31\n5 to 10\n32.030\n\n\n2019-12-31\n11 to 20\n20.112\n\n\n2019-12-31\n21+\n17.877\n\n\n2018-12-31\n1 to 4\n25.603\n\n\n2018-12-31\n5 to 10\n33.766\n\n\n2018-12-31\n11 to 20\n21.336\n\n\n2018-12-31\n21+\n19.295\n\n\n2017-12-31\n1 to 4\n24.535\n\n\n2017-12-31\n5 to 10\n34.758\n\n\n\n\n\n\n\n\nAgain, the new functionality supports powerful analyses with clean, easy-to-read code."
  },
  {
    "objectID": "published/window-functions.html#footnotes",
    "href": "published/window-functions.html#footnotes",
    "title": "Responsive open-source software: Two examples from dbplyr",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nRun install.packages(c(\"farr\", \"dplyr\", \"DBI\", \"duckdb\")) within R to install all the packages you need to run the code in this note.↩︎\nThe “go-ahead” seems to be implied by Hadley’s reopening of the issue on 24 June 2021.↩︎\nSee Chapter 17 of Empirical Research in Accounting: Tools and Methods for more on the michels_2017 data. Of course, a more careful approach would probably use trading days before and after events; see Chapter 12 of Empirical Research in Accounting: Tools and Methods for discussion of how to do this.↩︎\nI have written about this book here.↩︎\nIt is not necessary to specify a table name if we are just using dbplyr to analyse the data.↩︎\nThis makes sense as the dplyr/dbplyr code is translated into SQL behind the scenes.↩︎\nIn Chapter 4 of SQL for Data Analysis, year_ends is created using SQL in the relevant dialect, which is PostgreSQL in the book.↩︎\nThis data type is similar to factors in R, a topic covered in Chapter 2 of Empirical Research in Accounting: Tools and Methods.↩︎\nApart from formatting changes, the main modification I made to the query was the use of common-table expressions (CTEs) in place of subqueries. I discuss the merits of CTEs (and of using dbplyr to write SQL) here.↩︎"
  }
]