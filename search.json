[
  {
    "objectID": "published/yahoo-returns.html",
    "href": "published/yahoo-returns.html",
    "title": "Stock returns on Yahoo Finance",
    "section": "",
    "text": "There appear to be few easy-to-use sources of free stock price data out there, but one venerable source is Yahoo Finance, which was a source of such data before Google was even a twinkle in the eye of Larry Page and Sergey Brin. So you may have used Yahoo Finance yourself to calculate stock returns. Yahoo Finance offers two versions of daily closing prices: close (“close price adjusted for splits”) and adjusted (“adjusted for splits and dividend and/or capital gain distributions”). If I denote close and adjusted on date \\(t\\) as \\(c_t\\) and \\(a_t\\), respectively, you likely calculated returns as\n\\[ r_t = \\frac{a_t}{a_{t-1}} - 1 \\] And denoting dividends (including capital gain distributions) on date \\(t\\) as \\(d_t\\), you likely figured that the above was equivalent to the standard formula:\n\\[ r_t = \\frac{c_t + d_t}{c_{t-1}} - 1 \\] But I have discovered that this is not true. Instead, the adjusted stock price is calculated so that returns are calculated using the following expression:\n\\[ r_t = \\frac{a_t}{a_{t-1}} - 1 =  \\frac{c_t}{c_{t-1} - d_t} - 1 \\] I’m guessing that many finance experts would regard the latter formula as simply wrong. Interestingly, Investopedia suggests that one adjusts stock prices for dividends in precisely the way implied by the Yahoo Finance calculation: that is, you adjust \\(c_{t-1}\\) by subtracting \\(d_t\\) from it.1 I’m inclined to label it as unorthodox rather than simply wrong. While the good news is that the differences are not large, I wonder how many realize that this is how Yahoo Finance is doing things.\nIn effect, the standard calculation assumes that you buy for cash at close one day and sell the next day, getting the proceeds of the sale and the associated dividends at that time. In contrast, the Yahoo Finance calculation assumes that you only need to supply cash to buy the shares at close one day net of dividends and sell the next day. The former seems a bit easier to describe and perhaps to pull off. Try asking your broker if you can do the latter! Of course, there’s a bit of fiction in all these calculations with trades at closing prices and dividends being paid on the ex-dividend date, but they are useful benchmark.\nI think this case illustrates the reality that many data items are not well-documented and, even if they are, it makes sense to check that the data line up with the documentation."
  },
  {
    "objectID": "published/yahoo-returns.html#comparing-with-crsp-returns",
    "href": "published/yahoo-returns.html#comparing-with-crsp-returns",
    "title": "Stock returns on Yahoo Finance",
    "section": "2.1 Comparing with CRSP returns",
    "text": "2.1 Comparing with CRSP returns\nThe Center for Research in Security Prices, LLC (CRSP) is the de facto standard source of stock returns for US stocks in academic finance.4 I have a significant subset of CRSP in a repository of parquet files along the lines described in Appendix E of Empirical Research in Accounting: Tools and Methods.\nHere I collect data from the CRSP daily stock file (crsp.dsf) for Coca-Cola and store it in crsp_rets_ko, a remote data frame.5\n\ndb &lt;- dbConnect(duckdb::duckdb())\n\nstocknames &lt;- load_parquet(db, \"stocknames\", \"crsp\")\ndsf &lt;- load_parquet(db, \"dsf\", \"crsp\")\n\ncrsp_rets_ko &lt;- \n  stocknames |&gt;\n  filter(ticker == \"KO\") |&gt; \n  distinct(permno) |&gt; \n  inner_join(dsf, by = \"permno\") |&gt;\n  select(permno, date, prc, ret)\n\nThe standard measure of returns on CRSP is ret and I compare this with ret_yahoo (calculated using adjusted stock prices) and with ret_std, calculated using the “standard” formula discussed above. As can be seen below, the CRSP calculation (ret) and the standard formula (ret_std) line up pretty much perfectly.\n\ncrsp_rets_ko |&gt;\n  inner_join(ko_rets, by = \"date\", copy = TRUE) |&gt;\n  mutate(ret_std = (close + adj_amt) / lag_close - 1) |&gt;\n  filter(adj_amt != 0) |&gt;\n  select(date, ret_yahoo, ret_std, ret) |&gt;\n  arrange(desc(date)) |&gt;\n  collect()\n\n# A tibble: 32 × 4\n   date        ret_yahoo    ret_std       ret\n   &lt;date&gt;          &lt;dbl&gt;      &lt;dbl&gt;     &lt;dbl&gt;\n 1 2024-11-29  0.002111   0.002095   0.002095\n 2 2024-09-13  0.009400   0.009336   0.009336\n 3 2024-06-14  0.0007200  0.0007144  0.000714\n 4 2024-03-14 -0.002226  -0.002209  -0.002209\n 5 2023-11-30  0.01160    0.01151    0.01151 \n 6 2023-09-14  0.008279   0.008214   0.008214\n 7 2023-06-15  0.01374    0.01364    0.01364 \n 8 2023-03-16  0.005503   0.005461   0.005461\n 9 2022-11-30  0.02531    0.02513    0.02513 \n10 2022-09-15 -0.01359   -0.01349   -0.01349 \n# ℹ 22 more rows"
  },
  {
    "objectID": "published/yahoo-returns.html#footnotes",
    "href": "published/yahoo-returns.html#footnotes",
    "title": "Stock returns on Yahoo Finance",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis approach to calculating adjusted stock price works only on the day the ex-dividend date when the previous values of close and adjusted are equal.↩︎\nExecute install.packages(c(\"tidyquant\", \"tidyverse\", \"farr\", \"DBI\", \"duckdb\", \"dbplyr\") within R to install all the packages you need to run the code in this note.↩︎\nNote that I use round(., 4) to eliminate quirky issues related to less-significant digits with double-precision numbers.↩︎\nSee Section 7.2 of Empirical Research in Accounting: Tools and Methods for more on CRSP.↩︎\nFor more on remote data frames see Chapter 6 of Empirical Research in Accounting: Tools and Methods.↩︎"
  },
  {
    "objectID": "published/weather.html",
    "href": "published/weather.html",
    "title": "Defining winter and summer in Melbourne",
    "section": "",
    "text": "Figure 1: Average daily temperatures for 91 days following indicated date for period 2001–2023\nIn the United States, one often hears people speak of the “official” start of seasons. Ironically, there seems to be nothing that is official about these dates. However, there is consensus about the dates in the US. The “official” start of summer is the summer solstice (for 2024: 21 December in Melbourne, 20 June in Boston) and the “official” start of winter is the winter solstice (for 2024: 21 June in Melbourne, 21 December in Boston).4\nIn Australia, the usual convention is to divide seasons by months. On this basis, winter starts on 1 June and summer starts on 1 December.5\nIs there a sense in which one approach is more correct than the other? Focusing on summer and winter, one definition for these seasons would be that winter starts on the first day of the 91-day period that is the coldest such period for a year averaged over a number of years. Similarly, summer should starts on the first day of the 91-day period that is the hottest such period for a year averaged over a number of years.\nWe answer this question focusing on Melbourne, Australia (latitude of -37.814, longitude: 144.96332).\nDaily temperature data from Open-Meteo comprise a maximum and minimum temperature. So immediately we have two possible definitions of each season according to the temperature we use (e.g., summer could be the 91-day period that has the highest average minimum temperature or it could be the period that has the highest average maximum temperature). Here we consider both.\nThe start of winter based on the 91-day period with the lowest average maximum temperature is 27 May. The start of winter based on the 91-day period with the lowest average minimum temperature is 09 June.\nThe start of summer based on the 91-day period with the highest average maximum temperature is 19 December. The start of summer based on the 91-day period with the highest average minimum temperature is 24 December. So using maximums, we get close to the Australian convention for winter and close to the US convention for summer.\nInterestingly, it seems that using average maximums for summer and winter gets closest to the current approach in Australia. However, even using these we have the issue that spring begins on 26 August and autumn begins on 20 March. This implies a spring of 115 days and an autumn of 68 days."
  },
  {
    "objectID": "published/weather.html#footnotes",
    "href": "published/weather.html#footnotes",
    "title": "Defining winter and summer in Melbourne",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nUniversity of Melbourne, ian.gow@unimelb.edu.au↩︎\nUniversity of Melbourne, ian.gow@unimelb.edu.au↩︎\nUniversity of Melbourne, ian.gow@unimelb.edu.au↩︎\nSeasons reckoned in this way are known as astronomical seasons. See here.↩︎\nSeasons reckoned in this way are known as meteorological seasons. See here.↩︎"
  },
  {
    "objectID": "published/trading_dates.html",
    "href": "published/trading_dates.html",
    "title": "Trading days per year (crsp.dsf)",
    "section": "",
    "text": "The conventional notion is that there are (on average) about 252 trading days per year on US stock exchanges. We don’t have to accept this, as we can use data on CRSP’s daily stock file (crsp.dsf) to count trading dates per year.\nIn this note, we examine the number of trading days in each year using date from crsp.dsf. To start, we calculate n_days, the number of trading days in that year, for each year.\nFrom Table 1, we see that 1925 is clearly an odd year (and we will exclude it from subsequent analysis). Also, 1968 is an outlier.\nTable 1: Years with the fewest trading dates\n\n\n\n\n\n\nyear\nn_days\n\n\n\n\n1925\n1\n\n\n1968\n226\n\n\n2001\n248\n\n\n1961\n250\n\n\n1969\n250\nFrom Table 2, it can be seen that there is a cluster of years with between 251 and 253 trading days.\nTable 2: Years with days between 249 and 260\n\n\n\n\n\n\nn_days\nn\n\n\n\n\n250\n4\n\n\n251\n10\n\n\n252\n32\n\n\n253\n21\n\n\n254\n3\nBut, looking at Figure 1, we can see an unexpected cluster of years with more than 280 trading days.\nFigure 1: Distribution of number of trading days per year\nIn Figure 2, we see that the years with an unexpectedly high number of trading days are in the earlier part of the sample.\nFigure 2: Trading days per year over time\nFrom Figure 4 we learn that some trading days are actually Saturdays. We can do a version of Figure 2 that includes information about the days of the week. As we can see in Figure 3, the Saturdays are in the earlier part of the sample. It also seems that the “issue” with 1968 is concentrated in Wednesdays.\nFigure 3: Trading days per year over time with days of the week\nAccording to tradinghours.com, “in May 1887, the trading hours were officially set to Monday to Friday 10am to 3pm and Saturday from 10am to noon. … In [September] 1952, the Saturday trading session was finally retired.”\nFigure 4: Trading days: Days of the week\nWe can identify “missing” dates in 1968 by doing an anti_join() of a table of non-weekend dates with the list of trading dates.1 It turns out that a crisis in managing trading volumes known as the “paperwork crisis” forced the NYSE to restrict trading to four days a week. According to Market Memoir, “for months the exchange closed on Wednesdays, and sometimes needed to close early on other days to give firms additional time to combat severe backlogs.” The missing Wednesdays are quite apparent in Figure 5.\nFigure 5: Weekdays of dates ‘missing’ from 1968 data"
  },
  {
    "objectID": "published/trading_dates.html#footnotes",
    "href": "published/trading_dates.html#footnotes",
    "title": "Trading days per year (crsp.dsf)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote that some of these “missing” dates would be public holidays.↩︎"
  },
  {
    "objectID": "published/sas_to_pd.html",
    "href": "published/sas_to_pd.html",
    "title": "Using SAS to create pandas data",
    "section": "",
    "text": "A strong point of pandas is its expressiveness. Its API allows users to explore data using succinct and (generally) intuitive code. However, some of this expressiveness relies on data being in forms (for example, with dates ready to serve as an index) that often differ from the data we have, and pandas can struggle to manipulate the data into those forms, especially with larger data sets.\nSAS might be another approach to manipulating data for pandas. My Python package wrds2pg offers a sas_to_pandas() function that can run code on the WRDS server and return the results as a pandas dataframe. While not quite as fast as using Ibis with the PostgreSQL server, SAS performs pretty well with this task.\n\n\n\n\n\n\nTip\n\n\n\nThe following command (run in the terminal on your computer) installs the packages you need.\n\npip install wrds2pg --upgrade\npip install pandas\n\nThe code assumes you have set the environment variable WRDS_ID to your WRDS ID.\nThis note was written using Quarto. The source code for this note is available here and the latest version of this PDF is here."
  },
  {
    "objectID": "published/gino-colada.html",
    "href": "published/gino-colada.html",
    "title": "The Gino-Colada Affair",
    "section": "",
    "text": "We can reproduce the equivalent of the published results of an ANOVA with the three conditions as categorical predictor variable and deductions as outcome variable using linear regression. Results are reported in column (1) of Table 1. In addition, the original article reported that each difference between the experimental “signature-on-top” and the two control conditions (“signature-on-bottom”, “no signature”) was significant. This is confirmed in columns (2) and (3) of Table 1.\nNext, we can repeat the analysis without rows 67 to 72. Results are reported in Table 2. Without the six contested cases, the results are no longer statistically significant, \\(F( 2, 92) = 2.96\\), \\(p = 0.057\\), as seen in column (1) of Table 2. The comparisons of the experimental group with the two control groups were also statistically significant (see columns (2) and (3) of Table 2). Combining the two control groups into one and comparing it to the experimental group and presenting the results as a planned contrast would also have produced a significant result (see column (4) of Table 2).\nOf course, the accusation is that she switched rows with low values to the experimental condition and rows with high values to the control condition. To attempt to reverse this manipulation, we can recode the contested rows 67–69 as signature-at-the-bottom and 70–72 as signature-at-the-top and repeat the analysis. In this case, there was no evidence that the group means differed from each other, \\(F( 2, 98) = 0.454\\), \\(p = 0.637\\). Results are presented in Column (1) of Table 3. Neither comparison of the experimental group with each of the two control groups was statistically significant (see columns (2) and (3) of Table 3).\n\n\n\n\nTable 1: Reproduction of results from the paper\n\n\n\n\n\n    \n\n    \n\n    \n    \n    \n      \n        \n        \n              \n                 \n                (1)\n                (2)\n                (3)\n              \n        \n        \n        \n                \n                  (Intercept)\n                  8.445\n                  5.271\n                  8.445\n                \n                \n                  \n                  (0.966)\n                  (0.906)\n                  (0.906)\n                \n                \n                  Cond1\n                  -3.174\n                  \n                  -3.174\n                \n                \n                  \n                  (1.346)\n                  \n                  (1.263)\n                \n                \n                  Cond2\n                  1.179\n                  4.353\n                  \n                \n                \n                  \n                  (1.366)\n                  (1.300)\n                  \n                \n                \n                  F\n                  5.633\n                  11.203\n                  6.312\n                \n                \n                  p\n                  0.005\n                  0.001\n                  0.014\n                \n        \n      \n    \n\n\n\n\n\n\n\n\n\n\nTable 2: Reproduction of results without disputed observations\n\n\n\n\n\n    \n\n    \n\n    \n    \n    \n      \n        \n        \n              \n                 \n                (1)\n                (2)\n                (3)\n                (4)\n              \n        \n        \n        \n                \n                  (Intercept)\n                  8.445\n                  5.703\n                  8.445\n                  8.417\n                \n                \n                  \n                  (0.895)\n                  (0.827)\n                  (0.909)\n                  (0.645)\n                \n                \n                  Cond1\n                  -2.742\n                  \n                  -2.742\n                  \n                \n                \n                  \n                  (1.276)\n                  \n                  (1.296)\n                  \n                \n                \n                  Cond2\n                  -0.059\n                  2.684\n                  \n                  \n                \n                \n                  \n                  (1.297)\n                  (1.189)\n                  \n                  \n                \n                \n                  Cond == 1TRUE\n                  \n                  \n                  \n                  -2.714\n                \n                \n                  \n                  \n                  \n                  \n                  (1.110)\n                \n                \n                  F\n                  2.956\n                  5.098\n                  4.478\n                  5.975\n                \n                \n                  p\n                  0.057\n                  0.028\n                  0.038\n                  0.016\n                \n        \n      \n    \n\n\n\n\n\n\n\n\n\n\nTable 3: Reproduction of results with corrected data\n\n\n\n\n\n    \n\n    \n\n    \n    \n    \n      \n        \n        \n              \n                 \n                (1)\n                (2)\n                (3)\n              \n        \n        \n        \n                \n                  (Intercept)\n                  8.445\n                  7.100\n                  8.445\n                \n                \n                  \n                  (1.015)\n                  (0.979)\n                  (1.062)\n                \n                \n                  Cond1\n                  -1.345\n                  \n                  -1.345\n                \n                \n                  \n                  (1.415)\n                  \n                  (1.480)\n                \n                \n                  Cond2\n                  -0.761\n                  0.585\n                  \n                \n                \n                  \n                  (1.436)\n                  (1.405)\n                  \n                \n                \n                  F\n                  0.454\n                  0.173\n                  0.826\n                \n                \n                  p\n                  0.637\n                  0.679\n                  0.367"
  },
  {
    "objectID": "published/gino-colada.html#study-1",
    "href": "published/gino-colada.html#study-1",
    "title": "The Gino-Colada Affair",
    "section": "",
    "text": "We can reproduce the equivalent of the published results of an ANOVA with the three conditions as categorical predictor variable and deductions as outcome variable using linear regression. Results are reported in column (1) of Table 1. In addition, the original article reported that each difference between the experimental “signature-on-top” and the two control conditions (“signature-on-bottom”, “no signature”) was significant. This is confirmed in columns (2) and (3) of Table 1.\nNext, we can repeat the analysis without rows 67 to 72. Results are reported in Table 2. Without the six contested cases, the results are no longer statistically significant, \\(F( 2, 92) = 2.96\\), \\(p = 0.057\\), as seen in column (1) of Table 2. The comparisons of the experimental group with the two control groups were also statistically significant (see columns (2) and (3) of Table 2). Combining the two control groups into one and comparing it to the experimental group and presenting the results as a planned contrast would also have produced a significant result (see column (4) of Table 2).\nOf course, the accusation is that she switched rows with low values to the experimental condition and rows with high values to the control condition. To attempt to reverse this manipulation, we can recode the contested rows 67–69 as signature-at-the-bottom and 70–72 as signature-at-the-top and repeat the analysis. In this case, there was no evidence that the group means differed from each other, \\(F( 2, 98) = 0.454\\), \\(p = 0.637\\). Results are presented in Column (1) of Table 3. Neither comparison of the experimental group with each of the two control groups was statistically significant (see columns (2) and (3) of Table 3).\n\n\n\n\nTable 1: Reproduction of results from the paper\n\n\n\n\n\n    \n\n    \n\n    \n    \n    \n      \n        \n        \n              \n                 \n                (1)\n                (2)\n                (3)\n              \n        \n        \n        \n                \n                  (Intercept)\n                  8.445\n                  5.271\n                  8.445\n                \n                \n                  \n                  (0.966)\n                  (0.906)\n                  (0.906)\n                \n                \n                  Cond1\n                  -3.174\n                  \n                  -3.174\n                \n                \n                  \n                  (1.346)\n                  \n                  (1.263)\n                \n                \n                  Cond2\n                  1.179\n                  4.353\n                  \n                \n                \n                  \n                  (1.366)\n                  (1.300)\n                  \n                \n                \n                  F\n                  5.633\n                  11.203\n                  6.312\n                \n                \n                  p\n                  0.005\n                  0.001\n                  0.014\n                \n        \n      \n    \n\n\n\n\n\n\n\n\n\n\nTable 2: Reproduction of results without disputed observations\n\n\n\n\n\n    \n\n    \n\n    \n    \n    \n      \n        \n        \n              \n                 \n                (1)\n                (2)\n                (3)\n                (4)\n              \n        \n        \n        \n                \n                  (Intercept)\n                  8.445\n                  5.703\n                  8.445\n                  8.417\n                \n                \n                  \n                  (0.895)\n                  (0.827)\n                  (0.909)\n                  (0.645)\n                \n                \n                  Cond1\n                  -2.742\n                  \n                  -2.742\n                  \n                \n                \n                  \n                  (1.276)\n                  \n                  (1.296)\n                  \n                \n                \n                  Cond2\n                  -0.059\n                  2.684\n                  \n                  \n                \n                \n                  \n                  (1.297)\n                  (1.189)\n                  \n                  \n                \n                \n                  Cond == 1TRUE\n                  \n                  \n                  \n                  -2.714\n                \n                \n                  \n                  \n                  \n                  \n                  (1.110)\n                \n                \n                  F\n                  2.956\n                  5.098\n                  4.478\n                  5.975\n                \n                \n                  p\n                  0.057\n                  0.028\n                  0.038\n                  0.016\n                \n        \n      \n    \n\n\n\n\n\n\n\n\n\n\nTable 3: Reproduction of results with corrected data\n\n\n\n\n\n    \n\n    \n\n    \n    \n    \n      \n        \n        \n              \n                 \n                (1)\n                (2)\n                (3)\n              \n        \n        \n        \n                \n                  (Intercept)\n                  8.445\n                  7.100\n                  8.445\n                \n                \n                  \n                  (1.015)\n                  (0.979)\n                  (1.062)\n                \n                \n                  Cond1\n                  -1.345\n                  \n                  -1.345\n                \n                \n                  \n                  (1.415)\n                  \n                  (1.480)\n                \n                \n                  Cond2\n                  -0.761\n                  0.585\n                  \n                \n                \n                  \n                  (1.436)\n                  (1.405)\n                  \n                \n                \n                  F\n                  0.454\n                  0.173\n                  0.826\n                \n                \n                  p\n                  0.637\n                  0.679\n                  0.367"
  },
  {
    "objectID": "published/gino-colada.html#study-2",
    "href": "published/gino-colada.html#study-2",
    "title": "The Gino-Colada Affair",
    "section": "2 Study #2",
    "text": "2 Study #2\n\n\n\n\nTable 4: Study 2: Reproduction of results with OSF data\n\n\n\n\n\n    \n\n    \n\n    \n    \n    \n      \n        \n        \n              \n                 \n                Cheating\n                SumDeductions\n                SumEthicsWords\n              \n        \n        \n        \n                \n                  (Intercept)\n                  3.567\n                  7.063\n                  0.867\n                \n                \n                  \n                  (5.491)\n                  (7.266)\n                  (4.720)\n                \n                \n                  SignAtTop\n                  -1.900\n                  -3.830\n                  0.533\n                \n                \n                  \n                  (-2.068)\n                  (-2.786)\n                  (2.054)\n                \n                \n                  F\n                  4.279\n                  7.761\n                  4.218\n                \n                \n                  p\n                  0.043\n                  0.007\n                  0.045\n                \n        \n      \n    \n\n\n\n\n\n\nThe original results from the paper are reported in Table 4.\nFrom the calcChain.xml file, it appears that just three observations (P# values 1, 59, 61) have been moved “out of order” from “sign at the bottom” to “sign at the top”. These observations are (now) in rows 2, 60, and 61 of the OSF spreadsheet. It seems these changes involved moving a row from the bottom to the top and two rows from the top to the bottom.\n&lt;c r=\"I58\" i=\"1\"/&gt;\n&lt;c r=\"K58\" i=\"1\"/&gt;\n&lt;c r=\"I59\" i=\"1\"/&gt;\n&lt;c r=\"K59\" i=\"1\"/&gt;\n&lt;c r=\"I2\" i=\"1\"/&gt;\n&lt;c r=\"K2\" i=\"1\"/&gt;\nand\n&lt;c r=\"I60\" i=\"1\"/&gt;\n&lt;c r=\"K60\" i=\"1\"/&gt;\n&lt;c r=\"I61\" i=\"1\"/&gt;\n&lt;c r=\"K61\" i=\"1\"/&gt;\n&lt;c r=\"I3\" i=\"1\"/&gt;\n&lt;c r=\"K3\" i=\"1\"/&gt;\n&lt;c r=\"I4\" i=\"1\"/&gt;\nLet’s see what happens if we move it back? Results are reported in Table 5.\n\n\n\n\nTable 5: Study 2: Reproduction of results with corrected data\n\n\n\n\n\n    \n\n    \n\n    \n    \n    \n      \n        \n        \n              \n                 \n                Cheating\n                SumDeductions\n                SumEthicsWords\n              \n        \n        \n        \n                \n                  (Intercept)\n                  3.375\n                  6.950\n                  0.906\n                \n                \n                  \n                  (5.314)\n                  (7.390)\n                  (5.066)\n                \n                \n                  SignAtTop\n                  -1.625\n                  -3.861\n                  0.487\n                \n                \n                  \n                  (-1.748)\n                  (-2.804)\n                  (1.858)\n                \n                \n                  F\n                  3.055\n                  7.864\n                  3.453\n                \n                \n                  p\n                  0.086\n                  0.007\n                  0.068"
  },
  {
    "objectID": "published/delisting.html",
    "href": "published/delisting.html",
    "title": "Adding delisting returns to monthly data",
    "section": "",
    "text": "This short note demonstrates how to convert SAS code provided as delistings.sas by Richard Price here. This code is presumably closely related to code used in Beaver et al. (2007)."
  },
  {
    "objectID": "published/delisting.html#introduction",
    "href": "published/delisting.html#introduction",
    "title": "Adding delisting returns to monthly data",
    "section": "",
    "text": "This short note demonstrates how to convert SAS code provided as delistings.sas by Richard Price here. This code is presumably closely related to code used in Beaver et al. (2007)."
  },
  {
    "objectID": "published/delisting.html#setting-up-tables",
    "href": "published/delisting.html#setting-up-tables",
    "title": "Adding delisting returns to monthly data",
    "section": "2 Setting up tables",
    "text": "2 Setting up tables\nIn the R code that folows, we will use a number of packages, including pacakges to connect to a PostgreSQL database containing WRDS data. Instructions for this can be found here.1\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(broom)\nlibrary(DBI)\nlibrary(dbplyr)\nlibrary(purrr)\n\n\npg &lt;- dbConnect(RPostgres::Postgres(), bigint = \"integer\")\n\nWe will use three tables. The data on “regular” returns come from crsp.msf. The tables crsp.mse and crsp.dsedelist are used for delisting returns.\n\ncrsp.msf &lt;- tbl(pg, sql(\"SELECT * FROM crsp.msf\"))\ncrsp.mse &lt;- tbl(pg, sql(\"SELECT * FROM crsp.mse\"))\ncrsp.dsedelist &lt;- tbl(pg, sql(\"SELECT * FROM crsp.dsedelist\"))"
  },
  {
    "objectID": "published/delisting.html#the-dataset-with-monthly-return-data",
    "href": "published/delisting.html#the-dataset-with-monthly-return-data",
    "title": "Adding delisting returns to monthly data",
    "section": "3 The dataset with monthly return data",
    "text": "3 The dataset with monthly return data\nHere we get a table of “regular” returns, we will incorporate delisting returns in the returns from this table. The code includes “an arbitrary restriction of the sample for illustration purposes.” The data are restricted to 2003 and permno values less than 12000.\nproc sql;\n    create table monthlyreturns as\n    select permno, date, ret\n    from crsp.msf\n    where year(date)=2003 and permno&lt;12000;  \n\nmonthlyreturns &lt;-\n  crsp.msf %&gt;%\n  filter(year(date) == 2003, permno &lt; 12000) %&gt;%\n    select(permno, date, ret)"
  },
  {
    "objectID": "published/delisting.html#the-monthly-delisting-dataset",
    "href": "published/delisting.html#the-monthly-delisting-dataset",
    "title": "Adding delisting returns to monthly data",
    "section": "4 The monthly delisting dataset",
    "text": "4 The monthly delisting dataset\n\n4.1 Get the base data for delisting returns\nThe following code uses crsp.mse for the values of dlret. It’s not clear what this table provides that crsp.dsedelist does not, but using crsp.dsedelist here—renaming dlstdt as date to conform with later code—results in small differences in the results below.\nThe first portion of SAS code is easily translated to R.\ndata delist;\n  set crsp.mse;\n  where dlstcd &gt; 199;\n  keep permno date dlstcd dlpdt dlret;\nrun;\n\ndelist &lt;-\n  crsp.mse %&gt;%\n  filter(dlstcd &gt; 199) %&gt;%\n  select(permno, date, dlstcd, dlpdt, dlret)\n\n\n\n4.2 Calculate replacement values\nCompute replacement values for missing delisting returns using daily delisting returns. Richard says modify year range as needed, but I drop this filter in the R code, as it makes little difference to performance and (surprisingly) no difference to the results.\nproc sql;\n  create table rvtemp as\n    select * from crsp.dsedelist\n    where dlstcd &gt; 199 and 1960 le year(DATE) le 2020\n    order by dlstcd;\nI omit the step of creating rvtemp, as it’s easy enough to include a single line of code in creating rv below.\nThe following code calculates the mean values of dlret by dlstcd. Later will use these values to fill missing values of dlret. Richard says in a comment “could use median=median_dlret and probm=median_pvalue if you do not like mean delisting returns as the replacement value.”\nproc univariate data=rvtemp noprint;\n    var dlret;\n    output out=rv mean=mean_dlret probt=mean_pvalue;\n    by dlstcd;\nrun;\n\n* require replacement values to be statistically significant;\ndata rv;\n    set rv;\n      * adjust p-value as desired;\n    if mean_pvalue le 0.05 then rv = mean_dlret; \n    else rv = 0; * adjust as desired;\n    keep dlstcd rv;\nrun;\nThe SAS code uses PROC UNIVARIATE. We just need a small function to return the \\(p\\)-value, which I call prt() to match the name of a similar function in SAS. We could use the t.test() function, but it’s easy enough to calculate the two-sided \\(p\\)-value ourselves.2\n\nprt &lt;- function(x) {\n  p &lt;- pt(mean(x) * sqrt(length(x)) / sd(x), length(x))\n  2 * pmin(p, 1 - p)\n}\n\nAs disussed I use filter(dlstcd &gt; 199) without between(year(dlstdt), 1960, 2020) because it makes no noticeable difference to performance or results. I call the resulting table rvs (“replacement values”) to distinguish it from the variable rv it contains. This makes no functional difference, but (to my mind) makes the code a little easier to read. Note that we bring data from PostgreSQL into R to calculate p_val and then return it to PostgreSQL using copy_inline(). An alternative might be to calculate \\(t\\)-statistics in PostgreSQL and just bring summary values into R to calculate \\(p\\)-values, but this approach works fine.\n\nrvs &lt;-\n  crsp.dsedelist %&gt;%\n  filter(dlstcd &gt; 199) %&gt;%\n  select(dlstcd, dlret) %&gt;%\n  filter(!is.na(dlret)) %&gt;%\n  collect() %&gt;% \n  group_by(dlstcd) %&gt;%\n  summarize(mean = mean(dlret),\n            p_val = prt(dlret),\n            .groups = \"drop\") %&gt;%\n  mutate(rv = if_else(p_val &lt;= 0.05, mean, 0)) %&gt;%\n  select(dlstcd, rv) %&gt;%\n  copy_inline(pg, .)\n\n\n\n4.3 Merge replacement values with delisting returns\nAgain the R code is a simple translation of the SAS code. One difference here is that I use a new table name delist_rv, as I find re-using table names to be confusing when debugging code (though no real debugging was required here because I am mimicking someone else’s code). Note that I add month and year variables, as we will use these to merge below data sets below.\nproc sql;\n    create table delist as\n    select a.*, b.rv\n    from delist a left join rv b\n    on a.dlstcd = b.dlstcd;\n\ndelist_rv &lt;- \n  delist %&gt;%\n  left_join(rvs, by = \"dlstcd\") %&gt;%\n  mutate(month = month(date),\n         year = year(date)) %&gt;%\n  rename(dldate = date)\n\n\n\n4.4 Creating a function\nOf course, all of the above could be easily be put into a function. Making functions in R is much easier than making macros in SAS.3 Note that if using the function, we could omit the code creating crsp.mse and crsp.dsedelist above. Optionally, we could easily use dates taken from crsp.msi in the returned table that we could merge with monthly returns without any need to create month and year fields. Ideally, we would also incorporate the steps to correct dlret covered in the next section in the code here.\nIt would be quite straightforward to add this function to my farr package.4 The only thing needed to be provided to the function is the PostgreSQL database connection (conn).\n\nget_delist &lt;- function(conn) {\n  crsp.mse &lt;- tbl(conn, sql(\"SELECT * FROM crsp.mse\"))\n  crsp.dsedelist &lt;- tbl(conn, sql(\"SELECT * FROM crsp.dsedelist\"))\n  \n  delist &lt;-\n    crsp.mse %&gt;%\n    filter(dlstcd &gt; 199) %&gt;%\n    select(permno, date, dlstcd, dlpdt, dlret)\n\n  prt &lt;- function(x) {\n    p &lt;- pt(mean(x) * sqrt(length(x)) / sd(x), length(x))\n    2 * pmin(p, 1 - p)\n  }\n  \n  rvs &lt;-\n    crsp.dsedelist %&gt;%\n    filter(dlstcd &gt; 199) %&gt;%\n    select(dlstcd, dlret) %&gt;%\n    filter(!is.na(dlret)) %&gt;%\n    collect() %&gt;% \n    group_by(dlstcd) %&gt;%\n    summarize(mean = mean(dlret),\n              p_val = prt(dlret),\n              .groups = \"drop\") %&gt;%\n    mutate(rv = if_else(p_val &lt;= 0.05, mean, 0)) %&gt;%\n    select(dlstcd, rv) %&gt;%\n    copy_inline(conn, .)\n  \n  delist %&gt;%\n    left_join(rvs, by = \"dlstcd\") %&gt;%\n    mutate(month = month(date),\n           year = year(date)) %&gt;%\n    rename(dldate = date)\n}\n\nSo we can replace the delist_rv data table we created above with one produced by the get_delist() function.\n\ndelist_rv &lt;- get_delist(pg)"
  },
  {
    "objectID": "published/delisting.html#merge-monthly-returns-with-delisting-data",
    "href": "published/delisting.html#merge-monthly-returns-with-delisting-data",
    "title": "Adding delisting returns to monthly data",
    "section": "5 Merge monthly returns with delisting data",
    "text": "5 Merge monthly returns with delisting data\nTranslating the SAS code here is pretty straightforward. The SAS code involves PROC SQL followed by a data step, but I do it all in one series of pipes.\nproc sql;\n  create table monthlyreturns as\n      select a.*, b.dlret, b.dlstcd, b.rv, b.date as dldate, b.dlpdt\n      from monthlyreturns a left join delist b\n      on (a.permno = b.permno)\n      and (month(a.date)= month(b.date))\n      and (year(a.date) = year(b.date));\nquit;\n      \ndata monthlyreturns;\n    set monthlyreturns;\n    ret_orig = ret;\n  \n    if not missing(dlstcd) and missing(dlret) then dlret=rv;\n    else if not missing(dlstcd) and dlpdt le dldate and not missing(dlret)\n        then dlret=(1+dlret)*(1+rv)-1;\n\n    ** Then, incorporate delistings into monthly return measure;\n    if not missing(dlstcd) and missing(ret) then ret=dlret;\n    else if not missing(dlstcd) and not missing(ret) \n        then ret=(1+ret)*(1+dlret)-1;\nrun;\nThe code below first uses replacement values where necessary (is.na(dlret)). Note, this will happen when the delisting occurs on the last day of the month and ret is not missing, but the delisting return is unknown. If the delisting return is a partial month return, CRSP flags it by setting dlpdt to a date less than or equal to the delisting date. Richard says that one could use a single replacement value as in Shumway (1997) (\\(-0.35\\)) or Sloan (1996) (\\(-1.0\\)) and that he would only do single replacement value for a subset of delisting codes &gt; 499.\nAgain I use a new name (monthlyreturns_delist) for the resulting table, as I prefer not to reuse table names.\n\nmonthlyreturns_delist &lt;-\n  monthlyreturns %&gt;%\n  mutate(month = month(date),\n         year = year(date)) %&gt;%\n  left_join(delist_rv, join_by(permno, month, year)) %&gt;%\n  mutate(ret_orig = ret,\n         dlret = case_when(!is.na(dlstcd) & is.na(dlret) ~ rv,\n                           !is.na(dlstcd) & dlpdt &lt; dldate & !is.na(dlret) ~ \n                             (1 + dlret) * (1 + rv) - 1,\n                           .default = dlret),\n         ret = case_when(!is.na(dlstcd) & is.na(ret) ~ dlret,\n                         !is.na(dlstcd) & !is.na(ret) ~ \n                           (1 + ret)*(1 + dlret) - 1,\n                         .default = ret)) %&gt;%\n  collect()\n\n\n5.1 Comparison of output with that of SAS code\nRichard includes output from SAS’s PROC MEANS that we can use to compare our results with his. Comparing the numbers in Table 1 with Richard’s output confirms that our R code has done the same thing as his.\n\n\n\n\nTable 1: Summary statistics for monthly returns with delisting returns\n\n\n\n\n\n\nN\nMean\nStd Dev\nMinimum\nMaximum\n\n\n\n\n3859\n0.0418905\n0.1820073\n-0.9913043\n5.1785717\n\n\n3842\n0.0429354\n0.1800152\n-0.5890411\n5.1785717\n\n\n20\n-0.1684432\n0.3543438\n-0.9913043\n0.3333333\n\n\n\n\n\n\n\n\n\n\n5.2 Performance\nRunning the SAS code above takes between 17 and 20 seconds on the WRDS server. The R code takes 9 seconds using a database on my laptop, and about 20 seconds on the same laptop, but using the remote WRDS database.5\nI would call this performance comparison in R’s favour because the R code is also generating the PDF document you are reading, which takes a few seconds. If using the SAS code, you would likely also need to add time to retrieve the data from the WRDS server if you are running analysis on a local computer."
  },
  {
    "objectID": "published/delisting.html#footnotes",
    "href": "published/delisting.html#footnotes",
    "title": "Adding delisting returns to monthly data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYou don’t need to install all the packages listed there, just the ones listed below plus RPostgres.↩︎\nIt’s probably a good thing to do this “by hand” just so you remember your statistics. Of course I checked that I got the same answer as t.test().↩︎\nI don’t think you’d cover macros in a first SAS class, but I cover making functions here.↩︎\nOnce I understand this code better, I may do this, as it would be good to use returns with delisting returns in the chapter replicating Sloan [1996].↩︎\nRunning the code above actually takes 27 seconds, but that’s because I unnecessarily create delist_rv twice. This is the portion of code that takes longer using a remote database because of the need to pull data into R.↩︎"
  },
  {
    "objectID": "published/ctes.html",
    "href": "published/ctes.html",
    "title": "Writing better SQL without writing SQL",
    "section": "",
    "text": "In this note, I use a query from Tanimura (2021) to illustrate first how one can re-write an SQL query using common table expressions (CTEs) and then how one can re-write that query again using dbplyr. I then do the analysis again from scratch, but using dbplyr expressions. I find that the SQL query contains inaccuracies, while the written-from-scratch dbplyr query does now. I conjecture that the “building blocks” approach to SQL facilitated by dbplyr may lead to more accurate of queries for many users.\nIn writing this note, I used the packages listed below.1 This note was written using Quarto and compiled with RStudio, an integrated development environment (IDE) for working with R. The source code for this note is available here and the latest version of this PDF is here.\nlibrary(DBI)\nlibrary(tidyverse)\nlibrary(dbplyr)"
  },
  {
    "objectID": "published/ctes.html#reproducing-figure-4-12-of-tanimura2021sql",
    "href": "published/ctes.html#reproducing-figure-4-12-of-tanimura2021sql",
    "title": "Writing better SQL without writing SQL",
    "section": "4.1 Reproducing Figure 4-12 of Tanimura (2021)",
    "text": "4.1 Reproducing Figure 4-12 of Tanimura (2021)\nFinally, I reproduce Figure 4-12 of Tanimura (2021, p. 163). This plot requires changes to the cohorts, to the cutoffs (now 10 and 20 years), and (it seems) to the population. From visual inspection of Figure 4-12 of Tanimura (2021), it seems that we now require a representative’s first term to have begun before 2000.\nNote that the query underlying Figure 4-12 of Tanimura (2021) is not provided in the book. Nor is the code for generating the plot itself. Because Tanimura (2021) only include SQL code, readers are on their own when it comes to code for generating the plots; this is an additional weakness of focusing on SQL code.\nThe first step I take is to make a new version of pct_rep_then_sen with cohort now based on decades and with the stricter filter on first_rep_term. Note that I simply overwrite whatever value for cohort was already in cohorts_revised. I also embed the calculation of reps (the number of members of each cohort) in the same pipeline as the other calculations.\n\npct_rep_then_sen &lt;-\n  cohorts_revised |&gt;\n  filter(first_rep_term &lt;= \"1999-12-31\") |&gt;\n  mutate(cohort = decade(first_rep_term),\n         event_time = age(first_sen_term, first_rep_term)) |&gt;\n  mutate(reps = n(), .by = cohort) |&gt;\n  mutate(cum_ids = cumsum(1), .by = cohort, .order = event_time) |&gt;\n  mutate(cum_ids = max(cum_ids, na.rm = TRUE),\n         .by = c(cohort, event_time)) |&gt;\n  mutate(pct = cum_ids / reps) \n\nI replace event_time_cutoffs with the new values (10 and 20 years):\n\nevent_time_cutoffs &lt;-\n  tibble(cutoff = c(10, 20)) |&gt;\n  copy_to(db, df = _, name = \"event_time_cutoffs\",\n          overwrite = TRUE) |&gt;\n  mutate(cutoff = years(cutoff))\n\nMaking the plot is now quite straightforward and the results of the following code can be seen in Figure 2:\n\npct_rep_then_sen |&gt;\n  cross_join(event_time_cutoffs) |&gt;\n  filter(event_time &lt;= cutoff) |&gt;\n  summarize(pct = max(pct, na.rm = TRUE),\n            .by = c(cohort, cutoff)) |&gt;\n  mutate(cutoff = as.character(year(cutoff))) |&gt;\n  ggplot(aes(x = cohort, y = pct, color = cutoff, group = cutoff)) +\n  geom_line()\n\n\n\n\n\n\n\nFigure 2: Share of representatives who become senators by decade"
  },
  {
    "objectID": "published/ctes.html#sec-fun",
    "href": "published/ctes.html#sec-fun",
    "title": "Writing better SQL without writing SQL",
    "section": "4.2 Making a function",
    "text": "4.2 Making a function\nCreating the code underlying Figure 4-12 of Tanimura (2021) likely involved a lot of copy-pasting and editing (e.g., to create new CASE statements for the new cutoffs and new WHERE clauses) even before moving the code (or data) to Python or Tableau to make the plot. The code above suggests that we might accomplish variants on the plot more programmatically and I pursue this idea in this section.\nOne benefit of doing this is that I can show how putting the survival data into a canonical structure can make it easier to run variants based on different populations and cohort definitions.\nFirst, I put the essence of the code in the following function with the only real edits being that I use more generic names for the tables and fields and put the cutoffs to be used in a variable cutoffs.\n\nmake_plot &lt;- function(cohorts, survival_data, cutoffs = c(10, 20)) {\n  plot_data &lt;-\n    survival_data |&gt;\n    mutate(event_time = age(event_date, entry_date)) |&gt;\n    inner_join(cohorts, by = \"id\") |&gt;\n    mutate(reps = n(), .by = cohort) |&gt;\n    mutate(cum_ids = cumsum(1), .by = cohort, .order = event_time) |&gt;\n    mutate(cum_ids = max(cum_ids, na.rm = TRUE),\n           .by = c(cohort, event_time)) |&gt;\n    mutate(pct = cum_ids / reps) \n  \n  event_time_cutoffs &lt;-\n    tibble(cutoff = cutoffs) |&gt;\n    copy_to(db, df = _, name = \"event_time_cutoffs\",\n            overwrite = TRUE) |&gt;\n    mutate(cutoff = years(cutoff))\n  \n  plot_data |&gt;\n    cross_join(event_time_cutoffs) |&gt;\n    filter(event_time &lt;= cutoff) |&gt;\n    summarize(pct = max(pct, na.rm = TRUE),\n              .by = c(cohort, cutoff)) |&gt;\n    mutate(cutoff = as.character(year(cutoff))) |&gt;\n    ggplot(aes(x = cohort, y = pct, color = cutoff, group = cutoff)) +\n    geom_line()\n}\n\nI next construct survival_data in a canonical form with id, entry_date, and event_date as the fields.\n\nsurvival_data &lt;-\n  first_terms |&gt;\n  filter(!is.na(first_rep_term), \n         first_term == first_rep_term) |&gt;\n  rename(id = id_bioguide,\n         entry_date = first_rep_term,\n         event_date = first_sen_term) |&gt;\n  select(id, entry_date, event_date)\n\nNow I can easily reproduce Figure 2 using the following code with the results being seen in Figure 3:\n\nsurvival_data |&gt;\n  filter(entry_date &lt;= \"1999-12-31\") |&gt;\n  mutate(cohort = decade(entry_date)) |&gt;\n  select(id, cohort) |&gt;\n  make_plot(survival_data = survival_data,\n            cutoffs = c(10, 20))\n\n\n\n\n\n\n\nFigure 3: Share of representatives who become senators by decade (encore)\n\n\n\n\n\nAnd with a few lines of code, I can make a plot version of Table 4, which can be seen in Figure 4:\n\nsurvival_data |&gt;\n  filter(entry_date &lt;= \"2009-12-31\") |&gt;\n  mutate(cohort = century(entry_date)) |&gt;\n  select(id, cohort) |&gt;\n  make_plot(survival_data = survival_data,\n            cutoffs = c(5, 10, 15))\n\n\n\n\n\n\n\nFigure 4: Share of representatives who become senators by century"
  },
  {
    "objectID": "published/ctes.html#footnotes",
    "href": "published/ctes.html#footnotes",
    "title": "Writing better SQL without writing SQL",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nExecute install.packages(c(\"tidyverse\", \"DBI\", \"duckdb\", \"dbplyr\") within R to install all the packages you need to run the code in this note.↩︎\nI made minor punctuation edits here.↩︎\nI put the ugly details of the db_get_csv() function that I use here in Listing 1.↩︎\nI edited the query slightly to reflect code style guidelines I use in later queries. I also rename the column cohort_century to cohort throughout. The value of using a more generic name will be seen in Section 4.2.↩︎\nMy view is that the choice made early in the development dplyr of a de facto default of .groups = \"drop_last\" is a rather unfortunate one.↩︎\nNote that if_else(age &lt;= years(5), id_bioguide, NA) would be an alternative way to get the same result as case_when(age &lt;= years(5) ~ id_bioguide) gives.↩︎\nAn alternative would’ve been to use pct_ in place of num_ in the age_cuts query.↩︎\nStrictly speaking, it should be “on or before 31 December 2009”, but legislators never start terms on 31 December.↩︎\nSee https://iangow.github.io/cohorts/intermezzo.html for some discussion on this point.↩︎\nThis is equivalent to count(DISTINCT id_bioguide) OVER (PARTITION BY cohort ORDER BY event_time), but we do not need the DISTINCT here because each value of id_bioguide is unique in this query.↩︎\nI explain why I think it is a more precise solution in Section 5.↩︎\nI made minor punctuation edits here.↩︎"
  },
  {
    "objectID": "published/bklyz.html",
    "href": "published/bklyz.html",
    "title": "Should Bao et al. (2020) be retracted?",
    "section": "",
    "text": "Walker (2022) calls “for an investigation at the Journal of Accounting Research (JAR) into academic research misconduct” related to Bao et al. (2020). In this short note, I examine a somewhat different question: Should Bao et al. (2020) be retracted?\nI argue that the current presentation of Bao et al. (2020) (the original paper plus an erratum) is apt to mislead regarding its key findings. As such, some form of retraction seems appropriate, even if only to provide (through a “retract and republish” approach) a research record that is both clear and free from known error.\nI then identify a number of factors that seem relevant to evaluating the merits of providing the authors of Bao et al. (2020) the opportunity to republish if retraction were pursued.1"
  },
  {
    "objectID": "published/bklyz.html#is-there-evidence-of-misconduct",
    "href": "published/bklyz.html#is-there-evidence-of-misconduct",
    "title": "Should Bao et al. (2020) be retracted?",
    "section": "Is there evidence of misconduct?",
    "text": "Is there evidence of misconduct?\nW3 claims to “make the case that there is evidence of academic misconduct and make the recommendation that the Journal of Accounting Research launch a full and independent investigation into the matter.” I cannot imagine that the editors of the Journal of Accounting Research would be keen to get into questions of academic misconduct, given the implications of any such finding. Instead, it seems more relevant to focus on issues pertinent to what is published in JAR.\nWhile I argue that a better presentation of the research record requires some kind of retraction, the COPE guidelines cited above appear to afford some latitude as to whether a journal will “in some instances … wish to work with authors to concurrently retract an article that was found to be fundamentally flawed while simultaneously publishing a linked and corrected version of the work.” In other words, the Journal of Accounting Research arguably enjoys wide discretion over whether to “republish” a corrected version of BKLYZ1.\nThe remainder of this note collects some information that I conjecture the Journal of Accounting Research might consider in reaching its decision on the best course of action in response to the call from W3."
  },
  {
    "objectID": "published/bklyz.html#test-period",
    "href": "published/bklyz.html#test-period",
    "title": "Should Bao et al. (2020) be retracted?",
    "section": "Test period",
    "text": "Test period\nOne thing that BKLYZ1 is very clear on is that the test period is 2003–2008. The main results (“performance increase of 7.9% and 75%”) of that paper are all based on this test period.\nYet the erratum [BKLYZ3, p. 1636] mysteriously seems to emphasize 2003–2005 as the test period: “Using NDCG@k as a performance measure RUSBoost … continues the dominate the performance of the other models for the test period 2003–2005.” The published erratum [p. 1636] even suggests that BLKYZ1 “argued that this test period was the cleanest”. Yet this is simply an impossible reading of BKLYZ1. As a reader, one needs to be confident that the “best test period” is not simply the test period that delivers the most favorable “out-of-sample” performance for RUSBoost.\nAt this point, a careful reader might point out that 2003–2005 was actually the test period used in BKLYZ0. Hopefully, BKLYZ would not be the ones to point this out, because this choice of test period was justified in BKLYZ0 on the following two two bases. First, BKLYZ0 state [p. 4] that “the SEC’s Accounting and Auditing Enforcement Releases (AAERs) available to us end in September 2010”. Second, “there is an average of five-year gap between a fraud occurrence and the AAER publication date.”\nAs the sample period in BKLYZ1 includes AAERs that extend to 2014, the first item seems to suggest that (on the logic of BKLYZ0 itself) with the test period could be updated to from 2003–2005 to 2003–2009 (i.e., adding four years).\nBut it is important to note that BKLYZ0 included in their training sample frauds that were also in the test periods, even though the AAER publication dates would have been after the test periods in question.8\nIf the second basis were maintained, but the issues of “serial fraud” addressed, then by the logic of BKLYZ1, the “gap” of two years used in BKLYZ1 would have to be five years, and thus the feasible test sample could not begin until 2006. BKLYZ1 [p. 209] “require a gap of 24 months between the financial results announcement of the last training year and the results announcement of a test year … because Dyck, Morse, and Zingales (2010) find that it takes approximately 24 months, on average, for the initial disclosure of the fraud.”9"
  },
  {
    "objectID": "published/bklyz.html#the-coding-error",
    "href": "published/bklyz.html#the-coding-error",
    "title": "Should Bao et al. (2020) be retracted?",
    "section": "The “coding error”",
    "text": "The “coding error”\nBKLYZ3 states [p. 1635] that [W1 and W2] “identified an error in the program codes [sic] of BKLYZ1 posted on Github that led to an overstatement of model performance metrics. This erratum corrects this error ….” It is difficult to disagree with W3’s claim that this statement is false. There is nothing in the code posted on GitHub that created this error, instead the “error” was in data used by that code.\nW3 claims that “to this date, the authors have offered no explanation as to why they did what they did” in recoding certain frauds to have different identifiers. This seems correct. In BKLYZ2, BKLYZ provide what may be best described as a non-explanation for what they did.\nThe example in Figure 1 of BKLYZ2 illustrates the “coding error” made in BKLYZ1 if there are missing items for the affected firm in 2004. If such missing items exist, firm-years in 2001 through 2003 would be given a different fraud ID from the 2005 firm-year (in such cases BKLYZ appended the character “1” to the fraud ID for 2001 through 2003, and the character “2” to the fraud ID for 2005). Because the fraud IDs for 2001–2003 no longer appear in the test year (2005), they are not recoded as zero. In BKLYZ2, the practice of not recoding these frauds in this way is described as “Walker’s approach” even though (as W2 points out), this is not so much Walker’s approach as the approach described in BKLYZ1.\nEvidenced of this approach to recoding frauds is not legitimate is provided by the fact that doing it that way has been characterized as a “coding error” in BKLYZ3.\nBut relabelling “serial frauds” also does not make sense in that it is recoding a fraud as 1 in 2003 so that a “prediction” of the same underlying fraud can be made in 2005 even though, by the terms of the example itself (Figure 1 of BKLYZ), the SEC does not release an AAER until 2007.\nThat training models using data on frauds that are not released until after the test period is problematic seems obvious. And it seems clear from BKLYZ1’s discussion of “serial fraud” that the authors were well aware of these issues. In fact, they seem aware of the underlying issue even in BKLYZ0. As discussed above, the BKLYZ0 “sample ends in 2005 because … a significant portion of the accounting frauds that occurred over 2006–2010 are likely still unreported by the AAERs as of the end of 2010.”\nOne response the authors might have is that there might be a “key event [that] reveals 2001–2003 fraud labels” before 2004 (and before the AAER publication date in 2007). But this implies a completely different prediction problem from that studied in BKLYZ1 (or BKLYZ0), which identifies frauds as AAER events. The only reliable “key event” that identifies an AAER is the release of an AAER. As discussed above, disclosed accounting fraud might not result in AAERs for a number of reasons.\nSaying that sometimes information is released that suggests a high likelihood of a future AAER event transforms the prediction problem from one about predicting confirmed AAERs using financial statement features into one about predicting confirmed AAERs using financial statement features and also some unidentified information about possible future AAERs.\nEven if we expand the information set to include the “maybe-future-AAERs” as is done in the “not Walker’s approach” depicted in Figure 1 of BKLYZ2, there is no rationale provided as to why missing values for some items on Compustat in 2004 should be assumed to precipitate a fraud revelation event before 2004. In fact, it seems hard to conceive of one.\nAnd this “coding error” is surely not something that happened by mistake. The strident defence of their approach provided in BKLYZ2 suggests that the authors did this consciously, and only in BKLYZ3 did they suggest it was a “coding error”. A reproduction of the “coding error” is included in an appendix to this note and it seems difficult to see how this “coding error” could have been made inadvertently.\nAt the very least, I think the authors should be required to provide more information of how the “coding error” was implemented so that the editors can assess the likelihood that it was indeed a “coding error” (as it is framed in BKLYZ3) and not a deliberate research design choice (as it is framed in BKLYZ2). If it seems that it was a deliberate research design choice, then I think the authors need to explain the rationale for it and also the rationale for dissembling its existence in the code posted to GitHub upon publication of BKLYZ1.\nOne possible response by the BKLYZ team might be that they have more than complied with the JAR data policy in effect when they submitted their paper and therefore do not need to account for the “coding error” beyond the code they have already provided.\nI do not think this kind of response would be helpful in this case. There appears to be no data policy at The Accounting Review (see discussion here), but this did not prevent the retraction of Bird and Karolyi (2017), where the journal stated “the authors were unable to provide the original data and code requested by the publisher” to support an assertion made in the paper and therefore retracted the paper.\nGiven the “coding error” of BKLYZ3 appears to have been a conscious decision (see BKLYZ2), I think that the onus is on the authors of BKLYZ1 to show that the “coding error” was made in good faith."
  },
  {
    "objectID": "published/bklyz.html#meta-parameters",
    "href": "published/bklyz.html#meta-parameters",
    "title": "Should Bao et al. (2020) be retracted?",
    "section": "Meta-parameters",
    "text": "Meta-parameters\nIn BKLYZ2, the authors suggested that W1 was flawed because he “did not recalibrate the most important parameter of RUSBoost, number of trees, after changing the fraud training samples using his approach [i.e., the approach described in BKLYZ1].” This is somewhat understandable, as BKLYZ1 does not describe the process of calibrating these meta-parameters in the first place. Indeed, there are several meta-parameters used in BKLYZ1: number of trees, MinLeafSize, LearnRate, and RatioToSmallest. It is critical that such meta-parameters be fixed using the training and validation data prior to evaluating model performance against test data, lest these parameters be selected based on test performance, thus overstating the predictive value of the model.\nIn this regard, it is somewhat concerning that the number of trees parameter that is selected for the BKLYZ1 specification is 1,000 according to W3, not the 3,000 used in BKLYZ1.10 This creates the unfortunate impression that the idea of selecting meta-parameters in a transparent fashion using only validation data emerged only after observing a decline in test performance upon switching to “Walker’s approach” in preparing the erratum."
  },
  {
    "objectID": "published/bklyz.html#other-data-issues",
    "href": "published/bklyz.html#other-data-issues",
    "title": "Should Bao et al. (2020) be retracted?",
    "section": "Other data issues",
    "text": "Other data issues\nWhile it is possible to explain changes in the code between BKLYZ1 and BKLYZ3 using GitHub, some concerns remain. For example, as detailed here, the two data files provided are not consistent. While the PDF-rendered SAS code suggests that one data file depends on the other, there are observations on AAERs not found in the former."
  },
  {
    "objectID": "published/bklyz.html#how-unusual-are-the-issues-in-bklyz1",
    "href": "published/bklyz.html#how-unusual-are-the-issues-in-bklyz1",
    "title": "Should Bao et al. (2020) be retracted?",
    "section": "How unusual are the issues in BKLYZ1?",
    "text": "How unusual are the issues in BKLYZ1?\nSome readers may be surprised to learn that the core results of a published paper can disappear when a “coding error” is detected and corrected. I am not surprised. Having replicated many papers, I conjecture that the kinds of issues observed with BKLYZ1 are commonplace. Papers that say one thing, but do another (most papers using “regression discontinuity designs” fit here). Papers with genuine coding errors. Papers with results that are very sensitive to “design choices” that are difficult to rationalize (see here and here). If such issues abound, then the merit of singling out BKLYZ1 seems to be low.\nAnother factor that seems relevant is JAR’s data policy. While not perfect, arguably JAR’s policy was critical in helping to unearth the issues not only in BKLYZ1, but in the replications I refer to above.11 If BKLYZ1 had been accompanied by a very perfunctory effort to comply with the data policy (e.g., “here is the list of CIKs for the fraud firms in our sample”), then it would not have been possible to detect the issue raised by W1 and corrected in BKLYZ3."
  },
  {
    "objectID": "published/bklyz.html#the-source-data-sets",
    "href": "published/bklyz.html#the-source-data-sets",
    "title": "Should Bao et al. (2020) be retracted?",
    "section": "The source data sets",
    "text": "The source data sets\nPDF-rendered SAS course code supplied by BKLYZ suggests that the final data set used in Bao et al. (2020) was constructed by merging data on AAERs (aaer_firm_year) with data on raw Compustat variables (compustatindustrial7815). To reproduce the “coding error”, we need to retrace the process of merging these two tables.\nWhile BKLYZ provide a file AAER_firm_year.csv, it is easy to show that this was not the data file used to create the Bao et al. (2020) data set.13\nAs such, we use the “final” data set used by Bao et al. (2020) and reconstruct the relevant portions of the source data sets from that.\n\njar_data &lt;-\n    read_csv(paste0(\"https://raw.githubusercontent.com/JarFraud/\",\n                    \"FraudDetection/master/\",\n                    \"data_FraudDetection_JAR2020.csv\"),\n             col_types = \"d\") |&gt;\n    mutate(gvkey = str_pad(gvkey, 6, side = \"left\", pad = \"0\"),\n           fyear = as.integer(fyear),\n           p_aaer = as.character(p_aaer))\n\nWe first construct the original aaer_firm_year data set by filling in any gaps in firm-years for an AAER found in jar_data.\n\naaer_firm_year &lt;-\n    jar_data |&gt;\n    filter(!is.na(p_aaer)) |&gt;\n    group_by(p_aaer, gvkey) |&gt;\n    summarize(min_year = min(fyear), max_year = max(fyear), \n              .groups = 'drop') |&gt;\n    rowwise() |&gt;\n    mutate(fyear = list(seq(min_year, max_year, by = 1))) |&gt;\n    unnest(fyear) |&gt;\n    select(gvkey, fyear, p_aaer) \n\nAs we can see here, aaer_firm_year is the panel data set of firm-years affected by AAERs.\n\nhead(aaer_firm_year)\n\n# A tibble: 6 × 3\n  gvkey  fyear p_aaer\n  &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt; \n1 021110  1992 1033  \n2 008496  1992 1037  \n3 008496  1993 1037  \n4 028140  1993 1044  \n5 012455  1994 1047  \n6 025927  1993 1053"
  },
  {
    "objectID": "published/bklyz.html#firm-years-with-compustat-features",
    "href": "published/bklyz.html#firm-years-with-compustat-features",
    "title": "Should Bao et al. (2020) be retracted?",
    "section": "Firm-years with Compustat features",
    "text": "Firm-years with Compustat features\nWe next construct the data set comp_firm_years, which represents the firm-years in the Bao et al. (2020) sample. These will be missing some of the firm-years in aaer_firm_year because of missing items on Compustat.\n\ncomp_firm_years &lt;-\n  jar_data |&gt;\n  mutate(missing = 0) |&gt;\n  select(fyear, gvkey, missing)"
  },
  {
    "objectID": "published/bklyz.html#the-coding-error-1",
    "href": "published/bklyz.html#the-coding-error-1",
    "title": "Should Bao et al. (2020) be retracted?",
    "section": "The “coding error”",
    "text": "The “coding error”\nNow that we have the two data sets aaer_firm_year and comp_firm_years, we can reproduce the “coding error” from Bao et al. (2020).\nThe first step is to merge aaer_firm_year with comp_firm_years using the left_join function so that all observations on aaer_firm_year are retained even if there is no match on comp_firm_years. The cases where there is no match on comp_firm_years are indicated by the variable missing.\n\naaer_merged &lt;-\n  aaer_firm_year |&gt;\n  left_join(comp_firm_years, by = c(\"gvkey\", \"fyear\")) |&gt;\n  mutate(missing = coalesce(missing, 1)) |&gt;\n  select(gvkey, fyear, p_aaer, missing)\n\nhead(aaer_merged)\n\n# A tibble: 6 × 4\n  gvkey  fyear p_aaer missing\n  &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;\n1 021110  1992 1033         0\n2 008496  1992 1037         0\n3 008496  1993 1037         0\n4 028140  1993 1044         0\n5 012455  1994 1047         0\n6 025927  1993 1053         0\n\n\nNow we can recode AAERs following Bao et al. (2020). We do this by calculating sum_gap, a running sum of the indicator variable for one plus the number of gaps in the sample for a given AAER.14 This will start at 1 and increase to 2 after a “gap”. We then create the variable new_p_aaer by combining the original AAER identifier (p_aaer) with sum_gap.\n\naaer_sum_gap &lt;-\n  aaer_merged |&gt;\n  group_by(gvkey, p_aaer) |&gt;\n  arrange(fyear) |&gt;\n  mutate(lag_missing = coalesce(lag(missing), 0),\n         sum_gap = cumsum(lag_missing & !missing) + 1,\n         new_p_aaer = paste0(p_aaer, \n                             as.character(sum_gap))) |&gt;\n  select(gvkey, fyear, p_aaer, new_p_aaer) |&gt;\n  ungroup()\n\nIf successful, my code reproduces the “coding error” in Bao et al. (2020) in about 14 lines of code.15"
  },
  {
    "objectID": "published/bklyz.html#verifying-the-reproduction-of-the-coding-error",
    "href": "published/bklyz.html#verifying-the-reproduction-of-the-coding-error",
    "title": "Should Bao et al. (2020) be retracted?",
    "section": "Verifying the reproduction of the “coding error”",
    "text": "Verifying the reproduction of the “coding error”\nTo check that we have successfully reproduced the “coding error” corrected by Bao et al. (2022), we can reproduce Figure 2 of Walker (2021a), which we do with the following code.\n\nwalker_fig_2 &lt;-\n    aaer_sum_gap |&gt; \n    group_by(p_aaer) |&gt; \n    filter(n_distinct(new_p_aaer) &gt; 1) |&gt;\n    inner_join(comp_firm_years, by = c(\"gvkey\", \"fyear\")) |&gt;\n    pivot_wider(names_from = \"fyear\", id_cols = \"p_aaer\", \n              values_from = \"new_p_aaer\") |&gt;\n    ungroup() |&gt;\n    arrange(as.integer(p_aaer)) |&gt;\n    mutate(across(`1991`:`2014`, ~ coalesce(., \"\"))) \n\nFor reasons of space, we only include a portion of the table.\n\nwalker_fig_2 |&gt;\n    select(p_aaer, `1995`:`2004`) |&gt;\n    knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np_aaer\n1995\n1996\n1997\n1998\n1999\n2000\n2001\n2002\n2003\n2004\n\n\n\n\n857\n\n\n\n\n\n\n\n\n\n\n\n\n1542\n\n\n15421\n15421\n\n15422\n\n\n\n\n\n\n1839\n\n\n\n18391\n18391\n18391\n\n18392\n\n\n\n\n2472\n\n\n\n24721\n24721\n24721\n24721\n\n24722\n24722\n\n\n2504\n\n\n\n\n\n25041\n25041\n\n25042\n25042\n\n\n2591\n\n\n\n\n\n25911\n25911\n\n25912\n25912\n\n\n2754\n27541\n27541\n\n27542\n27542\n27542\n27542\n27542\n27542\n27542\n\n\n2894\n\n\n\n\n\n28941\n\n28942\n28942\n\n\n\n2937\n\n\n\n29371\n\n\n\n\n\n29372\n\n\n2949\n\n\n\n\n29491\n\n\n\n\n29492\n\n\n3022\n\n\n\n\n30221\n\n\n30222\n\n\n\n\n3045\n\n\n30451\n30451\n30451\n\n30452\n\n\n\n\n\n3156\n\n\n\n\n\n\n\n\n31561\n\n\n\n3217\n\n32171\n32171\n\n\n32172\n32172\n32172\n32172\n32172\n\n\n3909\n\n\n\n\n\n\n\n\n\n\n\n\n3996\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCareful comparison of my data with Figure 2 of Walker (2021a) suggests that I have almost perfectly reproduced the “coding error” of Bao et al. (2020). The one exception is that my table omits the AAER with p_aaer of 2957.\nExamining the underlying data, it seems that the issue here is the presence of multiple AAERs for the related firm for 2000 and 2001.\n\naaer_firm_year |&gt; \n    filter(gvkey == \"064630\") |&gt; \n    arrange(fyear)\n\n# A tibble: 8 × 3\n  gvkey  fyear p_aaer\n  &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt; \n1 064630  1997 2957  \n2 064630  1998 2957  \n3 064630  1999 2957  \n4 064630  2000 2259  \n5 064630  2000 2957  \n6 064630  2001 2259  \n7 064630  2001 2957  \n8 064630  2002 2957  \n\n\nReturning to the SAS code supplied with Bao et al. (2020), we see the following lines:\nPROC SORT DATA=temp nodupkey;\nBY gvkey fyear; RUN;\nThis code would have the effect of (essentially randomly) deleting data on one AAER for any firm-year where two AAERs apply. It is unclear whether the BKLYZ team would characterize this second basis for recoding AAERs as a research design choice (as the coding error was arguably characterized in Bao et al. 2021) or as an error like the “coding error” replicated above.\nThe use of PROC SORT nodupkey is common in accounting reserarch, but in general this is a problematic practice.16"
  },
  {
    "objectID": "published/bklyz.html#footnotes",
    "href": "published/bklyz.html#footnotes",
    "title": "Should Bao et al. (2020) be retracted?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe views expressed here are my own. I have no formal role in this process or association with any of the papers published in the Journal of Accounting Research or Econ Journal Watch. This note arises from observations I have made in preparing a chapter on prediction for a course book on accounting research. While I have sought feedback from a small number of people on the tone and content of this note, any errors herein are my own.↩︎\nSee the SEC website for details.↩︎\nIn some ways, the requirement for Matlab is unfortunate, as Matlab is proprietary software and offers less transparency than an implementation using one of the open-source alternatives, such as Python or R, would. Unfortunately there is no implementation of RUSBoost in the popular Python library, scikit-learn, though this library does have an implementation of AdaBoost. There is also no well-documented implementation of RUSBoost in R, though there are several implementations of the AdaBoost in R. Fortunately, it is possible to implement RUSBoost in R, and I include an implementation in the R package farr that I created as a complement to the course book found here. A chapter using RUSBoost will be forthcoming in the near future.↩︎\nThe one AAER in the Bao et al. (2020) sample connected to Enron actually covers the order for Citigroup to pay an amount in a settlement arising because “Citigroup assisted [Enron and Dynegy] in enhancing artificially their financial presentations through a series of complex structured transactions … to allow those companies to report proceeds of financings as cash from operating activities”.↩︎\nThe general result here being that statistical learning methods such as ensemble learning usually improve out-of-sample prediction performance relative to models—such as logistic regression—that tend to overfit the data used to train them.↩︎\nAAER dates are easily obtained from the SEC website, and included with the farr R package (see here). My own analysis suggests that AAERs are never released prior to the last affected period, so AAERs that affect test years are always in the “future” relative to that test year, and should never be coded as anything other than zero in the training sample.↩︎\nSee p.2 of Retraction Guidelines.↩︎\nThat this is the case is implicit in footnote 10 to BKLYZ1 and the discussion under “serial fraud” in BKLYZ1, which is not found in BKLYZ0.↩︎\nWhile this shorter period is definitely convenient for BKLYZ1, it seems less convenient that there is no evidence of the claim in Dyck, Morse, and Zingales (2010) itself. Perhaps the BKLYZ1 authors obtained underlying data from the authors of Dyck, Morse, and Zingales (2010).↩︎\nIn my own analysis, I found that 900 trees maximized performance in the validation sample, which is very close to the value found in W3.↩︎\nWe should not infer that the situation is better at journals without a data-sharing policy. If anything, we might expect it to be worse.↩︎\nInstall this using the command install.packages(\"tidyverse\") in R, if necessary.↩︎\nSee here for details. In short, the final data set used in Bao et al. (2020) contains AAERs not found in AAER_firm_year.csv.↩︎\nA “gap” is indicated by lag_missing being 1 and missing being zero.↩︎\nThe exact number depends on how one counts a line.↩︎\nIssues associated with this practice are explored in discussion questions here.↩︎"
  },
  {
    "objectID": "published/aus_ipos.html",
    "href": "published/aus_ipos.html",
    "title": "Analysis of IPOs on the ASX",
    "section": "",
    "text": "The purpose of this note is to compile some statistics about IPOs on the Australia Stock Exchange (ASX). The source code for this note can be found here.\n\n\n\n\n\n\n\n\n\nFigure 1: Distribution of IPO Day-1 returns (% over issue price)\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: IPOs by month (May 2017–April 2025)\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Average cumulative return on issue price with 95% confidence intervals"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Notes",
    "section": "",
    "text": "This site publishes a curated set of notes. Use the category filters to find notes by topic.\n\n\n\n\n\n\n   \n    \n    \n      Order By\n      Default\n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\n\nDate\n\n\n\nCategories\n\n\n\n\n\n\n\n\nThe Gino-Colada Affair\n\n\nFeb 19, 2026\n\n\n \n\n\n\n\n\n\nData curation: The case of Call Reports\n\n\nFeb 18, 2026\n\n\nData curation, Polars, DuckDB\n\n\n\n\n\n\nWorking with date and times\n\n\nFeb 2, 2026\n\n\n \n\n\n\n\n\n\nSome benchmarks with comp.g_secd\n\n\nJan 21, 2026\n\n\n \n\n\n\n\n\n\nThe best of both worlds: Using modern data frame libraries to create pandas data\n\n\nJan 20, 2026\n\n\nWRDS, Polars, Ibis, pandas\n\n\n\n\n\n\nUsing SAS to create pandas data\n\n\nJan 20, 2026\n\n\nSAS, pandas, wrds2pg\n\n\n\n\n\n\nShared code\n\n\nJan 15, 2026\n\n\nresearch, web data\n\n\n\n\n\n\nWriting better SQL without writing SQL\n\n\nDec 17, 2025\n\n\n \n\n\n\n\n\n\nResponsive open-source software: Two examples from dbplyr\n\n\nDec 17, 2025\n\n\n \n\n\n\n\n\n\nAnalysis of IPOs on the ASX\n\n\nSep 12, 2025\n\n\n \n\n\n\n\n\n\nDefining winter and summer in Oxford\n\n\nMar 10, 2025\n\n\n \n\n\n\n\n\n\nStock returns on Yahoo Finance\n\n\nFeb 26, 2025\n\n\n \n\n\n\n\n\n\nACNC Registry data: Arrow version\n\n\nOct 1, 2024\n\n\n \n\n\n\n\n\n\nConsumer Price Index\n\n\nSep 25, 2024\n\n\n \n\n\n\n\n\n\nA quick look at City of Melbourne bike data\n\n\nSep 19, 2024\n\n\n \n\n\n\n\n\n\nDefining winter and summer in Sydney\n\n\nApr 20, 2024\n\n\n \n\n\n\n\n\n\nDefining winter and summer in Melbourne\n\n\nApr 20, 2024\n\n\n \n\n\n\n\n\n\nTrading days per year (crsp.dsf)\n\n\nApr 10, 2024\n\n\n \n\n\n\n\n\n\nData visualization challenge\n\n\nApr 5, 2024\n\n\n \n\n\n\n\n\n\nThe elephant in the room: p-hacking and accounting research\n\n\nAug 8, 2023\n\n\nResearch methods, p-hacking\n\n\n\n\n\n\nAdding delisting returns to monthly data\n\n\nApr 7, 2023\n\n\n \n\n\n\n\n\n\nShould Bao et al. (2020) be retracted?\n\n\nOct 13, 2022\n\n\n \n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "published/acnc_registry_arrow.html",
    "href": "published/acnc_registry_arrow.html",
    "title": "ACNC Registry data: Arrow version",
    "section": "",
    "text": "This code shows how one can use list columns (e.g., in a parquet file) to provide a single-file (or single-table) representation of data that might naturally be stored as multiple tables in a more traditional relational database. The code to produce the parquet file used in the following analysis is provided here.\nIn the original registry data supplied by the ACNC, the data I have stored in list columns were spread over multiple columns. For example, “Operating locations (columns R-Z)” included columns such as “Operates in ACT” and “Operates in VIC” with values equal to either Y or blank. I converted these columns to a single column, states, with values such as VIC or VIC, NSW. While these look like simply comma-separated text values when viewing the data in software such as Tad, they are actually list columns.\nOther list columns include operating_countries (originally a single column, but as comma-separated text, not a list column), subtypes (originally “Subtypes (columns AA-AN)”), and beneficiaries (originally “Beneficiaries (columns AO-BN)”). Below I provide examples of working with such columns.\nIn writing this note, I use the packages listed below.1 This note was written using Quarto and compiled with RStudio, an integrated development environment (IDE) for working with R. The source code for this note is available here and the latest version of this PDF is here.\nlibrary(tidyverse)\nlibrary(tinytable)\nlibrary(arrow)\nlibrary(farr)\nWe start by downloading the data, which takes a few seconds.\nregistry &lt;-\n  read_parquet('https://go.unimelb.edu.au/5d78') |&gt;\n  collect() |&gt;\n  system_time()\n\n   user  system elapsed \n  0.427   0.059   3.897\nWe can construct the beneficiaries data frame by using unnest() with the list column beneficiaries.\nbeneficiaries &lt;-\n  registry |&gt;\n  select(abn, beneficiaries) |&gt;\n  unnest(beneficiaries) |&gt;\n  rename(beneficiary = beneficiaries)\nCharities vary in terms of the groups they serve, or beneficiaries. The results of the following code are shown in Table 1.\nregistry |&gt;\n  unnest(beneficiaries) |&gt;\n  count(beneficiaries, sort = TRUE) |&gt;\n  tt() |&gt;\n  style_tt(align = \"ld\") |&gt;\n  format_tt(escape = TRUE) \n\n\n\nTable 1: Number of charities serving each beneficiary type\n\n\n\n\n\n    \n\n    \n\n    \n    \n    \n      \n        \n        \n              \n                beneficiaries\n                n\n              \n        \n        \n        \n                \n                  Youth\n                  24633\n                \n                \n                  Adults\n                  23992\n                \n                \n                  Families\n                  23188\n                \n                \n                  General Community in Australia\n                  22638\n                \n                \n                  Children\n                  22130\n                \n                \n                  Aged Persons\n                  21763\n                \n                \n                  Females\n                  19027\n                \n                \n                  Males\n                  18012\n                \n                \n                  Financially Disadvantaged\n                  15826\n                \n                \n                  Early Childhood\n                  15184\n                \n                \n                  Rural Regional Remote Communities\n                  14757\n                \n                \n                  Ethnic Groups\n                  14384\n                \n                \n                  Aboriginal or TSI\n                  13528\n                \n                \n                  People with Disabilities\n                  13396\n                \n                \n                  People at risk of homelessness\n                  9493\n                \n                \n                  Unemployed Person\n                  9327\n                \n                \n                  People with Chronic Illness\n                  8082\n                \n                \n                  Other Charities\n                  6513\n                \n                \n                  Veterans or their families\n                  5656\n                \n                \n                  Victims of crime\n                  5220\n                \n                \n                  Victims of Disasters\n                  4856\n                \n                \n                  Communities Overseas\n                  4710\n                \n                \n                  Migrants Refugees or Asylum Seekers\n                  3735\n                \n                \n                  Pre Post Release Offenders\n                  3612\n                \n                \n                  Gay Lesbian Bisexual\n                  2890\nMany charities serve multiple beneficiary types. The most common pairs of beneficiary types are given in Table 2, which is produced using the following code.\nbeneficiaries |&gt;\n  inner_join(beneficiaries, by = \"abn\",\n             relationship = \"many-to-many\") |&gt;\n  filter(beneficiary.x &lt; beneficiary.y) |&gt;\n  count(beneficiary.x, beneficiary.y) |&gt;\n  arrange(desc(n)) |&gt;\n  head(n = 10) |&gt;\n  tt() |&gt;\n  style_tt(align = \"lld\") |&gt;\n  format_tt(escape = TRUE)\n\n\n\nTable 2: Most common beneficiary pairs\n\n\n\n\n\n    \n\n    \n\n    \n    \n    \n      \n        \n        \n              \n                beneficiary.x\n                beneficiary.y\n                n\n              \n        \n        \n        \n                \n                  Adults\n                  Aged Persons\n                  19277\n                \n                \n                  Adults\n                  Youth\n                  18429\n                \n                \n                  Children\n                  Youth\n                  17384\n                \n                \n                  Females\n                  Males\n                  17182\n                \n                \n                  Adults\n                  Families\n                  16247\n                \n                \n                  Aged Persons\n                  Youth\n                  15787\n                \n                \n                  Families\n                  Youth\n                  15381\n                \n                \n                  Aged Persons\n                  Families\n                  15048\n                \n                \n                  Adults\n                  Females\n                  14088\n                \n                \n                  Children\n                  Families\n                  13987\nThe results of the following code are shown in Table 3.\nregistry |&gt;\n  unnest(operating_countries) |&gt;\n  select(abn, operating_countries) |&gt;\n  filter(operating_countries != \"AUS\") |&gt;\n  count(operating_countries, sort = TRUE) |&gt;\n  head(n = 10) |&gt;\n  tt() |&gt;\n  format_tt(escape = TRUE)\n\n\n\nTable 3: Most common countries of operation\n\n\n\n\n\n    \n\n    \n\n    \n    \n    \n      \n        \n        \n              \n                operating_countries\n                n\n              \n        \n        \n        \n                \n                  IDN\n                  430\n                \n                \n                  PHL\n                  385\n                \n                \n                  PNG\n                  371\n                \n                \n                  KEN\n                  360\n                \n                \n                  UGA\n                  299\n                \n                \n                  NPL\n                  270\n                \n                \n                  FJI\n                  263\n                \n                \n                  IND\n                  247\n                \n                \n                  THA\n                  241\n                \n                \n                  VNM\n                  240\nThe results of the following code are shown in Table 4.\nregistry |&gt;\n  unnest(operating_countries) |&gt;\n  distinct(abn, operating_countries) |&gt;\n  filter(operating_countries != \"AUS\") |&gt;\n  count(abn, name = \"num_countries\", sort = TRUE) |&gt;\n  mutate(num_countries = if_else(num_countries &gt; 10, \"More than 10\", \n                                 as.character(num_countries)),\n         num_countries = fct_inorder(num_countries)) |&gt;\n  count(num_countries) |&gt;\n  arrange(desc(num_countries)) |&gt;\n  tt() |&gt;\n  style_tt(align = \"ld\") |&gt;\n  format_tt(escape = TRUE) \n\n\n\nTable 4: Number of countries of operation per charity\n\n\n\n\n\n    \n\n    \n\n    \n    \n    \n      \n        \n        \n              \n                num_countries\n                n\n              \n        \n        \n        \n                \n                  1\n                  1711\n                \n                \n                  2\n                  455\n                \n                \n                  3\n                  237\n                \n                \n                  4\n                  137\n                \n                \n                  5\n                  112\n                \n                \n                  6\n                  79\n                \n                \n                  7\n                  56\n                \n                \n                  8\n                  42\n                \n                \n                  9\n                  35\n                \n                \n                  10\n                  29\n                \n                \n                  More than 10\n                  187\nThe results of the following code are shown in Table 5.\nregistry |&gt;\n  unnest(subtypes) |&gt;\n  count(subtypes, sort = TRUE) |&gt;\n  head(n = 10) |&gt;\n  tt() |&gt;\n  style_tt(align = \"ld\") |&gt;\n  format_tt(escape = TRUE)\n\n\n\nTable 5: Most common charity subtypes\n\n\n\n\n\n    \n\n    \n\n    \n    \n    \n      \n        \n        \n              \n                subtypes\n                n\n              \n        \n        \n        \n                \n                  Advancing Religion\n                  16954\n                \n                \n                  Advancing social or public welfare\n                  12624\n                \n                \n                  Advancing Education\n                  11887\n                \n                \n                  PBI\n                  11696\n                \n                \n                  Purposes beneficial to ther general public and other analogous\n                  6674\n                \n                \n                  Advancing Health\n                  6305\n                \n                \n                  Advancing Culture\n                  5121\n                \n                \n                  HPC\n                  2463\n                \n                \n                  Advancing natual environment\n                  2153\n                \n                \n                  Promoting reconciliation  mutual respect and tolerance\n                  1440"
  },
  {
    "objectID": "published/acnc_registry_arrow.html#footnotes",
    "href": "published/acnc_registry_arrow.html#footnotes",
    "title": "ACNC Registry data: Arrow version",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nExecute install.packages(c(\"tidyverse\", \"arrow\", \"tinytable\", \"farr\")) within R to install all the packages you need to run the code in this note.↩︎"
  },
  {
    "objectID": "published/bikes.html",
    "href": "published/bikes.html",
    "title": "A quick look at City of Melbourne bike data",
    "section": "",
    "text": "In writing this note, I use the packages listed below.1 This note was written using Quarto and compiled with RStudio, an integrated development environment (IDE) for working with R. The source code for this note is available here and the latest version of this PDF is here.2\nlibrary(tidyverse)\nlibrary(duckdb)\nlibrary(tinytable)\nThe following code downloads the data and unzips the single file therein. It is cached (cache: true in chunk options) to save time with repeated runs of the code.\nt &lt;- tempfile(fileext = \".zip\")\nurl &lt;- str_c(\"https://opendatasoft-s3.s3.amazonaws.com/\",\n             \"downloads/archive/74id-aqj9.zip\")\ndownload.file(url, t)\nunzip(t)\nI create a database connection and load the icu extension, which contains time-zone information.3\ndb &lt;- dbConnect(duckdb::duckdb(), timezone_out = \"Australia/Melbourne\")\ndbExecute(db, \"INSTALL icu\")\ndbExecute(db, \"LOAD icu\")\nThe following SQL creates bikes_raw, which is fairly unprocessed data. Only RUNDATE is given a type, and this is TIMESTAMP because there is no time zone information in the data.\nCREATE OR REPLACE TABLE bikes_raw AS\n  SELECT *\n  FROM read_csv('74id-aqj9.csv',\n                timestampformat='%Y%m%d%H%M%S',\n                types={'RUNDATE': 'TIMESTAMP'});\nThe following SQL produces some information on the contents of bikes_raw that is shown in Table 1.\nSELECT column_name, column_type, max, null_percentage\nFROM (SUMMARIZE bikes_raw);\nTable 1: Information on unprocessed data (bikes_raw)\n\n\n\n\n\n    \n\n    \n\n    \n    \n    \n      \n        \n        \n              \n                column_name\n                column_type\n                max\n                null_percentage\n              \n        \n        \n        \n                \n                  ID\n                  BIGINT\n                  57\n                  0.00\n                \n                \n                  NAME\n                  VARCHAR\n                  Yorkshire Brewery - Wellington St - Collingwood\n                  0.00\n                \n                \n                  TERMINALNAME\n                  BIGINT\n                  60052\n                  0.00\n                \n                \n                  NBBIKES\n                  BIGINT\n                  39\n                  0.00\n                \n                \n                  NBEMPTYDOCKS\n                  BIGINT\n                  39\n                  0.00\n                \n                \n                  RUNDATE\n                  TIMESTAMP\n                  2018-09-04 10:00:10\n                  0.00\n                \n                \n                  INSTALLED\n                  BOOLEAN\n                  true\n                  0.00\n                \n                \n                  TEMPORARY\n                  BOOLEAN\n                  false\n                  0.00\n                \n                \n                  LOCKED\n                  BOOLEAN\n                  true\n                  0.00\n                \n                \n                  LASTCOMMWITHSERVER\n                  BIGINT\n                  1507119446229\n                  0.00\n                \n                \n                  LATESTUPDATETIME\n                  BIGINT\n                  1507119264599\n                  0.02\n                \n                \n                  REMOVALDATE\n                  VARCHAR\n                  NA\n                  100.00\n                \n                \n                  INSTALLDATE\n                  BIGINT\n                  1450061460000\n                  22.01\n                \n                \n                  LAT\n                  DOUBLE\n                  -37.79625\n                  0.00\n                \n                \n                  LONG\n                  DOUBLE\n                  144.988507\n                  0.00\n                \n                \n                  LOCATION\n                  VARCHAR\n                  (-37.867068, 144.976428)\n                  0.00\nThe following function is created in R, but generates SQL. The documentation for make_timestamptz() says that it returns “the TIMESTAMP WITH TIME ZONE for the given µs since the epoch.” But it seems the data we have are in milliseconds, not microseconds, so we need to multiply by 1000.\nepoch_to_ts &lt;- function(x) {\n  x &lt;- rlang::as_name(rlang::enquo(x))\n  dplyr::sql(stringr::str_c(\"make_timestamptz(\", x, \" * 1000)\"))\n}\nThe following code converts rundate to TIMESTAMPTZ assuming the original data are Melbourne times. It also converts lastcommwithserver, latestupdatetime, and installdate to TIMESTAMPTZ. Note that attention needs to be paid to time zones, because the epoch is defined as “the number of seconds since 1970-01-01 00:00:00 UTC”, which would be a different point in time from 1970-01-01 00:00:00 in Melbourne time.\nbikes &lt;-\n  tbl(db, \"bikes_raw\") |&gt;\n  rename_with(str_to_lower) |&gt;\n  select(-installed, -temporary, -removaldate) |&gt;\n  mutate(rundate = timezone(\"Australia/Melbourne\", rundate),\n         lastcommwithserver = !!epoch_to_ts(lastcommwithserver),\n         latestupdatetime = !!epoch_to_ts(latestupdatetime),\n         installdate = !!epoch_to_ts(installdate)) |&gt;\n  compute(name = \"bikes\", overwrite = TRUE)\nThe following SQL produces some information on the contents of bikes that is shown in Table 2.\nSELECT column_name, column_type, max, null_percentage\nFROM (SUMMARIZE bikes);\nTable 2: Information on processed data (bikes)\n\n\n\n\n\n    \n\n    \n\n    \n    \n    \n      \n        \n        \n              \n                column_name\n                column_type\n                max\n                null_percentage\n              \n        \n        \n        \n                \n                  id\n                  BIGINT\n                  57\n                  0.00\n                \n                \n                  name\n                  VARCHAR\n                  Yorkshire Brewery - Wellington St - Collingwood\n                  0.00\n                \n                \n                  terminalname\n                  BIGINT\n                  60052\n                  0.00\n                \n                \n                  nbbikes\n                  BIGINT\n                  39\n                  0.00\n                \n                \n                  nbemptydocks\n                  BIGINT\n                  39\n                  0.00\n                \n                \n                  rundate\n                  TIMESTAMP WITH TIME ZONE\n                  2018-09-03 20:00:10-04\n                  0.00\n                \n                \n                  locked\n                  BOOLEAN\n                  true\n                  0.00\n                \n                \n                  lastcommwithserver\n                  TIMESTAMP WITH TIME ZONE\n                  2017-10-04 08:17:26.229-04\n                  0.00\n                \n                \n                  latestupdatetime\n                  TIMESTAMP WITH TIME ZONE\n                  2017-10-04 08:14:24.599-04\n                  0.02\n                \n                \n                  installdate\n                  TIMESTAMP WITH TIME ZONE\n                  2015-12-13 21:51:00-05\n                  22.01\n                \n                \n                  lat\n                  DOUBLE\n                  -37.79625\n                  0.00\n                \n                \n                  long\n                  DOUBLE\n                  144.988507\n                  0.00\n                \n                \n                  location\n                  VARCHAR\n                  (-37.867068, 144.976428)\n                  0.00\nbikes |&gt; \n  select(lastcommwithserver, latestupdatetime, rundate, installdate) |&gt;\n  collect(n = 10)\n\n\n\nTable 3: Sample of date-time variables\n\n\n\n\n\n    \n\n    \n\n    \n    \n    \n      \n        \n        \n              \n                lastcommwithserver\n                latestupdatetime\n                rundate\n                installdate\n              \n        \n        \n        \n                \n                  2017-04-22 13:42:46.01\n                  2017-04-22 13:42:45.029\n                  2017-04-22 13:45:06\n                  2011-08-19 13:30:00\n                \n                \n                  2017-04-22 13:43:51.727\n                  2017-04-22 13:36:17.573\n                  2017-04-22 13:45:06\n                  NA\n                \n                \n                  2017-04-22 13:33:35.231\n                  2017-04-22 13:33:33.615\n                  2017-04-22 13:45:06\n                  NA\n                \n                \n                  2017-04-22 13:36:58.661\n                  2017-04-22 12:51:55.84\n                  2017-04-22 13:45:06\n                  NA\n                \n                \n                  2017-04-22 13:35:03.674\n                  2017-04-21 19:56:38.168\n                  2017-04-22 13:45:06\n                  NA\n                \n                \n                  2017-04-22 13:32:35.565\n                  2017-04-22 13:18:29.294\n                  2017-04-22 13:45:06\n                  2012-12-27 08:00:00\n                \n                \n                  2017-04-22 13:41:32.347\n                  2017-04-22 11:55:01.271\n                  2017-04-22 13:45:06\n                  NA\n                \n                \n                  2017-04-22 13:34:42.173\n                  2017-04-22 13:34:40.671\n                  2017-04-22 13:45:06\n                  NA\n                \n                \n                  2017-04-22 13:36:33.207\n                  2017-04-22 11:49:37.265\n                  2017-04-22 13:45:06\n                  NA\n                \n                \n                  2017-04-22 13:37:50.326\n                  2017-04-22 13:37:48.824\n                  2017-04-22 13:45:06\n                  2010-06-22 12:53:00\nIn making Figure 1, I convert the date component of runtime to the same date (2017-01-01). This facilitates plotting in R, as R has no native “time” type and thus things are easier using date-times. Unfortunately, it seems that all the timestamps in bikes are boring back-end times produced by systems, so there is nothing special about the distribution of these times. More interest plots might come from looking at when bikes are checked out and in (only net checkouts seem to be available) assuming that the data are sufficiently frequent.\nbikes |&gt;\n  mutate(runtime = make_timestamptz(2017L, 1L, 1L, \n                                    hour(rundate), minute(rundate), \n                                    second(rundate))) |&gt;\n  ggplot(aes(runtime)) +\n  geom_histogram(binwidth = 60 * 60) +\n  scale_x_datetime(date_breaks = \"1 hour\", date_labels = \"%H\")\n\n\n\n\n\n\n\nFigure 1: Distribution of times in runtime"
  },
  {
    "objectID": "published/bikes.html#footnotes",
    "href": "published/bikes.html#footnotes",
    "title": "A quick look at City of Melbourne bike data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nExecute install.packages(c(\"tidyverse\", duckdb\", \"tinytable\")) within R to install all the packages you need to run the code in this note.↩︎\nSome parts of the source code are ugly as I wrangled hurriedly with the output from SQL and LaTeX tables.↩︎\nYou may need to run INSTALL icu before LOAD icu depending on your DuckDB installation.↩︎"
  },
  {
    "objectID": "published/curate_call_reports.html",
    "href": "published/curate_call_reports.html",
    "title": "Data curation: The case of Call Reports",
    "section": "",
    "text": "I recently (Gow, 2026) proposed an extension to the data science “whole game” of R for Data Science (Wickham et al., 2023). In Gow (2026), I used Australian stock price data to illustrate the data curation process and, in this note, I use US bank “Call Report” data as a second illustration. In effect, I provide complete instructions for building a high-performance data library covering all Call Reports data provided by the FFIEC Bulk Data website that can be constructed in less than ten minutes on fast hardware (or a couple of hours on an older machine). I also give a few brief demonstrations of how to use the curated data, with examples for both R and Python. I conclude by discussing challenges encountered during processing and offering some observations about AI and data curation.\nMy extension of the data science “whole game”—depicted in Figure 1 below—adds a persist step to the original version, groups it with import and tidy into a single process, which I call Curate. As a complement to the new persist step, I also add a load step to the Understand process.1\nFigure 1: A representation of the data science workflow\nIn this note, as in Gow (2026), I focus on the data curation (Curate) process. My rationale for separating Curate from Understand is that I believe that thinking about these separately clarifies certain best practices in the curation of data. In Gow (2026), I used the notion of a service-level agreement to explain how the two processes can be delineated. My conception of Curate (Gow, 2026) encompasses some tasks that are included in the transform step (part of the Understand process) of Wickham et al. (2023).\nWhile I will argue that even the sole analyst who will perform all three processes can benefit from thinking about Curate separate from Understand, it is perhaps easiest to conceive of the Curate and Understand processes as involving different individuals or organizational units of the “whole game” of a data analysis workflow. In Gow (2026), I used the idea of a service-level agreement to delineate the division of responsibilities between the Curate and Understand teams. In effect, I will act as a self-appointed, single-person, unpaid Curate team and I imagine potential users of call report data as my Understand clients."
  },
  {
    "objectID": "published/curate_call_reports.html#sec-raw-data",
    "href": "published/curate_call_reports.html#sec-raw-data",
    "title": "Data curation: The case of Call Reports",
    "section": "1.1 Getting the raw data",
    "text": "1.1 Getting the raw data\nThe FFIEC Bulk Data Download site provides the Call Report data in two forms. The first is as zipped tab-delimited data files, one for each quarter. The second is as zipped XBRL data files, one for each quarter. At the time of writing, the standard approach to getting the complete data archive amounts to pointing and clicking to download each of the roughly 100 files for each format.4\nThe FFIEC data sets are not as amenable to automated downloading as those offered by other government agencies such as the SEC (see my earlier note on XBRL data), the PCAOB (see my note on Form AP data), or even the Federal Reserve itself (I used data from the MDRM site in preparing this note). However, a group of individuals has contributed the Python package ffiec_data_collector that we can use to collect the data.5\nTo install this Python package, you first need to install Python and then install the ffiec_data_collector using a command like pip install ffiec_data_collector.\nAs discussed in Appendix E of Gow and Ding (2024), I organize my raw and processed data in a repository comprising a single parent directory and several sub-directories corresponding to various data sources and projects. For some data sets, this approach to organization facilitates switching code from using (say) data sources provided by Wharton Research Data Services (WRDS) to using local data in my data repository. I will adopt that approach for the purposes of this note.\nAs the location of my “raw data” repository is found in the the environment variable RAW_DATA_DIR, I can identify that location in Python easily. The following code specifies the download directory as the directory ffiec within my raw data repository.6\n\nimport os\nfrom pathlib import Path\nprint(os.environ['RAW_DATA_DIR'])\n\n/Users/igow/Dropbox/raw_data\n\ndownload_dir = Path(os.environ['RAW_DATA_DIR']) / \"ffiec\"\n\nHaving specified a location to put the downloaded files, it is a simple matter to adapt a script provided on the package website to download the raw data files.\n\nimport ffiec_data_collector as fdc\nimport time\n\ndownloader = fdc.FFIECDownloader(download_dir=download_dir)\n\nperiods = downloader.select_product(fdc.Product.CALL_SINGLE)\n\nresults = []\nfor period in periods[:4]:\n    \n    print(f\"Downloading {period.yyyymmdd}...\", end=\" \")\n    result = downloader.download(\n        product=fdc.Product.CALL_SINGLE,\n        period=period.yyyymmdd,\n        format=fdc.FileFormat.TSV\n    )\n    results.append(result)\n    \n    if result.success:\n        print(f\"✓ ({result.size_bytes:,} bytes)\")\n    else:\n        print(f\"✗ Failed: {result.error_message}\")\n    \n    # IMPORTANT: Be respectful to government servers\n    # Add delay between requests to avoid overloading the server\n    time.sleep(1)  # 1 second delay - adjust as needed\n\nDownloading 20251231... ✓ (6,402,952 bytes)\nDownloading 20250930... ✓ (5,687,172 bytes)\nDownloading 20250630... ✓ (6,231,175 bytes)\nDownloading 20250331... ✓ (5,704,772 bytes)\n\n# Summary\nsuccessful = sum(1 for r in results if r.success)\nprint(f\"\\nCompleted: {successful}/{len(results)} downloads\")\n\n\nCompleted: 4/4 downloads\n\n\nNote that the code above downloads just the most recent four files available on the site. Remove [:4] from the line for period in periods[:4]: to download all files. Note that the package website recommends using time.sleep(5) in place of time.sleep(1) to create a five-second delay and this may be a more appropriate choice if you are downloading all 99 files using this code. Note that the downloaded files occupy about 800 MB of disk space, so make sure you have that available if running this code.\n\n1.1.1 XBRL files\nWhile this note does not use the XBRL files, you can download them using ffiec_data_collector by simply replacing TSV with XBRL in the code above. These zip files are larger than the TSV zip files, occupying about 6 GB of disk space. The ffiec.pq package does offer some rudimentary ability to process these files, but working with them is slow. To illustrate I process just one XBRL zip file.\n\nzipfiles &lt;- ffiec_list_zips(type = \"xbrl\")\nffiec_process_xbrls(zipfiles$zipfile[1]) |&gt; system_time()\n\n   user  system elapsed \n183.459  70.719 256.576 \n\n\n# A tibble: 1 × 4\n  zipfile                               date_raw date       parquet             \n  &lt;chr&gt;                                 &lt;chr&gt;    &lt;date&gt;     &lt;chr&gt;               \n1 FFIEC CDR Call Bulk XBRL 03312001.zip 20010331 2001-03-31 xbrl_20010331.parqu…"
  },
  {
    "objectID": "published/curate_call_reports.html#processing-the-data",
    "href": "published/curate_call_reports.html#processing-the-data",
    "title": "Data curation: The case of Call Reports",
    "section": "1.2 Processing the data",
    "text": "1.2 Processing the data\nWith the raw data files in hand, the next task is to process these into files useful for analysis. For reasons I will discuss below, I will process the data into Parquet files. The Parquet format is described in R for Data Science (Wickham et al., 2023, p. 393) as “an open standards-based format widely used by big data systems.” Parquet files provide a format optimized for data analysis, with a rich type system. More details on the Parquet format can be found in Chapter 22 of Wickham et al. (2023) and every code example in Gow and Ding (2024) can be executed against Parquet data files created using my db2pq Python package as described in Appendix E of that book.\nThe easiest way to run the code I used to process the data is to install the ffiec.pq R package I have made available on GitHub. And the easiest way to use the package is to set the locations for the downloaded raw data files from above and for the processed data using the environment variables RAW_DATA_DIR and DATA_DIR, respectively. By default, the ffiec.pq package assumes that the raw data files can be found in a directory ffiec that is a subdirectory of RAW_DATA_DIR. Also, the ffiec.pq package will place the processed data it creates in a directory ffiec that is a subdirectory of DATA_DIR.\nI already have these environment variables set:\n\nSys.getenv(\"RAW_DATA_DIR\")\n\n[1] \"/Users/igow/Dropbox/raw_data\"\n\nSys.getenv(\"DATA_DIR\")\n\n[1] \"/Users/igow/Dropbox/pq_data\"\n\n\nBut, even if I did not, I could set them within R using commands like the following. You should set RAW_DATA_DIR to match what you used above in Python and you should set DATA_DIR to point to the location where you want to put the processed files. The processed files will occupy about 3 GB of disk space, so make sure you have room for these there.\n\nSys.setenv(RAW_DATA_DIR=\"/Users/igow/Dropbox/raw_data\")\nSys.setenv(DATA_DIR=\"/Users/igow/Dropbox/pq_data\")\n\nHaving set these environment variables, I can load my package and run a single command ffiec_process() without any arguments to process all the raw data files.7 This takes about five minutes to run (for me):\n\nresults &lt;- ffiec_process(use_multicore = TRUE) |&gt; system_time()\n\n   user  system elapsed \n  7.489   4.616 206.225 \n\n\nNote that, behind the scenes, the ffiec_process() extracts the data in two phases. In the first phase, it processes the data for each schedule for each quarter into Parquet file. This results in 3713 Parquet files. In the second phase, ffiec_process() proceeds to organize the data in the 3713 Parquet files by variable type to facilitate working with the data. Once the data have been organized, the 3713 schedule-and-quarter-specific Parquet files are discarded.\nThe results table returned by the ffiec_process() function above reflects the outcome of the first phase, as that is when any problems arising from malformed data are expected to arise. If there were any issues in reading the data for a schedule in a quarter, then the variable ok for the corresponding row of results will be FALSE. We can easily confirm that all rows have ok equal to TRUE:\n\nresults |&gt; count(ok)\n\n# A tibble: 1 × 2\n  ok        n\n  &lt;lgl&gt; &lt;int&gt;\n1 TRUE   3713\n\n\nThe results table also includes the field repairs that we can inspect to determine if any “repairs” were made to the data as it was processed in the first phase. As can be seen, a minority of the 3713 first-phase files needed repairs. I discuss these repairs in more detail in Section 3.1.1.\n\nresults |&gt;\n  unnest(repairs) |&gt;\n  count(repairs)\n\n# A tibble: 2 × 2\n  repairs          n\n  &lt;chr&gt;        &lt;int&gt;\n1 newline-gsub   102\n2 tab-repair       2"
  },
  {
    "objectID": "published/curate_call_reports.html#using-the-data-with-r",
    "href": "published/curate_call_reports.html#using-the-data-with-r",
    "title": "Data curation: The case of Call Reports",
    "section": "2.1 Using the data with R",
    "text": "2.1 Using the data with R\nSo what has the ffiec.pq package just done? In a nutshell, it have processed each of the nearly 100 zip files into seven Parquet files, and I discuss these in turn.\n\n2.1.1 “Panel of Reporters” (POR) data\nThe first file is the “Panel of Reporters” (POR) table, which provides details on the financial institutions filing in the respective quarter.\nTo access the data using the ffiec.pq functions, we just need to create a connection to an in-memory DuckDB database, which is a simple one-liner.\n\ndb &lt;- dbConnect(duckdb::duckdb())\n\nFrom there we have the option to load a single Parquet file using the pq_file argument of the ffiec_scan_pqs() function:8\n\npor_20250930 &lt;- ffiec_scan_pqs(db, pq_file=\"por_20250930.parquet\")\npor_20250930 |&gt;\n  select(IDRSSD, financial_institution_name, everything()) |&gt;\n  head() |&gt;\n  collect()\n\n# A tibble: 6 × 13\n  IDRSSD financial_institution_name    fdic_certificate_num…¹ occ_charter_number\n   &lt;int&gt; &lt;chr&gt;                         &lt;chr&gt;                  &lt;chr&gt;             \n1     37 BANK OF HANCOCK COUNTY        10057                  &lt;NA&gt;              \n2    242 FIRST COMMUNITY BANK XENIA-F… 3850                   &lt;NA&gt;              \n3    279 BROADSTREET BANK, SSB         28868                  &lt;NA&gt;              \n4    354 BISON STATE BANK              14083                  &lt;NA&gt;              \n5    457 LOWRY STATE BANK              10202                  &lt;NA&gt;              \n6    505 BALLSTON SPA NATIONAL BANK    6959                   1253              \n# ℹ abbreviated name: ¹​fdic_certificate_number\n# ℹ 9 more variables: ots_docket_number &lt;chr&gt;,\n#   primary_aba_routing_number &lt;chr&gt;, financial_institution_address &lt;chr&gt;,\n#   financial_institution_city &lt;chr&gt;, financial_institution_state &lt;chr&gt;,\n#   financial_institution_zip_code &lt;chr&gt;,\n#   financial_institution_filing_type &lt;chr&gt;,\n#   last_date_time_submission_updated_on &lt;dttm&gt;, date &lt;date&gt;\n\npor_20250930 |&gt; count() |&gt; collect()\n\n# A tibble: 1 × 1\n      n\n  &lt;dbl&gt;\n1  4435\n\n\nBut it will generally be more convenient to just read all files in one step, which we can do like this.\n\npor &lt;- ffiec_scan_pqs(db, \"por\")\npor |&gt;\n  select(IDRSSD, financial_institution_name, everything()) |&gt;\n  head() |&gt;\n  collect() \n\n# A tibble: 6 × 13\n  IDRSSD financial_institution_name    fdic_certificate_num…¹ occ_charter_number\n   &lt;int&gt; &lt;chr&gt;                         &lt;chr&gt;                  &lt;chr&gt;             \n1     37 BANK OF HANCOCK COUNTY        10057                  &lt;NA&gt;              \n2    242 FIRST NATIONAL BANK OF XENIA… 3850                   12096             \n3    279 MINEOLA COMMUNITY BANK, SSB   28868                  &lt;NA&gt;              \n4    354 BISON STATE BANK              14083                  &lt;NA&gt;              \n5    439 PEOPLES BANK                  16498                  &lt;NA&gt;              \n6    457 LOWRY STATE BANK              10202                  &lt;NA&gt;              \n# ℹ abbreviated name: ¹​fdic_certificate_number\n# ℹ 9 more variables: ots_docket_number &lt;chr&gt;,\n#   primary_aba_routing_number &lt;chr&gt;, financial_institution_address &lt;chr&gt;,\n#   financial_institution_city &lt;chr&gt;, financial_institution_state &lt;chr&gt;,\n#   financial_institution_zip_code &lt;chr&gt;,\n#   financial_institution_filing_type &lt;chr&gt;,\n#   last_date_time_submission_updated_on &lt;dttm&gt;, date &lt;date&gt;\n\npor |&gt; count() |&gt; collect()\n\n# A tibble: 1 × 1\n       n\n   &lt;dbl&gt;\n1 662363\n\n\n\n\n2.1.2 Item-schedules data\nThe second data set is ffiec_schedules. The zip files provided by the FFIEC Bulk Data site comprise several TSV files organized into “schedules” corresponding the particular forms on which the data are submitted by filers. While the ffiec.pq package reorganizes the data by data type, information about the original source files for the data are retained in ffiec_schedules. We can load this using the following code:\n\nffiec_schedules &lt;- ffiec_scan_pqs(db, \"ffiec_schedules\")\n\nAnd here are the first 10 rows of this data set.\n\nffiec_schedules |&gt; head(10) |&gt; collect()\n\n# A tibble: 10 × 3\n   item     schedule  date      \n   &lt;chr&gt;    &lt;list&gt;    &lt;date&gt;    \n 1 RCFD0010 &lt;chr [2]&gt; 2001-03-31\n 2 RCFD0022 &lt;chr [1]&gt; 2001-03-31\n 3 RCFD0071 &lt;chr [1]&gt; 2001-03-31\n 4 RCFD0073 &lt;chr [1]&gt; 2001-03-31\n 5 RCFD0074 &lt;chr [1]&gt; 2001-03-31\n 6 RCFD0081 &lt;chr [1]&gt; 2001-03-31\n 7 RCFD0083 &lt;chr [1]&gt; 2001-03-31\n 8 RCFD0085 &lt;chr [1]&gt; 2001-03-31\n 9 RCFD0090 &lt;chr [1]&gt; 2001-03-31\n10 RCFD0211 &lt;chr [1]&gt; 2001-03-31\n\n\nFocusing on one item, RIAD4230, we can see from the output below that this item was provided on both Schedule RI (ri) and Schedule RI-BII (ribii) from 2001-03-31 until 2018-12-31, but since then has only been provided on Schedule RI-BII.\n\nffiec_schedules |&gt; \n  filter(item == \"RIAD4230\") |&gt;\n  mutate(schedule = unnest(schedule)) |&gt;\n  group_by(item, schedule) |&gt;\n  summarize(min_date = min(date, na.rm = TRUE),\n            max_date = max(date, na.rm = TRUE),\n            .groups = \"drop\") |&gt;\n  collect()\n\n# A tibble: 2 × 4\n  item     schedule min_date   max_date  \n  &lt;chr&gt;    &lt;chr&gt;    &lt;date&gt;     &lt;date&gt;    \n1 RIAD4230 ri       2001-03-31 2018-12-31\n2 RIAD4230 ribii    2001-03-31 2025-12-31\n\n\nThe next question might be: What is RIAD4230? We can get the answer from ffiec_items, a data set included with the ffiec.pq package:\n\nffiec_items |&gt; filter(item == \"RIAD4230\")\n\n# A tibble: 1 × 5\n  item     mnemonic item_code item_name                           data_type\n  &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;                               &lt;chr&gt;    \n1 RIAD4230 RIAD     4230      Provision for loan and lease losses Float64  \n\n\nSchedule RI is the income statement and “Provision for loan and lease losses” is an expense we would expect to see there for a financial institution. Schedule RI-BII is “Charge-offs and Recoveries on Loans and Leases” and provides detail on loan charge-offs and recoveries, broken out by loan category, for the reporting period. As part of processing the data, the ffiec.pq package confirms that the value for any given item for a specific IDRSSD and date is the same across schedules for all items in the data.\nEach of the other five files represents data from the schedules for that quarter for a particular data type, as shown in Table 1:\n\n\n\nTable 1: Table keys, arrow types, and access code\n\n\n\n\n\nKey\nArrow type\nAccess code\n\n\n\n\nfloat\nFloat64\nffiec_scan_pqs(db, \"ffiec_float\")\n\n\nint\nInt32\nffiec_scan_pqs(db, \"ffiec_int\")\n\n\nstr\nUtf8\nffiec_scan_pqs(db, \"ffiec_str\")\n\n\ndate\nDate32\nffiec_scan_pqs(db, \"ffiec_date\")\n\n\nbool\nBoolean\nffiec_scan_pqs(db, \"ffiec_bool\")\n\n\n\n\n\n\nWe can use the data set ffiec_items to find out where a variable is located, based on its Arrow type.\n\nffiec_items\n\n# A tibble: 5,141 × 5\n   item     mnemonic item_code item_name                               data_type\n   &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;                                   &lt;chr&gt;    \n 1 RCFA2170 RCFA     2170      Total assets                            Float64  \n 2 RCFA3128 RCFA     3128      Allocated transfer risk reserves        Float64  \n 3 RCFA3792 RCFA     3792      Total qualifying capital allowable und… Float64  \n 4 RCFA5310 RCFA     5310      General loan and lease valuation allow… Float64  \n 5 RCFA5311 RCFA     5311      Tier 2 (supplementary) capital          Float64  \n 6 RCFA7204 RCFA     7204      Tier 1 leverage capital ratio           Float64  \n 7 RCFA7205 RCFA     7205      Total risk-based capital ratio          Float64  \n 8 RCFA7206 RCFA     7206      Tier 1 risk-based capital ratio         Float64  \n 9 RCFA8274 RCFA     8274      Tier 1 capital allowable under the ris… Float64  \n10 RCFAA223 RCFA     A223      Risk-weighted assets (net of allowance… Float64  \n# ℹ 5,131 more rows\n\n\nAs might be expected, most variables have type Float64 and will be found in the ffiec_float tables.\n\nffiec_items |&gt; count(data_type, sort = TRUE)\n\n# A tibble: 5 × 2\n  data_type     n\n  &lt;chr&gt;     &lt;int&gt;\n1 Float64    4909\n2 Int32       115\n3 String       73\n4 Boolean      42\n5 Date32        2\n\n\n\n\n2.1.3 Example 1: Do bank balance sheets balance?\nIf we were experts in Call Report data, we might know that domestic total assets is reported as item RCFD2170 (on Schedule RC) for banks reporting on a consolidated basis and as item RCON2170 for banks reporting on an unconsolidated basis. We might also know about RCFD2948 and RCFD3210 and so on. But we don’t need to be experts to see what these items relate to:\n\nbs_items &lt;- c(\"RCFD2170\", \"RCON2170\",\n              \"RCFD2948\", \"RCON2948\",\n              \"RCFD3210\", \"RCON3210\",\n              \"RCFD3000\", \"RCON3000\")\n\nffiec_items |&gt; filter(item %in% bs_items)\n\n# A tibble: 8 × 5\n  item     mnemonic item_code item_name                                data_type\n  &lt;chr&gt;    &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;                                    &lt;chr&gt;    \n1 RCFD2170 RCFD     2170      Total assets                             Float64  \n2 RCFD2948 RCFD     2948      Total liabilities and minority interest  Float64  \n3 RCFD3000 RCFD     3000      Minority interest in consolidated subsi… Float64  \n4 RCFD3210 RCFD     3210      Total equity capital                     Float64  \n5 RCON2170 RCON     2170      Total assets                             Float64  \n6 RCON2948 RCON     2948      Total liabilities and minority interest  Float64  \n7 RCON3000 RCON     3000      Minority interest in consolidated subsi… Float64  \n8 RCON3210 RCON     3210      Total equity capital                     Float64  \n\n\nThe output above suggests we can make a “top level” balance sheet table using these items. The following code uses the DuckDB instance we created above (db) and the code provided in Table 1, to create ffiec_float, a “lazy” data table. Here “lazy” is a good thing, as it means we have access to all the data without having to load anything into RAM. As a result, this operation takes almost no time.\n\nffiec_float &lt;- ffiec_scan_pqs(db, \"ffiec_float\") |&gt; system_time()\n\n   user  system elapsed \n  0.036   0.016   0.015 \n\nffiec_float\n\n# A query:  ?? x 4\n# Database: DuckDB 1.4.4 [igow@Darwin 25.4.0:R 4.5.2/:memory:]\n   IDRSSD date       item        value\n    &lt;int&gt; &lt;date&gt;     &lt;chr&gt;       &lt;dbl&gt;\n 1     37 2001-03-31 RCON3562   0     \n 2     37 2001-03-31 RCON7701   0     \n 3     37 2001-03-31 RCON7702   0     \n 4    242 2001-03-31 RCON3562 329     \n 5    242 2001-03-31 RCON7701   0.08  \n 6    242 2001-03-31 RCON7702   0.085 \n 7    279 2001-03-31 RCON3562  27     \n 8    279 2001-03-31 RCON7701   0.0745\n 9    279 2001-03-31 RCON7702   0.08  \n10    354 2001-03-31 RCON3562   4     \n# ℹ more rows\n\n\nAs can be seen, the data in ffiec_float are in a long format, with each item for each bank for each period being a single row.\nI then filter() to get data on just the items in bs_items and then use the the convenience function ffiec_pivot() from the ffiec.pq package to turn the data into a more customary wide form. I then use coalesce() to get the consolidated items (RCFD) where available and the unconsolidated items (RCON) otherwise. Because I compute() this table (i.e., actually calculate the values for each row and column), this step takes a relatively long time, but not too long given that the underlying data files are in the order of tens of gigabytes if loaded in RAM.9\n\nbs_data &lt;-\n  ffiec_float |&gt; \n  ffiec_pivot(items = bs_items) |&gt;\n  mutate(total_assets = coalesce(RCFD2170, RCON2170),\n         total_liabilities = coalesce(RCFD2948, RCON2948),\n         equity = coalesce(RCFD3210, RCON3210),\n         nci = coalesce(RCFD3000, RCON3000)) |&gt;\n  mutate(eq_liab = total_liabilities + equity + nci) |&gt;\n  compute() |&gt;\n  system_time()\n\n   user  system elapsed \n  5.086   0.758   0.800 \n\n\nSo, do balance sheets balance? Well, not always.10\n\nbs_data |&gt;\n  count(bs_balance = eq_liab == total_assets) |&gt;\n  collect()\n\n# A tibble: 2 × 2\n  bs_balance      n\n  &lt;lgl&gt;       &lt;dbl&gt;\n1 TRUE       640570\n2 FALSE       21793\n\n\nWhat’s going on? Well, one possibility is simply rounding error. So in the following code, I set imbalance_flag to TRUE only if the gap is more than 1 (these are in thousands of USD).\n\nbalanced &lt;-\n  bs_data |&gt;\n  mutate(imbalance = total_assets - eq_liab,\n         imbalance_flag = abs(total_assets - eq_liab) &gt; 1) |&gt;\n  select(-starts_with(\"RC\"), -total_liabilities) |&gt;\n  collect()\n\nThis helps a lot. Now it seems that balance sheets usually balance, but not always.\n\nbalanced |&gt;\n  count(imbalance_flag)\n\n# A tibble: 2 × 2\n  imbalance_flag      n\n  &lt;lgl&gt;           &lt;int&gt;\n1 FALSE          662164\n2 TRUE              199\n\n\nThe vast majority of apparent imbalances are small …\n\nbalanced |&gt;\n  filter(imbalance_flag) |&gt;\n  select(IDRSSD, date, total_assets, imbalance) |&gt;\n  arrange(desc(imbalance))\n\n# A tibble: 199 × 4\n    IDRSSD date       total_assets imbalance\n     &lt;int&gt; &lt;date&gt;            &lt;dbl&gt;     &lt;dbl&gt;\n 1 1362246 2002-06-30       278586      5334\n 2  664653 2002-03-31        25490       224\n 3 2821825 2001-03-31        55220        30\n 4 3097243 2005-06-30        40939        10\n 5  920733 2001-12-31        48439        10\n 6  293053 2002-03-31       314157        10\n 7  678931 2001-09-30        31237        10\n 8   83151 2003-12-31        97324        10\n 9  178150 2004-03-31       103939        10\n10  536554 2001-03-31         4624        10\n# ℹ 189 more rows\n\n\n… and they’re all at least twenty years ago.\n\nbalanced |&gt;\n  filter(imbalance_flag) |&gt;\n  select(IDRSSD, date, total_assets, imbalance) |&gt;\n  arrange(desc(date))\n\n# A tibble: 199 × 4\n    IDRSSD date       total_assets imbalance\n     &lt;int&gt; &lt;date&gt;            &lt;dbl&gt;     &lt;dbl&gt;\n 1 3097243 2005-06-30        40939        10\n 2   33549 2005-06-30        43865         3\n 3   20147 2005-06-30        48130       -10\n 4  131940 2005-06-30        27432         8\n 5  475756 2005-06-30        68158         2\n 6  773546 2005-06-30        97558         2\n 7   22954 2005-06-30        65027       -10\n 8 2646327 2005-03-31        98814        -8\n 9   42037 2005-03-31        60913         2\n10  827953 2005-03-31        50080         5\n# ℹ 189 more rows\n\n\n\n\n2.1.4 Example 2: When do banks submit their Call Reports?\nWorking with dates and times (temporal data) can be a lot more complicated than is generally appreciated. Broadly speaking we might think of temporal data as referring to points in time, or instants, or to time spans, which include durations, periods, and intervals.11 Instants might refer to moments in time (e.g., in UTC) or as times of day (in local time).\nSuppose I were interested in understanding the times of day at which financial institutions submit their Call Reports. It seems that financial institutions file Call Reports with their primary federal regulator through the FFIEC’s Central Data Repository. So I am going to use the America/New_York time zone as the relevant local time zone for this analysis, as the FFIEC is based in Washington, DC.\nTo illustrate some subtleties of working with time zones, I will set my computer to a different time zone from that applicable to where I am: Australia/Sydney, as seen in Figure 2.12\n\n\n\n\n\n\n\n\nFigure 2: Setting my computer to a different time zone\n\n\n\n\n\nNow, R sees my time zone as Australia/Sydney:\n\nSys.timezone()\n\n[1] \"America/New_York\"\n\n\nIf we look at the underlying zip file for 2025-09-30, you will see that last_date_time_submission_updated_on for the bank with IDRSSD of 37 is \"2026-01-13T10:13:21\". In creating the ffiec.pq package, I assumed that this is a timestamp in America/New_York time.\nHow does that show up when I look at the processed data in R using DuckDB?\n\ndb &lt;- dbConnect(duckdb::duckdb())\n\npor &lt;- ffiec_scan_pqs(db, \"por\")\n\npor_default &lt;-\n  por |&gt; \n  filter(IDRSSD == 37, date == \"2025-09-30\") |&gt; \n  rename(dttm = last_date_time_submission_updated_on) |&gt;\n  mutate(dttm_text = as.character(dttm)) |&gt; \n  select(IDRSSD, date, dttm, dttm_text) |&gt; \n  collect()\n\ndbDisconnect(db)\n\npor_default\n\n# A tibble: 1 × 4\n  IDRSSD date       dttm                dttm_text             \n   &lt;int&gt; &lt;date&gt;     &lt;dttm&gt;              &lt;chr&gt;                 \n1     37 2025-09-30 2026-01-13 15:13:21 2026-01-13 15:13:21+00\n\npor_default$dttm[1]\n\n[1] \"2026-01-13 15:13:21 UTC\"\n\n\nBy default, R/DuckDB is showing this to me as UTC. This is fine, but I want to analyse this as a local time. Here is how I can achieve this. First, I set the R variable tz to \"America/New_York\".\n\ntz &lt;- \"America/New_York\"\n\nSecond, I connect to DuckDB anew, but I tell it I want it to use \"America/New_York\" as the time zone of output. But it’s important to note that this is just a “presentation layer” and doesn’t change how the database itself “thinks about” timestamps.\n\ndb &lt;- dbConnect(duckdb::duckdb(), timezone_out = tz)\n\nThird, I make DuckDB a “time zone wizard” but installing and loading the icu extension. This extension enables region-dependent collations and time zones. The icu extension is probably not installed and enabled by default because it is large and not all applications need these features. This allows me to set the DuckDB server’s time zone to America/New_York.\n\nrs &lt;- dbExecute(db, \"INSTALL icu\")\nrs &lt;- dbExecute(db, \"LOAD icu\")\nrs &lt;- dbExecute(db, str_glue(\"SET TimeZone TO '{tz}'\"))\n\nThen I run the query from above again.\n\npor &lt;- ffiec_scan_pqs(db, \"por\")\npor_ny &lt;-\n  por |&gt; \n  filter(IDRSSD == 37, date == \"2025-09-30\") |&gt; \n  rename(dttm = last_date_time_submission_updated_on) |&gt;\n  mutate(dttm_text = as.character(dttm)) |&gt; \n  select(IDRSSD, date, dttm, dttm_text) |&gt;\n  collect()\n\npor_ny\n\n# A tibble: 1 × 4\n  IDRSSD date       dttm                dttm_text             \n   &lt;int&gt; &lt;date&gt;     &lt;dttm&gt;              &lt;chr&gt;                 \n1     37 2025-09-30 2026-01-13 10:13:21 2026-01-13 10:13:21-05\n\npor_ny$dttm[1]\n\n[1] \"2026-01-13 10:13:21 EST\"\n\n\nNow, we see that everything is in America/New_York local time, including the way the server sees the data (dttm_text) and how it’s presented to the R user.\nNow that we have things working in local time, I will make a couple of plots of submission times. To show times on a single scale, I use the fudge of making them all times on a given day, which I somewhat arbitrarily choose to be 2025-01-01.13\nIn the first plot—Figure 3—I present submission times divided by whether banks are located in “western states” or not. It does seem that western banks file later.\n\nwestern_states &lt;- c(\"HI\", \"WA\", \"CA\", \"AK\", \"OR\", \"NV\")\n\nplot_data &lt;-\n  por |&gt; \n  rename(last_update = last_date_time_submission_updated_on) |&gt;\n  mutate(\n    q4 = quarter(date) == 4,\n    offset = last_update - sql(\"last_update AT TIME ZONE 'UTC'\"),\n    offset = date_part(\"epoch\", offset) / 3600,\n    tzone = if_else(offset == -4, \"EDT\", \"EST\"),\n    west = financial_institution_state %in% western_states,\n    ref = sql(str_glue(\"TIMESTAMPTZ '2025-01-01 00:00:00 {tz}'\")),\n    sub_date = date_trunc('days', last_update)) |&gt;\n  mutate(time_adj = last_update - sub_date + ref) |&gt;\n  select(IDRSSD, date, last_update, time_adj, west, q4, offset, tzone) |&gt;\n  collect()\n\n\n\n\n\n\n\n\n\nFigure 3: Submission times by year and region\n\n\n\n\n\nIn the second plot—Figure 4—I present submission times divided by whether America/New_York is on Eastern Daylight Time (EDT) or Eastern Standard Time (EST). Looking at the plot, it seems that submission times have a similar distribution across the two time zones, suggesting that banks do not follow UTC, in which case there should be a difference in distributions for EDT and EST.\nOne can definitely see a “lunch hour” and the submissions appear more likely to involve someone clicking a “Submit” button in some software package rather than IT setting up some overnight automated submission.\n\n\n\n\n\n\n\n\nFigure 4: Submission times by year and US/Eastern time zone"
  },
  {
    "objectID": "published/curate_call_reports.html#using-the-data-with-python",
    "href": "published/curate_call_reports.html#using-the-data-with-python",
    "title": "Data curation: The case of Call Reports",
    "section": "2.2 Using the data with Python",
    "text": "2.2 Using the data with Python\n\n2.2.1 Example 3: Plotting trends in total assets for the biggest banks\nLest you think that, because ffiec.pq is an R package, the processed data are of no interest to others, I now provide some basic analysis using Python. For this, I am going to use the Polars package rather than pandas.\nWhile pandas is the dominant data frame library in Python, it would struggle to work with Parquet data on the scale of what ffiec.pq has produced, even though it’s a fairly modest amount of data. Loading 50-100 GB of data into RAM is not fun for most people’s computer set-ups. Even if you have RAM in the hundreds of GBs, not loading it will save you time.\nAs we shall see, Polars does fine with the data and, if anything, is noticeably faster than DuckDB (using dplyr) for the queries I use in this note.\n\nfrom pathlib import Path\nimport polars as pl\nimport os\n\nBecause there is no ffiec.pq package for Python, I mimic the ffiec_scan_pqs() function using the following code.\n\ndef ffiec_scan_pqs(schedule=None, *, \n                   schema=\"ffiec\", data_dir=None):\n    if data_dir is None:\n        data_dir = Path(os.environ[\"DATA_DIR\"]).expanduser()\n\n    path = data_dir / schema if schema else data_dir\n\n    if schedule is None:\n        raise ValueError(\"You must supply `schedule`.\")\n    files = list(path.glob(f\"{schedule}_*.parquet\"))\n    if not files:\n        raise FileNotFoundError(\n          f\"No Parquet files found for schedule '{schedule}' in {path}\"\n        )\n    \n    return pl.concat([pl.scan_parquet(f) for f in files])\n\nNow I can “load” the data much as I did with R.\n\nffiec_float = ffiec_scan_pqs(\"ffiec_float\")\npor = ffiec_scan_pqs(schedule=\"por\")\n\nWhile I am going to focus on total assets in this analysis, I show the parallels between the R code and the Polars code by collecting data on the same items. I don’t need ffiec_pivot() with Polars because the built-in .pivot() method does everything I need. Polars is even faster than R/DuckDB.\n\nimport time\n\nbs_items = [\"RCFD2170\", \"RCON2170\",\n            \"RCFD2948\", \"RCON2948\",\n            \"RCFD3210\", \"RCON3210\",\n            \"RCFD3000\", \"RCON3000\"]\nstart = time.perf_counter()\nbs_data = (\n    ffiec_float\n    .filter(pl.col(\"item\").is_in(bs_items))\n    .pivot(\n        on = \"item\",\n        on_columns = bs_items,\n        index = [\"IDRSSD\", \"date\"],\n        values = \"value\")\n    .with_columns(\n        total_assets = pl.coalesce(pl.col(\"RCFD2170\"), pl.col(\"RCON2170\")),\n        total_liabs = pl.coalesce(pl.col(\"RCFD2948\"), pl.col(\"RCON2948\")),\n        equity = pl.coalesce(pl.col(\"RCFD3210\"), pl.col(\"RCON3210\")),\n        nci = pl.coalesce(pl.col(\"RCFD3000\"), pl.col(\"RCON3000\")),\n    )\n    .with_columns(\n        eq_liab = pl.col(\"total_liabs\") + pl.col(\"equity\") + pl.col(\"nci\")\n    )\n    .collect()\n)\nend = time.perf_counter()\nelapsed = end - start\nprint(f'Time taken: {elapsed:.6f} seconds')\n\nTime taken: 0.166538 seconds\n\n\nWe can peek at the data. So Polars has processed GBs of data and created a table with over 600,000 rows in well under a second. Don’t try this at home … if you’re using pandas.\nNote that ffiec_float is a pl.LazyFrame, but .collect() creates a non-lazy pl.DataFrame.\n\nimport polars.selectors as cs\n\nbs_data.select(cs.exclude(\"^(RCON|RCFD).*$\"))\n\n\nshape: (662_363, 7)\n\n\n\nIDRSSD\ndate\ntotal_assets\ntotal_liabs\nequity\nnci\neq_liab\n\n\ni32\ndate\nf64\nf64\nf64\nf64\nf64\n\n\n\n\n220527\n2015-06-30\n1.035031e6\n920130.0\n114901.0\n0.0\n1.035031e6\n\n\n426758\n2011-12-31\n80344.0\n71670.0\n8674.0\n0.0\n80344.0\n\n\n435750\n2025-06-30\n1.812065e6\n1.679108e6\n132957.0\n0.0\n1.812065e6\n\n\n804150\n2021-12-31\n56733.0\n49086.0\n7647.0\n0.0\n56733.0\n\n\n850456\n2025-06-30\n315792.0\n287334.0\n28458.0\n0.0\n315792.0\n\n\n…\n…\n…\n…\n…\n…\n…\n\n\n73031\n2007-12-31\n150664.0\n135391.0\n15273.0\n0.0\n150664.0\n\n\n179335\n2012-12-31\n189270.0\n163135.0\n26135.0\n0.0\n189270.0\n\n\n809539\n2015-03-31\n258152.0\n229046.0\n29106.0\n0.0\n258152.0\n\n\n924058\n2004-06-30\n96340.0\n88985.0\n7355.0\n0.0\n96340.0\n\n\n771140\n2012-09-30\n507492.0\n456884.0\n50608.0\n0.0\n507492.0\n\n\n\n\n\n\nI identify the top 5 banks by assets on 2025-09-30. Because I will want to merge this with information on por, which is a lazy data frame (pl.LazyFrame), I make it top_5 “lazy” by appending .lazy() at the end.\n\ntop_5 = (\n    bs_data\n    .filter(pl.col(\"date\") == pl.date(2025, 9, 30))\n    .sort(\"total_assets\", descending=True)\n    .with_row_index(\"ta_rank\", offset=1)\n    .filter(pl.col(\"ta_rank\") &lt;= 5)\n    .lazy()\n)\n\nI then grab the names of the banks from por.\n\ntop_5_names = (\n    top_5\n    .join(\n        por.select([\"IDRSSD\", \"date\", \"financial_institution_name\"]),\n        on=[\"IDRSSD\", \"date\"],\n        how=\"inner\",\n    )\n    .sort(\"ta_rank\")\n    .select([\"IDRSSD\", \"financial_institution_name\", \"ta_rank\"])\n    .rename({\"financial_institution_name\": \"bank\"})\n    .collect()\n)\n\n\ntop_5_names\n\n\nshape: (5, 3)\n\n\n\nIDRSSD\nbank\nta_rank\n\n\ni32\nstr\nu32\n\n\n\n\n852218\n\"JPMORGAN CHASE BANK, NATIONAL …\n1\n\n\n480228\n\"BANK OF AMERICA, NATIONAL ASSO…\n2\n\n\n476810\n\"CITIBANK, N.A.\"\n3\n\n\n451965\n\"WELLS FARGO BANK, NATIONAL ASS…\n4\n\n\n504713\n\"U.S. BANK NATIONAL ASSOCIATION\"\n5\n\n\n\n\n\n\nI can combine the names with bs_data using .join().\n\nbs_panel_data = bs_data.join(top_5_names, on=\"IDRSSD\", how=\"inner\")\n\nUsers of pandas who are unfamiliar might be impressed by the performance of Polars, but wonder how they can fit it into their workflows. Of course, it is easy enough to call .to_pandas() and create a pandas pd.DataFrame:\n\npdf = (\n    bs_panel_data\n    .filter(pl.col(\"date\") &gt;= pl.date(2020, 1, 1))\n    .to_pandas()\n)\n\nThis means a pandas user can use all the familiar tools used for plotting or statistical analysis. Because the names are a little long for plotting purposes, I use a little dictionary to replace them.\n\nbank_names = {\n    476810: \"Citibank\",\n    504713: \"US Bank\",\n    852218: \"JPMorgan Chase\",\n    451965: \"Wells Fargo\",\n    480228: \"Bank of America\",\n}\n\npdf[\"bank\"] = pdf[\"IDRSSD\"].map(bank_names)\n\nAnd then I use Seaborn and Matplotlib to make a small (but uninspiring) plot.\n\n\n\n\n\n\n\n\nFigure 5: Total assets for top 5 banks"
  },
  {
    "objectID": "published/curate_call_reports.html#reading-the-data",
    "href": "published/curate_call_reports.html#reading-the-data",
    "title": "Data curation: The case of Call Reports",
    "section": "3.1 Reading the data",
    "text": "3.1 Reading the data\nEach quarter’s zip file (zipfile) actually contains dozens of text files (.txt) in TSV (“tab-separated values”) form. The TSV is a close relative of the CSV (“comma-separated values”) and the principles applicable to one form apply to the other.\nI have seen code that just imports from these individual files (what I call inner_file) in some location on the user’s hard drive. This approach is predicated on the user having downloaded the zip files and unzipped them. While we have downloaded all the zip files—assuming you followed the steps outlined in Section 1.1—I don’t want to be polluting my hard drive (or yours) with thousands of .txt files that won’t be used after reading them once.\nInstead, R allows me to simply say:\n\ncon &lt;- unz(zipfile, inner_file)\n\nThe resulting con object is a temporary read-only connection to a single text file (inner_file) stored inside the zip file zipfile. The object allows R to stream the file’s contents directly from the zip archive, line by line, without extracting it. Given con, the core function used to read the data into R has the following basic form, where read_tsv() comes from the readr package, part of the Tidyverse:\n\ndf &lt;- read_tsv(\n  con,\n  col_names = cols,\n  col_types = colspec,\n  skip = skip,\n  quote = \"\",\n  na = c(\"\", \"CONF\"),\n  progress = FALSE,\n  show_col_types = FALSE\n)\n\n\n3.1.1 Handling embedded newlines and tabs\nExperienced users of the readr package might wince a little at the quote = \"\" argument above. What this means is that the data are not quoted. Wickham et al. (2023, p. 101) points out that “sometimes strings in a CSV file contain commas. To prevent them from causing problems, they need to be surrounded by a quoting character, like \" or '. By default, read_csv() assumes that the quoting character will be \".”\nAdapting this to our context and expanding it slightly, I would say: “sometimes strings in a TSV file contain tabs (\\t) and newline characters (\\n).14 To prevent them from causing problems, they need to be surrounded by a quoting character, like \" or '.” While this is a true statement, the TSV files provided on the FFIEC Bulk Data website are not quoted, which means that tabs and newlines characters embedded in strings will cause problems.\nThe approach taken by the ffiec.pq package is to attempt to read the data using a call like that above, which I term the “fast path” (in part because it is indeed fast). Before making that call, the code has already inspected the first row of the file to determine the column names (stored in cols) and used those column names to look up the appropriate type for each column (stored in colspecs). Any anomaly caused by embedded newlines or embedded tabs will almost certainly cause this first read_tsv() call to fail. But if there are no issues, then we pretty much have the data as we want them and can return df to the calling function.15 Fortunately, over 95% of files can be read successfully on the “fast path”.\nIt turns out that if the “fast path” read fails, the most likely culprit is embedded newlines. Let’s say the table we’re trying to read has seven columns and the text field that is the fourth field in the file contains, in some row of the data, an embedded newline, because the data submitted by the reporting financial institution contained \\n in that field. Because read_tsv() processes the data line by line and lines are “delimited” by newline characters (\\n), it will see the problematic line as terminating part way through the fourth column and, because cols tells read_tsv() to expect seven columns, read_tsv() will issue a warning.\nWhen a warning occurs on the “fast path”, the read function in ffiec.pq moves to what I call (unimaginatively) the “slow path”. A “feature” (it turns out) of the TSV files provided on the FFIEC Bulk Data website is that each line ends with not just \\n, but \\t\\n. This means we can assume that any \\n not preceded by \\t is an embedded newline, not a line-terminating endline.16 So I can read the data into the variable txt using readLines() and use a regular expression to replace embedded newlines with an alternative character. The alternative I use is a space and I use the gsub() function to achieve this: gsub(\"(?&lt;!\\\\t)\\\\n\", \" \", txt, perl = TRUE) The regular expression here is (?&lt;!\\\\t)\\\\n is equivalent to (?&lt;!\\t)\\n in Perl or r\"(?&lt;!\\t)\\n\" in Python.17 In words, the regular expression literally means “any newline character that is not immediately preceded by a tab” and the gsub() function will replace such characters with spaces (\" \") because that is the second argument to gsub().\nThis fix addresses almost all the problematic files. “Almost all” means “all but two”. The issue with the remaining two files is (as you might have guessed) embedded tabs. Unfortunately, there’s no easy “identify the embedded tabs and replace them” fix, because there’s no easy way to distinguish embedded tabs from delimiting tabs. However, we can detect the presence of embedded tabs in an affected from the existence of too many tabs in that row.\nFor one of the two “bad” files, there is only one text field, so once we detect the presence of the embedded tab, we can assume that the extra tab belongs in that field: Problem solved. For the other “bad” file, there are several text fields and, while there is just one bad row, we cannot be sure which text field has the embedded tab. The ffiec.pq package just assumes that the embedded tab belongs in the last field and moves on. This means that the textual data for one row (i.e., one financial institution) for one schedule for one quarter cannot be guaranteed to be completely correct.18 Such is life.\nEven without addressing the issue, given that only two files are affected, it’s possible to measure the “damage” created by embedded tabs.\nLet’s look at the earlier file, which turns out to affect the Schedule RIE data for The Traders National Bank (IDRSSD of 490937) for June 2004.19 Traders National Bank in Tennessee was “Tullahoma’s second oldest bank”, until changing its name to Wellworth Bank (seemingly after some mergers), and it listed total assets of $117,335,000 in the affected Call Report.\nLet’s look at the textual data we have in our Parquet files for this case:20\n\nffiec_str &lt;- ffiec_scan_pqs(db, schedule = \"ffiec_str\") \n\nffiec_str |&gt; \n  filter(IDRSSD == \"490937\", date == \"2004-06-30\") |&gt; \n  select(IDRSSD, item, value) |&gt; \n  filter(!is.na(value)) |&gt;\n  collect() |&gt;\n  system_time()\n\n   user  system elapsed \n  0.018   0.023   0.019 \n\n\n# A tibble: 6 × 3\n  IDRSSD item     value                                                   \n   &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;                                                   \n1 490937 TEXT4464 ATM Fees, Exam Fees, Dues & Chairitable Cont            \n2 490937 TEXT4467 Telephone, BanClub, Federal Res Fees, NSF and Other Loss\n3 490937 TEXT4468 Courier, Audit Tax, Deff Comp, Other                    \n4 490937 TEXT4469 ns Exp, Bus Dev, P                                      \n5 490937 TEXT3549 IENC SECURITIES                                         \n6 490937 TEXT3550 IENC CD'S                                               \n\n\nWe can compare this with what we see in the Call Report in Figure 6, where we see that the value of TEXT4468 should be something like \"Courier, Audit Tax, Deff Comp, Other\\tns Exp, Bus Dev, P\". The embedded tab has split this into \"Courier, Audit Tax, Deff Comp, Other\" for TEXT4468 and \"ns Exp, Bus Dev, P\" for TEXT4469, which should be NA. If the values for TEXT4468and TEXT4469 for Traders National Bank in June 2004 are important to your analysis, you could fix this “by hand” easily enough.\n\n\n\n\n\n\n\n\nFigure 6: Extract from June 2004 Call Report for The Traders National Bank\n\n\n\n\n\nLooking at the later file, there were embedded tabs in two rows of Schedule NARR for December 2022. I compared the values in the Parquet file with those in the Call Reports for the two affected banks and the values in the Parquet file match perfectly.21 Because Schedule NARR (“Optional Narrative Statement Concerning the Amounts Reported in the Consolidated Reports of Condition and Income”) has just one text column (TEXT6980), the fix employed by the ffiec.pq package will work without issues.22\n\n\n3.1.2 Handling missing-value sentinels\nUsers familiar with both readr and Call Report data might also have noticed the use of na = c(\"\", \"CONF\") in the call to read_tsv() above. The default value for this function is na = c(\"\", \"NA\") means that empty values and the characters NA are treated as missing values. As I saw no evidence that the export process for FFIEC Bulk Data files used \"NA\" to mark NA values, I elected not to treat \"NA\" as NA. However, a wrinkle is that the reporting firms some times populate text fields—but not numerical fields—with the value \"NA\".23 While the most sensible interpretation of such values is as NA, without further investigation it is difficult to be sure that \"NA\" is the canonical form in which firms reported NA values rather than \"N/A\" or \"Not applicable\" or some other variant.\nThis approach seems validated by the fact that I see the value \"NR\" in text fields of PDF versions of Call Reports and these values show up as empty values in the TSV files, suggesting that \"NR\", not \"NA\" is the FFIEC’s canonical way of representing NA values in these files, while \"NA\" is literally the text value \"NA\", albeit perhaps one intended by the reporting firm to convey the idea of NA. Users of the FFIEC data created by the ffiec.pq package who wish to use textual data should be alert to the possibility that values in those fields may be intended by the reporting firm to convey the idea of NA, even if they are not treated as such by the FFIEC’s process for creating the TSV files.\nThe other value in the na argument used above is \"CONF\", which denotes that the the reported value is confidential and therefore not publicly disclosed. Ideally, we might distinguish between NA, meaning “not reported by the firm to the FFIEC” or “not applicable to this firm” or things like that, from \"CONF\", meaning the FFIEC has the value, but we do not. Unfortunately, the value \"CONF\" often appears in numeric fields and there is no simple way to ask read_tsv() to record the idea that “this value is confidential”, so I just read these in as NA.24\nI say “no simple way” because there are probably workarounds that allow \"CONF\" to be distinguished from true NAs. For example, I could have chosen to have read_tsv() read all numeric fields as character fields and then convert the value CONF in such fields to a sentinel value such as Inf (R’s way of saying “infinity” or \\(\\infty\\)).25 This would not be terribly difficult, but would have the unfortunate effect of surprising users of the data who (understandably) didn’t read the manual and starting finding that the mean values of some fields are Inf. Perhaps the best way to address this would allow the user of ffiec.pq to choose that behaviour as an option, but I did not implement this feature at this time.\nIn addition to these missing values, I discovered in working with the data that the FFIEC often used specific values as sentinel values for NA. For example, \"0\" is used for some fields, while \"00000000\" is used to mark dates as missing, and \"12/31/9999 12:00:00 AM\" is used for timestamps.26 I recoded such sentinel values as NA in each case."
  },
  {
    "objectID": "published/curate_call_reports.html#storage-format",
    "href": "published/curate_call_reports.html#storage-format",
    "title": "Data curation: The case of Call Reports",
    "section": "4.1 Storage format",
    "text": "4.1 Storage format\nIn principle, the storage format should fairly minor detail determined by the needs of the Understand team. For example, if the Understand team works in Stata or Excel, then perhaps they will want the data in some kind of Stata format or as Excel files. However, I think it can be appropriate to push back on notions that data will be delivered in form that involves downgrading the data or otherwise compromises the process in a way that may ultimately add to the cost and complexity of the task for the Curate team. For example, “please send the final data as an Excel file attachment as a reply email” might be a request to be resisted because the process of converting to Excel can entail the degradation of data (e.g., time stamps or encoding of text).27 Instead it may be better to choose a more robust storage format and supply a script for turning that into a preferred format.\nOne storage format that I have used in the past would deliver data as tables in a (PostgreSQL) database. The Understand team could be given access data from a particular source organized as a schema in a database. Accessing the data in this form is easy for any modern software package. One virtue of this approach is that the data might be curated using, say, Python even though the client will analyse it using, say, Stata.28 I chose to use Parquet files for ffiec.pq, in part because I don’t have a PostgreSQL server to put the data into and share with you. But Parquet files offer high performance, are space-efficient, and can be used with any modern data analysis tool."
  },
  {
    "objectID": "published/curate_call_reports.html#good-database-principles",
    "href": "published/curate_call_reports.html#good-database-principles",
    "title": "Data curation: The case of Call Reports",
    "section": "4.2 Good database principles",
    "text": "4.2 Good database principles\nWhile I argued that one does not want to get “particularly fussy about database normalization”, if anything I may have pushed this further than some users might like. However, with ffiec_pivot(), it is relatively easy (and not too costly) to get the data into a “wide” form if that is preferred. The legacy version of Call Reports data offered by WRDS went to the other extreme with a “One Big Table” approach, which meant that this data set never moved to PostgreSQL because of limits there.29"
  },
  {
    "objectID": "published/curate_call_reports.html#primary-keys",
    "href": "published/curate_call_reports.html#primary-keys",
    "title": "Data curation: The case of Call Reports",
    "section": "4.3 Primary keys",
    "text": "4.3 Primary keys\nIn Gow (2026), I suggested that “the Curate team should communicate the primary key of each table to the Understand team. A primary key of a table will be a set of variables that can be used to uniquely identify a row in that table. In general a primary key will have no missing values. Part of data curation will be confirming that a proposed primary key is in fact a valid primary key.”\n\n\n\n\nTable 2: Primary key checks\n\n\n\n\n\n\nSchedule\nPrimary key\nCheck\n\n\n\n\npor\nIDRSSD, date\nTRUE\n\n\nffiec_float\nIDRSSD, date, item\nTRUE\n\n\nffiec_int\nIDRSSD, date, item\nTRUE\n\n\nffiec_str\nIDRSSD, date, item\nTRUE\n\n\nffiec_bool\nIDRSSD, date, item\nTRUE\n\n\nffiec_date\nIDRSSD, date, item\nTRUE\n\n\nffiec_schedules\nitem, date\nTRUE\n\n\n\n\n\n\n\n\nValid primary keys for each schedule are shown in Table 2. To checking these, I used the function ffiec_check_pq_keys(), which checks the validity of a proposed primary key for a schedule. That every column except value forms part of the primary key is what allows us to use ffiec_pivot() to create unique values in the resulting “wide” tables."
  },
  {
    "objectID": "published/curate_call_reports.html#data-types",
    "href": "published/curate_call_reports.html#data-types",
    "title": "Data curation: The case of Call Reports",
    "section": "4.4 Data types",
    "text": "4.4 Data types\nIn Gow (2026), I proposed that “each variable of each table should be of the correct type. For example, dates should be of type DATE, variables that only take integer values should be of INTEGER type. Date-times should generally be given with TIMESTAMP WITH TIME ZONE type. Logical columns should be supplied with type BOOLEAN.”30\nThis element is (to the best of my knowledge) satisfied with one exception. The Parquet format is a bit like the Model T Ford: it supports time zones, and you can use any time zone you want, so long as it is UTC.31 As discussed above, there is only one timestamp in the whole set-up, last_date_time_submission_updated_on on the POR files and I discussed this field above."
  },
  {
    "objectID": "published/curate_call_reports.html#no-manual-steps",
    "href": "published/curate_call_reports.html#no-manual-steps",
    "title": "Data curation: The case of Call Reports",
    "section": "4.5 No manual steps",
    "text": "4.5 No manual steps\nWhen data vendors are providing well-curated data sets, much about the curation process will be obscure to the user. This makes some sense, as the data curation process has elements of trade secrets. But often data will be supplied by vendors in an imperfect state and significant data curation will be performed by the Curate team working for or within the same organization as the Understand team.\nFocusing on the case where the data curation process transforms an existing data set—say, one purchased from an outside vendor—into a curated data set in sense used here, there are a few ground rules regarding manual steps.\n“First, the original data files should not be modified in any way.” Correct. The ffiec.pq package does not modify the FFIEC Bulk Data files after downloading them. I do make some corrections to the item_name variable in the ffiec_items package, but these “manual steps [are] extensively documented and applied in a transparent, automated fashion.” The code for these steps can be found on the GitHub page for the ffiec.pq package."
  },
  {
    "objectID": "published/curate_call_reports.html#documentation",
    "href": "published/curate_call_reports.html#documentation",
    "title": "Data curation: The case of Call Reports",
    "section": "4.6 Documentation",
    "text": "4.6 Documentation\n“The process of curating the data should be documented sufficiently well that someone else could perform the curation steps should the need arise.” I regard that having the ffiec.pq package do all the work of processing the data satisfies this requirement.\nA important idea here is that the code for processing the data is documentation in its own right. Beyond that the document you are reading now is a form of documentation, as is the documentation in the ffiec.pq package."
  },
  {
    "objectID": "published/curate_call_reports.html#update-process",
    "href": "published/curate_call_reports.html#update-process",
    "title": "Data curation: The case of Call Reports",
    "section": "4.7 Update process",
    "text": "4.7 Update process\nIf a new zip file appears on the FFIEC Bulk Data website, you can download it using the process outlined in Section 1.1. Just changing the [:4] to [0] and the script downloads the latest file.\nThen run the following code and the data will be updated:\n\nresults &lt;-\n  ffiec_list_zips() |&gt;\n  filter(date == max(date)) |&gt;\n  select(zipfile) |&gt;\n  pull() |&gt;\n  ffiec_process() |&gt;\n  system_time()\n\n   user  system elapsed \n 12.411   1.567  11.363 \n\nresults |&gt; count(date, ok)\n\n# A tibble: 1 × 3\n  date       ok        n\n  &lt;date&gt;     &lt;lgl&gt; &lt;int&gt;\n1 2025-12-31 TRUE     39"
  },
  {
    "objectID": "published/curate_call_reports.html#data-version-control",
    "href": "published/curate_call_reports.html#data-version-control",
    "title": "Data curation: The case of Call Reports",
    "section": "4.8 Data version control",
    "text": "4.8 Data version control\nWelch (2019) argues that, to ensure that results can be reproduced, “the author should keep a private copy of the full data set with which the results were obtained.” This imposes a significant cost on the Understand team to maintain archives of data sets that may run to several gigabytes or more and it would seem much more efficient for these obligations to reside with the parties with the relevant expertise. Data version control is a knotty problem and one that even some large data providers don’t appear to have solutions for.\nI am delivering the Call Report data not as the data files, but as an R package along with instructions for obtaining the zip files from the FFIEC Bulk Data website. So I cannot be said to be providing much version control of data here. That said, if a user retains the downloaded zip files, the application of the ffiec.pq functions to process these into Parquet files should provide a high degree of reproducibility of the data for an individual researcher.32\nFor my own purposes, I achieve a modest level of data version control by using Dropbox, which offers the ability to restore some previous versions of data files."
  },
  {
    "objectID": "published/curate_call_reports.html#footnotes",
    "href": "published/curate_call_reports.html#footnotes",
    "title": "Data curation: The case of Call Reports",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAs will be seen this, this load step will not generally be an elaborate one. The inclusion of a separate load step serves more to better delineate the distinction between the Curate process and the Understand process.↩︎\nExecute install.packages(c(\"tidyverse\", \"farr\", \"DBI\", \"duckdb\", \"pak\", dbplyr\") within R to install all the packages other than ffiec.pq that you will need to run the R code in this note.↩︎\nI already included pak in the command in the footnote to the previous paragraph.↩︎\nAt the time of writing, there are 99 files, but each quarter will bring a new file to be processed.↩︎\nI discovered this package after writing most of this note. In my case, I pointed-and-clicked to get many of the files and Martien Lubberink of Victoria University of Wellington kindly provided the rest.↩︎\nI recommend that readers follow a similar approach if following along with this note, as it makes subsequent steps easier to implement. A reader can simply specify os.environ['RAW_DATA_DIR'] = \"/Users/igow/Dropbox/raw_data\", substituting a location where the data should go on his or her computer.↩︎\nNote I am using the argument use_multicore = TRUE, which gives me about a five-time improvement in performance, but you will see different results depending on your system. You might test the code on a few files before running the whole thing. The ffiec_process() can accept a list of fully qualified paths to zipfiles, which can be created with the help of ffiec_list_zips().↩︎\nNote that nothing is being “loaded” into RAM, the file is merely being scanned by DuckDB.↩︎\nParquet files are compressed, so they use less space on disk than they do when they are loaded into RAM.↩︎\nNote that collect() here actually brings the data into R; compute() merely materializes the data as a table in the DuckDB database.↩︎\nSee Wickham et al. (2023), p. 311-315, for discussion of time spans.↩︎\nI am writing this from the Boston area, so it would be insufficiently confusing if I did not make this change to my settings. In effect, I want to ensure that the code works for someone in a different time zone.↩︎\nI say somewhat arbitrarily because you probably don’t want to choose a day that is missing an hour due to shift from EST to EDT.↩︎\nTabs and newlines are what are sometimes called invisibles because their presence is not apparent from viewing their usual representation as text (for example, a tab might look the same as a series of spaces). The \\t and \\n representations are quite standard ways of making these characters visible to humans.↩︎\nIn practice, there’s a little clean-up to be done before returning df, as I will explain shortly.↩︎\nUnfortunately, the read_tsv() function does not allow us to specify an alternative to the default for line-terminating characters. It seems that other R and Python packages also do not offer this option.↩︎\nThere are extra backslashes in the R version to specify that the backslashes represent backslashes to be passed along to gsub(), not special characters to be interpreted by R itself.↩︎\nThis is not a completely insoluble problem in that we could inspect the XBRL data to determine the correct form of the data in this case. This is “above my pay grade” in this setting. (I’m not being paid to do this!)↩︎\nNote that it would be somewhat faster to use ffiec_text &lt;- load_parquet(db, \"ffiec_str_20040630\", \"ffiec\"), but this code doesn’t take too long to run.↩︎\nOnly the textual data will have embedded tabs and, because such data is arranged after numerical data, only text data will be affected by embedded tabs.↩︎\nSee here for the gory details. Interestingly, the WRDS Call Report data have the same issue with the earlier case and have incorrect data for one of the banks in the latter case. This seems to confirm that WRDS uses the TSV data itself in creating its Call Report data sets.↩︎\nRecall that the “fix” assumes that embedded tab belongs in last available text column.↩︎\nIf \"NA\" appeared in a numeric field, my code would report an error. As I detected no errors in importing the data, I know there are no such values.↩︎\nThis is the kind of situation where SAS’s approach to coding missing values would be helpful.↩︎\nIn a sense, this would be doing the opposite of what the Python package pandas did in treating np.NaN as the way of expressing what later became pd.NA; I’d be using Inf to distinguish different kinds of missing values.↩︎\nNote that this timestamp sentinel appears in the “MDRM” data from the Federal Reserve that I used to construct ffiec_items, not in the FFIEC data sets themselves.↩︎\nI discuss some of the issues with Excel as a storage format below.↩︎\nOne project I worked on involved Python code analysing text and putting results in a PostgreSQL database and a couple of lines of code were sufficient for a co-author in a different city to load these data into Stata.↩︎\nA rule of thumb might be that, if you cannot store your fairly standard data in PostgreSQL, then perhaps you need to revisit the structure of the data.↩︎\nGow (2026) is referring to PostgreSQL types. The ffiec.pq package uses (logical) Parquet types DATE, INT32, TIMESTAMP(isAdjustedToUTC = true), and BOOLEAN types, respectively for these types. Floating-point numbers are stored as FLOAT64 and strings as STRING.↩︎\nHenry Ford famously said of the Model T that “any customer can have a car painted any color that he wants so long as it is black.” Strictly speaking, the Parquet format supports timezone-aware timestamps, but only as UTC instants, as other time zones are not supported.↩︎\nNote that a researcher might need to use a specific version of the ffiec.pq package to achieve full reproducibility, but the pak package allows for that.↩︎"
  },
  {
    "objectID": "published/datetimes.html",
    "href": "published/datetimes.html",
    "title": "Working with date and times",
    "section": "",
    "text": "The purpose of this note is address the topic of temporal data (dates and times) in more detail than found in the “Dates and Times” chapter of R for Data Science (Wickham et al., 2023) and to provide a hands-on application of working with them. The application I work with is based on the SEC submissions data I discussed in another recent note. I study this application not only using R, but also DuckDB and PostgreSQL, as moving data between systems can be a pain point, especially with date-times.\nAs discussed in a recent post on LinkedIn, one goal of Empirical Research in Accounting: Tools and Methods (Gow and Ding, 2024) is to provide a pathway to mastery of the contents of Wickham et al. (2023).1 That said, I identified a few gaps, including spreadsheets, hierarchical data, and dates and times. One recent note covered hierarchical data and a forthcoming note will address spreadsheets.\nThe purpose of this note is address the gap regarding dates and times in more detail—including a hands-on application—than found in the “Dates and Times” chapter of Wickham et al. (2023). The application I work with is based on the SEC submissions data I discussed in another recent note. I study this application not only using R, but also DuckDB and PostgreSQL, as moving data between systems can be a pain point, especially with date-times.\nIn writing this note, I use the packages listed below.2 This note was written using Quarto and compiled with RStudio, an integrated development environment (IDE) for working with R. The source code for this note is available here and the latest version of this PDF is here.\nlibrary(tidyverse)\nlibrary(DBI)\nlibrary(farr)\nlibrary(scales)"
  },
  {
    "objectID": "published/datetimes.html#dates",
    "href": "published/datetimes.html#dates",
    "title": "Working with date and times",
    "section": "Dates",
    "text": "Dates\nIn some respects, dates are relatively straightforward. A constant bane of data analysts is the representation of dates in ambiguous formats. For example, in many parts of the world \"11/12/2023\" represents 11 December 2023; in other parts of the world (read “the United States of America”), \"11/12/2023\" represents November 12, 2023. Converting string representations of dates to dates is an important element of the data curation process.4 R for Data Science provides an excellent discussion of issues that arise in converting string data to dates (Wickham et al., 2023, pp. 296–302).\nWe might also encounter dates in data sets from other programs, such as SAS, Stata, or Excel. Functions such as read_sas(), read_stata(), and read_excel() will generally detect and convert dates. Otherwise exporting data from those programs to text with dates represented in an unambiguous text format is perhaps the best approach. For example, in my wrds2pg Python package, I apply SAS code that uses format=YYMMDD10. to represent dates using the ISO 8601 format discussed in R for Data Science before exporting to CSV using SAS’s PROC EXPORT command.5\nDates are generally transferred without any issues from databases such as DuckDB or PostgreSQL to R or vice versa. Once properly encoded as type Date, dates do not present particular difficulties in R in many contexts. The as.Date() function converts an ISO 8601 string to a date (i.e., an object with class class Date in R):\n\na_date &lt;- as.Date(\"2024-05-28\")\nclass(a_date)\n\n[1] \"Date\"\n\n\nFunctions provided by the lubridate package (part of the core Tidyverse) allow us to extract information about dates, such as the year …\n\nyear(a_date)\n\n[1] 2024\n\n\n… the month (as a number) …\n\nmonth(a_date)\n\n[1] 5\n\n\n… and the month (as a word in the relevant locale; I’m using English).\n\nmonth(a_date, label = TRUE)\n\n[1] May\n12 Levels: Jan &lt; Feb &lt; Mar &lt; Apr &lt; May &lt; Jun &lt; Jul &lt; Aug &lt; Sep &lt; ... &lt; Dec\n\n\nMuch of the complexity surrounding dates and times arises from the existence of time zones. It is important to note that one does not completely avoid this complexity with dates. For example, the date 2024-06-28 in the America/New_York time zone will include points of time that are associated with the date 2024-06-29 in the Australia/Sydney time zone. So if a date-time related to Australia is converted to a date based on UTC or America/New_York, then it may end up on a different date that would result if converted to a date using Australia/Melbourne.\nAnother aspect of dates that is easy to overlook is that some things that seem simple turn out to be complicated when examined more closely. For example, what date is one month after 2024-01-31? Also, what date is one year after 2024-02-29? Note that R will return NA as the answer to each of these questions.6"
  },
  {
    "objectID": "published/datetimes.html#date-times",
    "href": "published/datetimes.html#date-times",
    "title": "Working with date and times",
    "section": "Date-times",
    "text": "Date-times\nA date-time, also known as a timestamp, combines a date with a time and thus represents an instant in time. From this perspective, I would argue that a timestamp only has meaning if understood in the context of a time zone, as a date-time with a different time zone represents a different instant in time, as can be seen from the following examples.\n\nts &lt;- \"2008-06-30 16:52:26\"\nt1 &lt;- parse_date_time(ts, orders = \"ymdHMS\")\nt2 &lt;- parse_date_time(ts, tz = \"Australia/Melbourne\", orders = \"ymdHMS\")\nt3 &lt;- parse_date_time(ts, tz = \"America/New_York\", orders = \"ymdHMS\")\n\nIt turns out that t1 is a date-time representing 2008-12-31 16:52:26 UTC, which is a different point in time from both t2, which is the “same” time in the Australia/Melbourne time zone, and t3, which is the “same” time in the America/New_York time zone.\n\nt1 - t2\n\nTime difference of 10 hours\n\nt1 - t3\n\nTime difference of -4 hours\n\n\nTo determine the correct string to use for a given time zone, inspect the output of OlsonNames(). Many denizens of US time zone will indicate them using abbreviations such as EST for New York even at times of the year when that time zone does not apply (e.g., in June, when EDT is the applicable time zone). In this regard, using America/New_York obviates the risk of a mismatch such as that implied by 2008-06-30 16:52:26 EST."
  },
  {
    "objectID": "published/datetimes.html#duckdb-and-time-zones",
    "href": "published/datetimes.html#duckdb-and-time-zones",
    "title": "Working with date and times",
    "section": "DuckDB and time zones",
    "text": "DuckDB and time zones\nWe create a DuckDB instance by connecting to it as follows.\n\ndb &lt;- dbConnect(duckdb::duckdb(), timezone_out =\"America/New_York\")\n\nWe specify timezone_out =\"America/New_York\" so that data returned to R are in the time zone that makes most sense for plotting and such like. Note that this does not change the actual moment in time reflected in the data, just how it is displayed.\nBy default, DuckDB does not load information about time zones. These are found in the package icu, which can be installed and loaded using the commands below.7\n\ndbExecute(db, \"INSTALL icu\")\ndbExecute(db, \"LOAD icu\")\n\nWe will use functions like hour(), minute(), and second() to prepare data for plotting. To understand these functions, we create a table containing just one value.\n\na_datetime &lt;- \"2008-12-31 16:52:26 America/New_York\"\nsample &lt;- tbl(db, sql(str_c(\"SELECT '\", a_datetime, \"'::TIMESTAMPTZ AS a_datetime\")))\n\nIntuitively, we would expect hour(a_datetime) to return 16, minute(a_datetime) to return 52, and second() to return 26. However, these functions are interpreted with respect to the time zone setting of the database. The default setting for DuckDB is the local time, but we can set this to UTC using the following command.\n\ndbExecute(db, \"SET TIME ZONE 'UTC'\")\n\nThus we get the value 21 because UTC was five hours ahead of New York time on 31 December 2008.\n\nsample |&gt; mutate(hour = hour(a_datetime)) |&gt; collect()\n\n# A tibble: 1 × 2\n  a_datetime           hour\n  &lt;dttm&gt;              &lt;dbl&gt;\n1 2008-12-31 16:52:26    21\n\n\nSo, we set the DuckDB time zone to New York time …\n\ndbExecute(db, \"SET TIME ZONE 'America/New_York'\")\n\n… and try again.\n\nsample |&gt; mutate(hour = hour(a_datetime)) |&gt; collect()\n\n# A tibble: 1 × 2\n  a_datetime           hour\n  &lt;dttm&gt;              &lt;dbl&gt;\n1 2008-12-31 16:52:26    16"
  },
  {
    "objectID": "published/datetimes.html#working-with-sec-filings-data",
    "href": "published/datetimes.html#working-with-sec-filings-data",
    "title": "Working with date and times",
    "section": "Working with SEC filings data",
    "text": "Working with SEC filings data\nWe start by loading the filings data stored in a parquet file filings.parquet in the data subdirectory of our current project.8\n\nfilings &lt;- load_parquet(db, table = \"filings\", data_dir = \"../data\")\n\nAn important thing to note about the creation of this file is that the original data were coded such that the time zone of the data was UTC. For example, the original text representation of acceptanceDateTime was of the form 2024-04-25T17:04:01.000Z. The Z is meant to indicate that the timestamp is expressed in UTC.9 However, I ignored that in importing the data and interpreted these timestamps as relating to America/New_York. As we will see shortly, this interpretation is correct based on other information.10\nTo understand the date-times on filings, I extract some components of the timestamp to create a variable acceptance_time that converts the date component of each acceptanceDateTime to the same date (2000-01-01) as this facilitates plotting the times ignoring the dates.11\n\nfiling_times &lt;- \n  filings |&gt;\n  mutate(year = year(acceptanceDateTime),\n         hour = hour(acceptanceDateTime),\n         minute = minute(acceptanceDateTime),\n         second = second(acceptanceDateTime),\n         acceptance_time = make_timestamptz(2000L, 01L, 01L, \n                                            hour, minute, second)) |&gt;\n  select(cik, year, accessionNumber, acceptanceDateTime, acceptance_time) |&gt;\n  compute()\n\nFigure 1 plots the acceptance times for all filings in our sample. One observation is that we have over a million filings right around midnight.\n\nfiling_times |&gt;\n  ggplot(aes(x = acceptance_time)) + \n  geom_histogram(binwidth = 5 * 60) +\n  scale_x_datetime(date_breaks = \"1 hour\",\n                   date_labels = \"%H:%M\") +\n  theme(axis.text.x = element_text(angle = 90))\n\n\n\n\n\n\n\nFigure 1: Distribution of acceptance time for all filings\n\n\n\n\n\nFigure 2 digs into these “midnight” filings based on the year of filing. It appears that the majority of these are from 2002 or earlier. A plausible explanation is that most filings during that earlier period only have filing dates and acceptanceDateTime is expressed as midnight on those filing dates.\nIn Figure 2, we also see evidence of a small number of filings for years 1980 and 1993. The former value probably relates to errors, but a small number of filings did occur in 1993.\n\nfiling_times |&gt;\n  mutate(midnight = acceptance_time == \"2000-01-01\") |&gt;\n  mutate(year = as.character(year)) |&gt;\n  ggplot(aes(x = year, fill = midnight)) + \n  geom_bar() +\n  theme(axis.text.x = element_text(angle = 90))\n\n\n\n\n\n\n\nFigure 2: Year of ‘midnight’ filings\n\n\n\n\n\nFigure 3 focuses on cases before 2002 with non-midnight acceptance times. While there are some, these appear to be very few in number, perhaps reflecting a pilot program of some sort.12 Note that—with one stray exception before 07:00—filings appear to occur between 08:00 and 22:00 during this period.\n\nfiling_times |&gt; \n  filter(year &lt; 2002,\n         acceptance_time != \"2000-01-01\") |&gt;\n  mutate(year = as.character(year)) |&gt;\n  ggplot(aes(x = acceptance_time, fill = year)) + \n  geom_histogram(binwidth = 5 * 60) +\n  scale_x_datetime(date_breaks = \"1 hour\",\n                   date_labels = \"%H:%M\") +\n  theme(axis.text.x = element_text(angle = 90))\n\n\n\n\n\n\n\nFigure 3: Distribution of acceptance time–pre-2002 with times\n\n\n\n\n\nFigure 4 focuses on non-midnight filings in 2002 or later, we see that the bulk of filings are between 06:00 and 22:00, consistent with the statement on the SEC website that “EDGAR is available to accept filings from 6 a.m. to 10 p.m. ET weekdays (except federal holidays).” In fact, it was this information that allowed me to conclude that the timestamps in the underlying JSON files are in New York times, as interpreting them as UTC times put many filings outside this window.\nHowever, some pre-08:00 filings appear in Figure 4.\n\nfiling_times |&gt;\n  filter(year &gt;= 2002,\n         acceptance_time &gt; \"2000-01-01\") |&gt;\n  ggplot(aes(x = acceptance_time)) + \n  geom_histogram(binwidth = 5 * 60) +\n  scale_x_datetime(date_breaks = \"1 hour\",\n                   date_labels = \"%H:%M\") +\n  theme(axis.text.x = element_text(angle = 90))\n\n\n\n\n\n\n\nFigure 4: Distribution of acceptance time–post-2002 with times\n\n\n\n\n\nFigure 5 examines the pre-08:00 filings that appear in Figure 4. It appears that these are oddly concentrated at around 00:15 and are perhaps data errors.\n\nfiling_times |&gt;\n  filter(between(acceptance_time, \"2000-01-01 00:00:01\", \"2000-01-01 01:00:00\")) |&gt;\n  ggplot(aes(x = acceptance_time)) + \n  geom_histogram(binwidth = 1 * 60) +\n  scale_x_datetime(date_breaks = \"5 min\",\n                   date_labels = \"%H:%M\") +\n  theme(axis.text.x = element_text(angle = 90))\n\n\n\n\n\n\n\nFigure 5: Distribution of acceptance time–early-morning outliers\n\n\n\n\n\nWhile not apparent in Figure 2, Figure 6 reveals that there are some post-2003 midnight filings. However, these are all concentrated in 2009 and are perhaps evidence of data issues.\n\nfiling_times |&gt;\n  mutate(year = year(acceptanceDateTime)) |&gt; \n  filter(year &gt; 2003,\n         acceptance_time == \"2000-01-01 00:00:00\") |&gt;\n  ggplot(aes(x = acceptanceDateTime)) + \n  geom_histogram(binwidth = 60 * 60 * 24 * 7) +\n  scale_x_datetime(date_breaks = \"1 month\", date_labels = \"%Y-%m-%d\") +\n  theme(axis.text.x = element_text(angle = 90))\n\n\n\n\n\n\n\nFigure 6: Distribution of dates–post-2003 midnight filers\n\n\n\n\n\nGiven the issues revealed above, Figure 7—our final plot of acceptance times—focuses on filings after 2003 and with filing times after 05:00 and before 23:00.\n\nfiling_times |&gt;\n  filter(year &gt; 2003,\n         between(acceptance_time,\n                 \"2000-01-01 05:00:00\",\n                 \"2000-01-01 23:00:00\")) |&gt;\n  ggplot(aes(x = acceptance_time)) + \n  geom_histogram(binwidth = 5 * 60) +\n  scale_x_datetime(date_breaks = \"1 hour\",\n                   date_labels = \"%H:%M\") +\n  theme(axis.text.x = element_text(angle = 90))\n\n\n\n\n\n\n\nFigure 7: Distribution of acceptance times with valid times\n\n\n\n\n\nThere are some interesting patterns to be observed in Figure 7.\n\nFor filings before 10:00, there are spikes in the number of filings on the hour. This may reflect filings that are pre-programmed at specific times in pre-trading hours.13\nThese spikes are not apparent during trading hours.\nThere is a big spike in the number of filings just after 16:00, consistent with many filings being delayed until the close of trading.\nPutting aside the pre-trading spikes, there is a steady rise int he number of filings until about 12:00, then a flattening out till about 13:30, when filings steadily rise until 16:00.\nThere is a sharp drop in the number of filings at about 17:30.\nAfter 17:30, the number of filings steadily decreases until 22:00.\nThere are tiny spike on the hour after 17:00."
  },
  {
    "objectID": "published/datetimes.html#appendix-postgresql",
    "href": "published/datetimes.html#appendix-postgresql",
    "title": "Working with date and times",
    "section": "Appendix: PostgreSQL",
    "text": "Appendix: PostgreSQL\nAt the outset I alluded to using PostgreSQL and DuckDB. In this appendix, I show how one can achieve the same result as shown in Figure 8 using PostgreSQL.\n\nGetting data into PostgreSQL\nIf we were starting with data already in PostgreSQL, we could skip this step. However, a big attraction of DuckDB is its ability to glue together data from many different sources and we can use DuckDB to populate PostgreSQL.\nWe first need to install and load the postgres extension for DuckDB, which is done with the following lines of code.\n\ndbExecute(db, \"INSTALL postgres\")\ndbExecute(db, \"LOAD postgres\")\n\nI have a PostgreSQL database running on my computer, so I can connect to it with an empty connection string. See the documentation for the postgres extension if you need to supply additional information to connect to a PostgreSQL database.18\n\ndbExecute(db, \"ATTACH '' AS pg (TYPE POSTGRES)\")\n\nThe first step is to copy the data in filings.parquet to PostgreSQL. We need to go from the parquet file as we did not materialize a table with these data in DuckDB. This step takes some time, but would be a one-off step. I include IF NOT EXISTS to avoid overwriting any existing table you might have.\n\nsql &lt;- \"CREATE TABLE IF NOT EXISTS pg.filings AS\n        SELECT * FROM '../data/filings.parquet'\"\ndbExecute(db, sql) |&gt; system_time()\n\n   user  system elapsed \n 36.372   1.023  33.115 \n\n\n[1] 12120262\n\n\nWe did materialize named (temporary) tables for tickers and sample_calls, so we can copy directly from those to PostgreSQL. These small tables are populated very quickly.\n\ndbExecute(db, \"CREATE TABLE IF NOT EXISTS pg.tickers AS\n               SELECT * FROM tickers\")\ndbExecute(db, \"CREATE TABLE IF NOT EXISTS pg.sample_calls AS\n               SELECT * FROM sample_calls\")\n\n\n\nRe-running the analysis using PostgreSQL\nThe first step is to connect to PostgreSQL, again using the timezone argument for the same reasons we discussed above with DuckDB.\n\npg &lt;- dbConnect(RPostgres::Postgres(), timezone = \"America/New_York\")\n\nWe next create remote data tables for each of the three underlying data tables we used above.\n\nfilings &lt;- tbl(pg, \"filings\")\ntickers &lt;- tbl(pg, \"tickers\")\nsample_calls &lt;- tbl(pg, \"sample_calls\")\n\nNote that DuckDB creates tables with lower-case column names, so we need to adapt the code above to reflect this. This is not an inherent limitation of PostgreSQL, which can support case-sensitive column names. Apart from the lower-case column names, the following query converts hour and minute to integers, as the make_timestamptz() function in PostgreSQL expects integers in those slots. Note that I do not compute the table filing_times, as doing so is unnecessary and actually slows the code down.\n\nfiling_times &lt;- \n  filings |&gt;\n  mutate(year = year(acceptanceDateTime),\n         hour = as.integer(hour(acceptanceDateTime)),\n         minute = as.integer(minute(acceptanceDateTime)),\n         second = second(acceptanceDateTime),\n         acceptance_time = make_timestamptz(2000L, 1L, 1L, \n                                            hour, minute, second)) |&gt;\n  select(cik, year, accessionNumber, acceptanceDateTime, acceptance_time)\n\nThe code for creating earnings_filings is unchanged from above apart from using lower-case column names, as PostgreSQL offers the same regexp_split_to_table() function that we used in DuckDB.\n\nearnings_filings &lt;-\n  filings |&gt;\n  inner_join(tickers, by = \"cik\") |&gt;\n  mutate(item = regexp_split_to_table(items, \",\")) |&gt;\n  filter(form == \"8-K\", item == \"2.02\") |&gt;\n  select(cik, ticker, filingDate, acceptanceDateTime, item)\n\nThe code creating sample_merged differs slightly (apart from using lower-case column names) because start_date - acceptancedatetime results in a column of type interval in PostgreSQL that R is unable to interpret easily. To address this, I use the date_part('epoch', x) function to convert it to the number of seconds between the two timestamps.\n\nsample_merged &lt;-\n  earnings_filings |&gt;\n  inner_join(sample_calls, join_by(ticker, filingDate)) |&gt;\n  mutate(time_diff = date_part('epoch', start_date - acceptanceDateTime)) |&gt;\n  collect() |&gt;\n  system_time()\n\n   user  system elapsed \n  0.020   0.000  23.169 \n\n\nFinally, the code to produce Figure 9 is unchanged from the equivalent code above. Happily, Figure 9 looks identical to Figure 8.\n\nsample_merged |&gt;\n  mutate(time_diff = time_diff / 60) |&gt;\n  ggplot(aes(x = time_diff, fill = ticker)) +\n  geom_histogram(binwidth = 5)\n\n\n\n\n\n\n\nFigure 9: Distribution of times between 8-K filings and conference calls: PostgreSQL version\n\n\n\n\n\nMy experience is that DuckDB generally offers better performance than PostgreSQL, but in this case PostgreSQL seems quite performant relative to DuckDB once the data have been loaded into PostgreSQL. PostgreSQL does offer some benefits, including the ease of sharing data with others and avoidance of messy details of data files. For example, I could easily run the code to create data on my server in Massachusetts and access the data from my current location in Melbourne, Australia. In any case, the code above shows that the task can be accomplished in either DuckDB or PostgreSQL, so we are not forced to choose in this case."
  },
  {
    "objectID": "published/datetimes.html#footnotes",
    "href": "published/datetimes.html#footnotes",
    "title": "Working with date and times",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nGow and Ding (2024) was published in print form by CRC Press in December 2024 and remains free online.↩︎\nExecute install.packages(c(\"tidyverse\", \"DBI\", duckdb\", \"scales\", \"farr\")) within R to install all the packages you need to run the code in this note.↩︎\nIt is pointed out in that R does not have a native class for storing times (Wickham et al., 2023, p. 296).↩︎\nFor more on the data curation process as part of the data science workflow, see my note here.↩︎\nAfter rummaging through the depths of the SAS documentation, I landed on format=E8601DT19. as the way to represent date-times being exported by SAS.↩︎\nTo see this, enter as.Date(\"2024-01-31\") + months(1) and as.Date(\"2024-02-29\") + years(1), respectively.↩︎\nThe INSTALL icu command is only needed if you have not already installed it, but it’s harmless to run it if it’s already present.↩︎\nAdjust data_dir to match the location of the downloaded file on your compute. The process for creating this file is described here. You can obtain a copy of this file here. If you have set up a parquet data repository with this data in the submissions schema, then use filings &lt;- load_parquet(db, table = \"filings\", schema = \"submissions\") instead.↩︎\nSee the Wikipedia page for ISO 8601.↩︎\nNote that the information is stored in parquet files in UTC.↩︎\nAs mentioned above, R has no native type for times, but a set of date-times with the date fixed on a common date will provide us with what we need.↩︎\nThough we might expect a preponderance of these filings to occur in the years just before 2002 in this case, and the filings appear to be fairly spread out over the pre-2002 years.↩︎\nNYSE and NASDAQ trading hours are 09:30–16:00.↩︎\nSee here for information about each of the items.↩︎\nSee the note discussed earlier for details on how this file was created from data on SEC EDGAR.↩︎\nNote that we don’t strictly “create” this table in the sense of SQL’s CREATE TABLE using compute(), as we will only use a small portion of it and it would be expensive—about 10 seconds and 1.5GB of RAM—to compute() this whole table.↩︎\nBy default, the result is a difftime, which ggplot2 is unsure about.↩︎\nNote that you will need to have write access to this database.↩︎"
  },
  {
    "objectID": "published/dsf_polars.html",
    "href": "published/dsf_polars.html",
    "title": "The best of both worlds: Using modern data frame libraries to create pandas data",
    "section": "",
    "text": "A strong point of pandas is its expressiveness. Its API allows users to explore data using succinct and (generally) intuitive code. However, some of this expressiveness relies on data being in forms (for example, with dates ready to serve as an index) that often differ from the data we have, and pandas can struggle to manipulate the data into those forms, especially with larger data sets.\nA number of modern data frame libraries have emerged that address weaknesses of pandas. In this note, I use polars and Ibis to show how one can use these libraries to get the data into a form in which pandas can shine.\nWhile the underlying data would occupy over 10 GB in memory, the polars variant below runs in about half a second. This approach may have particular appeal to pandas experts because the code is likely more familiar to experienced analysts of data frames.\nI consider two Ibis alternatives. The first uses the same underlying parquet files used in the polars variant, but perhaps takes even less time than polars does. The second tweaks just a small portion of the earlier Ibis code to source the underlying data directly from the WRDS PostgreSQL database. Even so, it takes less than a second to run.\nSo we get the best of both worlds. We can use modern, lazy libraries to perform the heavy data manipulation, and then hand off a compact result to pandas for exploration and visualization.\n\n\n\n\n\n\nTip\n\n\n\nThis note uses several Python packages and parts of it rely on the existence of a local data repository of parquet files for WRDS data of the kind described in Appendix E of Empirical Research in Accounting: Tools and Methods. The following command (run in the terminal on your computer) installs the packages you need.\n\npip install 'ibis-framework[duckdb, postgres]' pandas polars db2pq\n\nThe code assumes you have set the environment variables DATA_DIR and WRDS_ID to point to the location of the parquet repository on your computer and your WRDS ID, respectively.\nThe necessary files for the parquet repository for this note can be created using the db2pq package using the following code. Note that wrds_update_pq(\"dsf\", \"crsp\") will take about 12 minutes with a fast connection to WRDS, but only runs if the file on your computer is not current with the data on the WRDS server.\n\nfrom db2pq import wrds_update_pq\n\nwrds_update_pq(\"dsf\", \"crsp\")  \nwrds_update_pq(\"stocknames\", \"crsp\");\n\ncrsp.dsf already up to date.\ncrsp.stocknames already up to date.\n\n\nThis note was written using Quarto. The source code for this note is available here and the latest version of this PDF is here."
  },
  {
    "objectID": "published/dsf_polars.html#sec-ibis",
    "href": "published/dsf_polars.html#sec-ibis",
    "title": "The best of both worlds: Using modern data frame libraries to create pandas data",
    "section": "3.1 Generating Figure 1 using Ibis and DuckDB",
    "text": "3.1 Generating Figure 1 using Ibis and DuckDB\nAn alternative to using polars would be to use Ibis and its default backend, DuckDB.\nI import Ibis and the _ placeholder, as the latter facilitates more succinct code. I also turn on interactive mode to make it easier to inspect the data in tables if I need to do so.\n\nimport ibis\nfrom ibis import _\nibis.options.interactive = True\n\nI next make a load_parquet_ibis() function that I can use to load data from my parquet repository.\n\ndef load_parquet_ibis(con, table, schema, *, data_dir=None):\n    if data_dir is None:\n        data_dir = Path(os.environ[\"DATA_DIR\"]).expanduser()\n\n    path = data_dir / schema / f\"{table}.parquet\"\n    # register the parquet file as an Ibis table\n    return con.read_parquet(str(path), table_name=f\"{schema}_{table}\")\n\nThe short time taken to “load” the data below suggests that, analogous to the results of pl.scan_parquet(), the tables created here are lazy Ibis expressions rather than materialized data frames.\n\n%%ptime\ncon = ibis.duckdb.connect()\ndsf = load_parquet_ibis(con, \"dsf\", \"crsp\")\nstocknames = load_parquet_ibis(con, \"stocknames\", \"crsp\")\n\nWall time: 87.29 ms\n\n\nA lot of the remaining code is a largely a straightforward translation of the polars code into Ibis equivalents. We start by creating tickers.\n\n%%ptime\nend_date = ibis.literal(data.index.max())\n\ntickers = (\n    stocknames\n    .filter(_.ticker.isin(clean_tickers))\n    .filter((_.namedt &lt;= end_date) & (end_date &lt;= _.nameenddt))\n    .select(\"permno\", \"ticker\")\n)\n\nWall time: 1.60 ms\n\n\nThen the Ibis version of dsf_sub from above.\n\n%%ptime\nstart_date = ibis.literal(data.index.min())\n\nnum_cols = [\"prc\", \"ret\", \"retx\"]\n\ndsf_sub = (\n    dsf\n    .inner_join(tickers, predicates=[dsf.permno == tickers.permno])\n    .filter(_.date.between(start_date, end_date))\n    .select(\"ticker\", \"date\", *num_cols)\n    .mutate(**{c: getattr(_, c).cast(\"float64\") for c in num_cols})\n)\n\nWall time: 1.89 ms\n\n\nNote that I am using some Python tricks here. First, *num_cols unpacks the list num_cols into positional arguments. So, .select(\"ticker\", \"date\", *num_cols) is equivalent to .select(\"ticker\", \"date\", \"prc\", \"ret\", \"retx\").\nSecond, ** unpacks a dictionary into keyword arguments. So .mutate(**{c: getattr(_, c).cast(\"float64\") for c in num_cols}) unpacks into the following:\n\n{\n    \"prc\": _.prc.cast(\"float64\"),\n    \"ret\": _.ret.cast(\"float64\"),\n    \"retx\": _.retx.cast(\"float64\"),\n}\n\nAnd finally, the equivalent of dsf_adj. Here I am using window functions, which are well-supported by DuckDB. Originally, I had attempted to use Ibis with a polars backend, but the polars backend does not support window functions with Ibis. It is not clear to me whether this is an inherent limitation of polars, or is simply a gap in the implementation of the polars backend for Ibis that may be addressed in future versions.\nThe code here seems pretty transparent (once you understand the prc = prc_last * growth / growth_last logic discussed above).\n\n%%ptime\n# window for cumulative calculations (up to current row)\nw = ibis.window(group_by=_.ticker, order_by=_.date)\n\ndef cumprod1p(x):\n    # cumprod(1 + x) = exp(cumsum(log(1 + x)))\n    return (1 + x).ln().cumsum().over(w).exp().cast(\"float64\")\n\ndsf_adj = (\n    dsf_sub\n    .mutate(growth = cumprod1p(_.retx))\n    .mutate(\n        prc_last    = _.prc.last().over(w),\n        growth_last = _.growth.last().over(w),\n    )\n    .mutate(prc = _.prc_last * _.growth / _.growth_last)\n    .drop(\"growth\", \"prc_last\", \"growth_last\")\n)\n\nWall time: 1.85 ms\n\n\nAnd the final step is very similar to what we saw with polars. We simply pivot_wider(), sort by date (important for the pandas index) and then execute() to create a pandas date frame, which we can apply .set_index(\"date\") to just like above. Note that pivot_wider() is translated into a group-by aggregate query in SQL. If we don’t specify values_agg=\"max\", it seems that values_agg=\"first\" by default. This works with DuckDB, which has a first() aggregate, but will not work with PostgreSQL, which does not. Because there should be just one prc for each (date, ticker) combination, the use of max() should not affect the output.\n\n%%ptime\ndata_alt = (\n    dsf_adj\n    .select(\"date\", \"ticker\", \"prc\")  \n    .pivot_wider(\n        names_from=\"ticker\",\n        values_from=\"prc\",\n        values_agg=\"max\", \n    )\n    .order_by(\"date\")\n    .execute()\n    .set_index(\"date\")\n)\n\nWall time: 146.49 ms\n\n\nAs before, we .reindex() to make the plot look better.\n\n%%ptime\ndata_alt = data_alt.reindex(data.index)\n\nWall time: 0.46 ms\n\n\nAnd the resulting plot in Figure 3 suggests that again we have successfully recreated the pandas data frame seen in Hilpisch (2019).\n\ndata_alt.plot(figsize=(8, 8), subplots=True);\n\n\n\n\n\n\n\nFigure 3: Stock prices for five firms: Using CRSP, Ibis and DuckDB"
  },
  {
    "objectID": "published/dsf_polars.html#sec-wrds",
    "href": "published/dsf_polars.html#sec-wrds",
    "title": "The best of both worlds: Using modern data frame libraries to create pandas data",
    "section": "3.2 Generating Figure 1 using Ibis and WRDS PostgreSQL",
    "text": "3.2 Generating Figure 1 using Ibis and WRDS PostgreSQL\nNow that we have Ibis code, it is a trivial matter to replace the DuckDB backend with a PostgreSQL backend using the WRDS PostgreSQL database. I have my WRDS ID in the environment variable WRDS_ID. Only the first chunk of code below is changed from the Ibis-with-DuckDB version in Section 3.1. This first chunk takes a (relatively!) long time to run because Ibis needs to connect to the WRDS database and query it for metadata about the two tables it is using.\nLooking at Figure 4, it seems that this code also effectively reproduces the data frame created by the pandas code in Section 2.\n\n%%ptime\nwrds_id = os.environ['WRDS_ID']\n\ncon = ibis.postgres.connect(\n    host='wrds-pgdata.wharton.upenn.edu',\n    database='wrds',\n    user=wrds_id,\n    port=9737\n)\n\ndsf = con.table('dsf', database='crsp')\nstocknames = con.table('stocknames', database='crsp')\n\nWall time: 448.07 ms\n\n\nThe remaining code is identical to that above for Ibis.\n\n%%ptime\nend_date = ibis.literal(data.index.max())\n\ntickers = (\n    stocknames\n    .filter(_.ticker.isin(clean_tickers))\n    .filter((_.namedt &lt;= end_date) & (end_date &lt;= _.nameenddt))\n    .select(\"permno\", \"ticker\")\n)\n\nWall time: 2.04 ms\n\n\n\n%%ptime\nstart_date = ibis.literal(data.index.min())\n\nnum_cols = [\"prc\", \"ret\", \"retx\"]\n\ndsf_sub = (\n    dsf\n    .inner_join(tickers, predicates=[dsf.permno == tickers.permno])\n    .filter(_.date.between(start_date, end_date))\n    .select(\"ticker\", \"date\", *num_cols)\n    .mutate(**{c: getattr(_, c).cast(\"float64\") for c in num_cols})\n)\n\nWall time: 2.72 ms\n\n\n\n%%ptime\n# window for cumulative calculations (up to current row)\nw = ibis.window(group_by=_.ticker, order_by=_.date)\n\ndef cumprod1p(x):\n    # cumprod(1 + x) = exp(cumsum(log(1 + x)))\n    return (1 + x).ln().cumsum().over(w).exp().cast(\"float64\")\n\ndsf_adj = (\n    dsf_sub\n    .mutate(growth = cumprod1p(_.retx))\n    .mutate(\n        prc_last    = _.prc.last().over(w),\n        growth_last = _.growth.last().over(w),\n    )\n    .mutate(prc = _.prc_last * _.growth / _.growth_last)\n    .drop(\"growth\", \"prc_last\", \"growth_last\")\n)\n\nWall time: 4.32 ms\n\n\nThe next chunk takes a little longer than the same code chunk took in Section 3.1, because the data need to be transported from the WRDS PostgreSQL server to my computer.\n\n%%ptime\ndata_alt = (\n    dsf_adj\n    .select(\"date\", \"ticker\", \"prc\")  \n    .pivot_wider(\n        names_from=\"ticker\",\n        values_from=\"prc\",\n        values_agg=\"max\",   \n    )\n    .order_by(\"date\")\n    .execute()\n    .set_index(\"date\")\n)\n\nWall time: 267.78 ms\n\n\n\n%%ptime\ndata_alt = data_alt.reindex(data.index)\n\nWall time: 0.87 ms\n\n\n\ndata_alt.plot(figsize=(8, 8), subplots=True);\n\n\n\n\n\n\n\nFigure 4: Stock prices for five firms: Using CRSP, Ibis and PostgreSQL"
  },
  {
    "objectID": "published/dsf_polars.html#footnotes",
    "href": "published/dsf_polars.html#footnotes",
    "title": "The best of both worlds: Using modern data frame libraries to create pandas data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThough in Section 3.2, I effectively run SQL against the WRDS PostgreSQL database, but using the Ibis library to generate the SQL.↩︎\nThis was on an M4 Pro Mac mini with 24 GB of RAM.↩︎\nThe term “framework libraries” is one I made up. If you go to the Ibis website https://ibis-project.org, Ibis is described as “an open source dataframe library that works with any data system”. But Ibis relies on some other system to provide the execution engine, which it calls the backend. Most data frame libraries provide their own execution engines.↩︎\nNote, some of these data frame libraries are not limited to Python. For example, the polars documentation states that polars “is written in Rust, and available for Python, R and NodeJS [sic].” https://docs.pola.rs↩︎\nNote that I use a little custom “cell magic” %%ptime to produce execution times in a way that prints nicely in the PDF output. See the source code for this document for details.↩︎\nSee the source code for details.↩︎"
  },
  {
    "objectID": "published/g_secd.html",
    "href": "published/g_secd.html",
    "title": "Some benchmarks with comp.g_secd",
    "section": "",
    "text": "In this note, I use a simple query to benchmark performance of various approaches to accessing data in comp.g_secd. Many international researchers will know that comp.g_secd holds daily security-level data for non-US firms covered by Compustat Global. It is the global version of the North America daily security file, comp.secd, and, I guess, the closest analogue of crsp.dsf for finance and accounting researchers studying non-US firms.\nI use this data set to do some benchmarking. I find that a query using comp.g_secd that takes 6 minutes using SAS on the WRDS servers, takes about 1 minute using the WRDS PostgreSQL server and about 0.2 seconds using a local Parquet file. The Parquet file occupies less than 4 GB on my hard drive, which compares with about 145 GB for the SAS file on the WRDS server. While creating the Parquet file takes 45 minutes, this may be a reasonable trade-off for a researcher who is analysing comp.g_secd frequently and does not need the very latest iteration of comp.g_secd for research purposes."
  },
  {
    "objectID": "published/g_secd.html#the-benchmark-query",
    "href": "published/g_secd.html#the-benchmark-query",
    "title": "Some benchmarks with comp.g_secd",
    "section": "2.1 The benchmark query",
    "text": "2.1 The benchmark query\nNow that we understand a little about comp.g_secd and its SAS manifestation, I will introduce my benchmark query. The idea of the benchmark is that it is somewhat representative of the kinds of things a researcher might want to do with comp.g_secd without being overly complicated. Additionally, the benchmark should require actually looking at a significant part of the data (in this case all records) and, to keep it interesting, it should not be able to use a short cut of simply looking at an index.11\nThe SAS version of my benchmark query simply counts the number of rows associated with each value of curcdd, which Listing 2 tells us represents “ISO Currency Code - Daily”:\nproc sql;\n    CREATE TABLE curcdd_counts AS\n    SELECT curcdd, count(*) AS n\n    FROM comp.g_secd\n    GROUP BY curcdd\n    ORDER BY n DESC;\nquit;\n\nproc print data=curcdd_counts;\nrun;\nI put this SAS code into a file g_secd.sas in my home directory on the WRDS server and ran qsas g_secd.sas. Inspecting g_secd.log a few minutes later, I see The SAS System used: real time 6:08.66. So SAS needs more than 5 minutes to run this query. And here is a sample of the output in g_secd.lst:\n                                 The SAS System                                1\n                                        Wednesday, February 18, 2026 01:55:00 PM\n\n                           Obs    curcdd           n\n\n                             1     EUR      48130467\n                             2     JPY      31841749\n                             3     CNY      25687260\n                             4     INR      20676444\n                             5     GBP      20636852\n                             6     KRW      15657569\n                             7     HKD      14717583\n                             8     AUD      13812258"
  },
  {
    "objectID": "published/g_secd.html#footnotes",
    "href": "published/g_secd.html#footnotes",
    "title": "Some benchmarks with comp.g_secd",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nExecute install.packages(c(\"tidyverse\", \"DBI\", \"duckdb\", \"dbplyr\", \"farr\", \"RPostgres\") within R to install all the packages you need to run the code in this note.↩︎\nMy original code was in Perl, but by 2015 I had Python code to do the same job, and by 2019 there was an installable Python package, created with the help of Jingyu Zhang. Early on, I considered SQLite and MySQL as well as PostgreSQL, but I decided on PostgreSQL because of its rich type system, powerful SQL, and support for server programming (though I ended up not using the last one much). Though I had zero expertise, it seems that I somehow made the right choice. I was also an early adopter of RStudio in 2010 or 2011.↩︎\nThe only recent change was I recently exposed the (previously internal) sas_to_pandas() function that I used in my recent note on SAS and pandas.↩︎\nTrying to do Gow and Ding (2024) using pandas and SQL would be painful and slow.↩︎\nSee the Apache Arrow website at https://arrow.apache.org/overview/.↩︎\nA lot of early work involved using poorly documented libraries to keep the memory impact to a minimum and to facilitate the wrds_update_pq() conditional-update functionality.↩︎\nNote that the “using R” part is relatively unimportant. For example, it would be easy to do everything with essentially identical performance using Python.↩︎\nIf you look at Listing 1, you may notice “135GB”; this is incorrect and should be “135 GiB” (a GiB is a “binary” unit in which 1 GiB = \\(2^{30}\\) = 1,073,741,824 bytes).↩︎\nNote that the following Python code use the environment variable WRDS_ID. Call proc_contents(\"g_secd\", \"comp\", wrds_id=\"your_wrds_id\") if you don’t have this set.↩︎\nFor example, analogues of funda_mod as described in Chapter 6 of Gow and Ding (2024).↩︎\nWhether queries can use such shortcuts obviously depends on the specifics of the available indexes (in this case, there is no index to use for curcdd), but also on whether the backend system will use them for the query.↩︎\nNote that system_time() comes from the farr package.↩︎\nNote that the PostgreSQL server, like SAS, also has one or more indexes, which occupy an additional 15.75 GB.↩︎\nA recent update to db2pq means that the “last updated” string comes from the comments appended to the PostgreSQL table.↩︎"
  },
  {
    "objectID": "published/oxon_weather.html",
    "href": "published/oxon_weather.html",
    "title": "Defining winter and summer in Oxford",
    "section": "",
    "text": "Figure 1: Average daily temperatures for 91 days following indicated date for period 2001–2024\nIn the United States, one often hears people speak of the “official” start of seasons. Ironically, there seems to be nothing that is official about these dates. However, there is consensus about the dates in the US. The “official” start of summer is the summer solstice (for 2024: 20 June in Oxford and Boston) and the “official” start of winter is the winter solstice (for 2024: 21 December in Oxford and Boston).1\nIn Australia, the usual convention is to divide seasons by months. On this basis, winter starts on 1 June and summer starts on 1 December.2\nIs there a sense in which one approach is more correct than the other? Focusing on summer and winter, one definition for these seasons would be that winter starts on the first day of the 91-day period that is the coldest such period for a year averaged over a number of years. Similarly, summer should start on the first day of the 91-day period that is the hottest such period for a year averaged over a number of years.\nWe answer this question focusing on Oxford, England (latitude of 51.75222, longitude: -1.25596).\nDaily temperature data from Open-Meteo comprise a maximum and minimum temperature. So immediately we have two possible definitions of each season according to the temperature we use (e.g., summer could be the 91-day period that has the highest average minimum temperature or it could be the period that has the highest average maximum temperature). Here we consider both.\nThe start of winter based on the 91-day period with the lowest average maximum temperature is 29 November. The start of winter based on the 91-day period with the lowest average minimum temperature is 08 December.\nThe start of summer based on the 91-day period with the highest average maximum temperature is 14 June. The start of summer based on the 91-day period with the highest average minimum temperature is 16 June. So using maximums, we get close to the Australian convention for winter and close to the US convention for summer.\nInterestingly, it seems that using average maximums for summer and winter gets closest to the current approach in Australia. However, even using these we have the issue that spring begins on 28 February and autumn begins on 13 September. This implies a spring of 106 days and an autumn of 77 days."
  },
  {
    "objectID": "published/oxon_weather.html#footnotes",
    "href": "published/oxon_weather.html#footnotes",
    "title": "Defining winter and summer in Oxford",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSeasons reckoned in this way are known as astronomical seasons. See here.↩︎\nSeasons reckoned in this way are known as meteorological seasons. See here.↩︎"
  },
  {
    "objectID": "published/shared_code.html",
    "href": "published/shared_code.html",
    "title": "Shared code",
    "section": "",
    "text": "Open-source software dominates in certain areas. Probably most internet sites run on Linux machines. Python and R are huge in data science and statistics. All of these systems rely on thousands of open-source packages that are continually being improved in part because anyone can see how they work.\nAcademic research is quite different. Most research papers are really software development projects in disguise. While the final output is a PDF rather than an app, a tremendous amount of coding is often involved and multiple authors can be involved.\nYet the open-source model has not taken off in academia. While some journals are including data-and-code repositories as part of their process, these do not yet dominate. Authors are generally reluctant to share their code and data. There are multiple reasons for this in my view. First, the code is often a “trade secret” of sorts; future papers may be produced and authors may want to retain a competitive edge in what is essentially a zero-sum game.1 Second, having code available risks making it easy to show how fragile results are. It is difficult to replicate most research papers without access to the code and data, and many papers’ results are “fragile” at best. Third, a lot of authors likely fear embarrassment if others could see their code. Academics’ code is often inefficient, difficult to read, perhaps even wrong.\nFor some reason, a lot of the publicly available code in accounting research relates to two seemingly obscure topics: Fama-French industries and winsorization. As we cover both topics in Empirical Research in Accounting: Tools and Methods, I discuss these a little below.\n\n\n\n\n\n\nTip\n\n\n\nIn writing this note, I use several packages including those listed below.2 This note was written using Quarto and compiled with RStudio, an integrated development environment (IDE) for working with R. The source code for this note is available here and the latest version of this PDF is here.\n\nlibrary(dplyr)\nlibrary(farr)"
  },
  {
    "objectID": "published/shared_code.html#footnotes",
    "href": "published/shared_code.html#footnotes",
    "title": "Shared code",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nZero-sum because papers need to be published in a finite list of journals, which publish a finite number of papers. Publishing your paper on some topic often means not publishing mine.↩︎\nExecute install.packages(c(\"dplyr\", \"farr\", \"haven\")) within R to install all the packages you need to run the code in this note.↩︎\nAccording to https://siccode.com, “Standard Industrial Classification Codes (SIC Codes) identify the primary line of business of a company. It is the most widely used system by the US Government, public, and private organizations.”↩︎\nThe choices relate to handling of ties and interpolating values. See ? quantile in R for details.↩︎\nWhile the inclusion of prob = 0.01 is not strictly necessary given that that’s the default used by the function, this code does illustrate how you could choose prob = 0.02 to get winsorization at 2% and 98% levels, a popular alternative choice.↩︎"
  },
  {
    "objectID": "published/weather-syd.html",
    "href": "published/weather-syd.html",
    "title": "Defining winter and summer in Sydney",
    "section": "",
    "text": "Figure 1: Average daily temperatures for 91 days following indicated date for period 2001–2023\n\n\n\n\n\nIn the United States, one often hears people speak of the “official” start of seasons. Ironically, there seems to be nothing that is official about these dates. However, there is consensus about the dates. The “official” start of summer is the summer solstice (for 2024: 20 June in Boston, 21 December in Sydney) and the “official” start of winter is (for 2024: 21 December in Boston, 21 June in Sydney).\nIn Australia, the usual convention is to divide seasons by months. On this basis, winter starts on 1 June and summer starts on 1 December.\nIs there a sense in which one approach is more correct than the other? Focusing on summer and winter, one definition for these seasons would be that winter starts on the first day of the 91-day period that is the coldest such period for a year averaged over a number of years. Similarly, summer should starts on the first day of the 91-day period that is the hottest such period for a year averaged over a number of years.\nWe answer this question focusing on Sydney, Australia (latitude of -33.9, longitude: 151).\nDaily temperature data from Open-Meteo comprise a maximum and minimum temperature. So immediately we have two possible definitions of each season according to the temperature we use (e.g., summer could be the 91-day period that has the highest average minimum temperature or it could be the period that has the highest average maximum temperature. Here we consider both.\nThe start of winter based on the 91-day period with the lowest average maximum temperature is 27 May. The start of winter based on the 91-day period with the lowest average minimum temperature is 03 June. So whether we use maximums or minimums, we get close to the Australian convention for winter.\nThe start of summer based on the 91-day period with the highest average maximum temperature is 30 November. The start of summer based on the 91-day period with the highest average minimum temperature is 16 December. With summer, we get close to the US convention for summer using minimums, but close to the Australian convention using maximums.\nInterestingly, it seems that using average maximums for summer and winter gets closest to the current approach in Australia. However, even using these we have the issue that spring begins on 26 August and autumn begins on 01 March. This implies a spring of 96 days and an autumn of 87 days."
  },
  {
    "objectID": "published/window-functions.html",
    "href": "published/window-functions.html",
    "title": "Responsive open-source software: Two examples from dbplyr",
    "section": "",
    "text": "In this note, I explore some recent changes in the open-source R package dbplyr to illustrate some of the beauty of how open-source software evolves in practice. In particular, I offer two case studies where features requested by users became reality in dbplyr, which may be my favourite R package.\n\n\n\n\n\n\nTip\n\n\n\nIn writing this note, I used the packages listed below.1 At the time of writing, you also need to install the development version of dbplyr, which you can do using the remotes::install_github() command below. This note was written and compiled using Quarto with RStudio, an integrated development environment (IDE) for working with R. The source code for this note is available here and the latest version of this PDF is here.\n\n\n\nlibrary(farr)\nlibrary(dplyr, warn.conflicts = FALSE)\nlibrary(DBI)\n\n\nremotes::install_github(\"tidyverse/dbplyr\", ref = \"main\")"
  },
  {
    "objectID": "published/window-functions.html#sec-retail-data",
    "href": "published/window-functions.html#sec-retail-data",
    "title": "Responsive open-source software: Two examples from dbplyr",
    "section": "3.1 The retail sales data set",
    "text": "3.1 The retail sales data set\nThe first query I will use to illustrate the use of window functions with mutate() comes from Chapter 3 of SQL for Data Analysis, which uses data on retail sales by industry in the United States to explore ideas on time-series analysis.\nI will use DuckDB as my database engine. Creating a DuckDB database requires just one line of code:\n\ndb &lt;- dbConnect(duckdb::duckdb())\n\nI then call get_data() to load the data into the database. I name the table (\"retail_sales\") so that I can refer to it when using SQL.5\n\nretail_sales &lt;- get_data(db, \n                         dir = \"Chapter 3: Time Series Analysis\",\n                         file = \"us_retail_sales.csv\",\n                         name = \"retail_sales\")\n\nThe SQL version of the query provided in SQL for Data Analysis is as follows:\n\nSELECT sales_month,\n  avg(sales) OVER w AS moving_avg,\n  count(sales) OVER w AS records_count\nFROM retail_sales\nWHERE kind_of_business = 'Women''s clothing stores'\nWINDOW w AS (ORDER BY sales_month \n             ROWS BETWEEN 11 PRECEDING AND CURRENT ROW)\nORDER BY sales_month DESC;\n\n\nDisplaying records 1 - 10\n\n\nsales_month\nmoving_avg\nrecords_count\n\n\n\n\n2020-12-01\n2210.500\n12\n\n\n2020-11-01\n2301.917\n12\n\n\n2020-10-01\n2395.583\n12\n\n\n2020-09-01\n2458.583\n12\n\n\n2020-08-01\n2507.417\n12\n\n\n2020-07-01\n2585.667\n12\n\n\n2020-06-01\n2659.667\n12\n\n\n2020-05-01\n2763.417\n12\n\n\n2020-04-01\n2989.083\n12\n\n\n2020-03-01\n3248.167\n12\n\n\n\n\n\nTo do the same using dbplyr, I can just specify .order and .frame in the call to mutate().\n\nmvg_avg &lt;-\n  retail_sales |&gt;\n  filter(kind_of_business == \"Women's clothing stores\") |&gt;\n  mutate(moving_avg = mean(sales, na.rm = TRUE),\n         records_count = n(),\n         .order = sales_month,\n         .frame = c(-11, 0)) |&gt;\n  select(sales_month, moving_avg, records_count) |&gt;\n  arrange(desc(sales_month))\n\nAs can be seen in Table 1, the resulting data set is the same.6\n\nmvg_avg |&gt;\n  collect(n = 10)\n\n\n\nTable 1: Moving average sales for women’s clothing store (first 10 records)\n\n\n\n\n\n\nsales_month\nmoving_avg\nrecords_count\n\n\n\n\n2020-12-01\n2210.500\n12\n\n\n2020-11-01\n2301.917\n12\n\n\n2020-10-01\n2395.583\n12\n\n\n2020-09-01\n2458.583\n12\n\n\n2020-08-01\n2507.417\n12\n\n\n2020-07-01\n2585.667\n12\n\n\n2020-06-01\n2659.667\n12\n\n\n2020-05-01\n2763.417\n12\n\n\n2020-04-01\n2989.083\n12\n\n\n2020-03-01\n3248.167\n12"
  },
  {
    "objectID": "published/window-functions.html#the-legislators-data",
    "href": "published/window-functions.html#the-legislators-data",
    "title": "Responsive open-source software: Two examples from dbplyr",
    "section": "3.2 The legislators data",
    "text": "3.2 The legislators data\nAnother data set I will use to illustrate the use of mutate() is the legislators data set used in Chapter 4 of SQL for Data Analysis to explore cohort analysis. The legislators data set comprises two tables, which I read into my DuckDB database using the following code.\n\nlegislators_terms &lt;- get_data(db, \n                         dir = \"Chapter 4: Cohorts\",\n                         file = \"legislators_terms.csv\",\n                         name = \"legislators_terms\")\n\nlegislators &lt;- get_data(db, \n                         dir = \"Chapter 4: Cohorts\",\n                         file = \"legislators.csv\",\n                         name = \"legislators\")\n\nA third data set used in Chapter 4 of SQL for Data Analysis is the year_ends table, which I construct in R and copy to my DuckDB database using the following code.7\n\nyear_ends &lt;-\n  tibble(date = seq(as.Date('1770-12-31'), \n                    as.Date('2030-12-31'), \n                    by = \"1 year\")) |&gt;\n  copy_to(db, df = _, overwrite = TRUE, name = \"year_ends\")\n\nFinally, I add a minor tweak to original query by adding an enumerated data type that ensures the tables are ordered meaningfully.8\n\nCREATE TYPE band AS ENUM ('1 to 4', '5 to 10', '11 to 20', '21+')\n\nThe following is a modified version of the SQL query found on page 173 of Chapter 4 of SQL for Data Analysis.9 As can be seen, because of how we defined the band data type, it is meaningful to sort by tenure (check what happens if you omit the casting of tenure to type band using ::band).\n\nWITH term_dates AS (\n  SELECT DISTINCT a.id_bioguide, b.date\n  FROM legislators_terms a\n  JOIN year_ends b \n  ON b.date BETWEEN a.term_start AND a.term_end \n    AND b.date &lt;= '2020-01-01'),\n\ncum_term_dates AS (\n  SELECT id_bioguide, date,\n    count(date) OVER w AS cume_years\n  FROM term_dates\n  WINDOW w AS (PARTITION BY id_bioguide \n               ORDER BY date \n               ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)),\n  \ncum_term_bands AS (\n  SELECT date,\n    CASE WHEN cume_years &lt;= 4 THEN '1 to 4'\n         WHEN cume_years &lt;= 10 THEN '5 to 10'\n         WHEN cume_years &lt;= 20 THEN '11 to 20'\n         ELSE '21+' END AS tenure,\n    COUNT(DISTINCT id_bioguide) AS legislators\n  FROM cum_term_dates\n  GROUP BY 1,2)\n  \nSELECT date, tenure::band AS tenure,\n  legislators * 100.0 / sum(legislators) OVER w AS pct_legislators \nFROM cum_term_bands\nWINDOW w AS (partition by date)\nORDER BY date DESC, tenure;\n\n\nDisplaying records 1 - 10\n\n\ndate\ntenure\npct_legislators\n\n\n\n\n2019-12-31\n1 to 4\n29.98138\n\n\n2019-12-31\n5 to 10\n32.02980\n\n\n2019-12-31\n11 to 20\n20.11173\n\n\n2019-12-31\n21+\n17.87710\n\n\n2018-12-31\n1 to 4\n25.60297\n\n\n2018-12-31\n5 to 10\n33.76623\n\n\n2018-12-31\n11 to 20\n21.33581\n\n\n2018-12-31\n21+\n19.29499\n\n\n2017-12-31\n1 to 4\n24.53532\n\n\n2017-12-31\n5 to 10\n34.75836\n\n\n\n\n\nTranslating the query to dbplyr is greatly facilitated by the use of CTEs, as each CTE can be constructed as a separate remote or lazy data frame. Here remote means that the data are in a database. In this case, “remote” does not mean “far away”, but the data could be physically distant as in the case of the dsf data frame examined above. The term “lazy” refers to the fact that the underlying SQL query for the data frame is not executed until we ask it to be evaluated using functions like collect() (to bring the data into R) or compute() (to create a temporary table in the database).\n\nterm_dates &lt;-\n  legislators_terms |&gt;\n  inner_join(year_ends |&gt; filter(date &lt;= '2020-01-01'),\n             join_by(between(y$date, x$term_start, x$term_end))) |&gt;\n  distinct(id_bioguide, date) \n\nHere I use .order, .frame, and .by to get the same window used in the SQL above.\n\ncum_term_dates &lt;-\n  term_dates |&gt;\n  mutate(cume_years = n(),\n         .by = id_bioguide,\n         .order = date,\n         .frame = c(-Inf, 0)) |&gt;\n  select(id_bioguide, date, cume_years) \n\nThe following query aggregates the data into different ranges of tenure. Note that a glitch in n_distinct() in the duckdb package (presumably something that will be fixed soon enough) means that I need to directly call SQL COUNT(DISTINCT id_bioguide) as can be seen in the code below.\n\ncum_term_bands &lt;-\n  cum_term_dates |&gt; \n  mutate(tenure = case_when(cume_years &lt;= 4 ~ '1 to 4',\n                            cume_years &lt;= 10 ~ '5 to 10',\n                            cume_years &lt;= 20 ~ '11 to 20',\n                            TRUE ~ '21+')) |&gt;\n  mutate(tenure = sql(\"tenure::band\")) |&gt;\n  summarize(legislators = sql(\"COUNT(DISTINCT id_bioguide)\"),\n            .by = c(date, tenure))\n\nThe final query pulls everything together and uses a window function to count the number of legislators in the denominator of pct_legislators. As can be seen in Table 2, the resulting data set is the same as that produced by the SQL above.\n\ncum_term_bands |&gt;\n  mutate(sum_legislators = sum(legislators),\n         pct_legislators = legislators * 100.0 / sum_legislators,\n         .by = date) |&gt;\n  select(date, tenure, pct_legislators) |&gt;\n  arrange(desc(date), tenure) |&gt;\n  collect(n = 10)\n\n\n\nTable 2: Percentage of legislators by tenure (first 10 records)\n\n\n\n\n\n\ndate\ntenure\npct_legislators\n\n\n\n\n2019-12-31\n1 to 4\n29.981\n\n\n2019-12-31\n5 to 10\n32.030\n\n\n2019-12-31\n11 to 20\n20.112\n\n\n2019-12-31\n21+\n17.877\n\n\n2018-12-31\n1 to 4\n25.603\n\n\n2018-12-31\n5 to 10\n33.766\n\n\n2018-12-31\n11 to 20\n21.336\n\n\n2018-12-31\n21+\n19.295\n\n\n2017-12-31\n1 to 4\n24.535\n\n\n2017-12-31\n5 to 10\n34.758\n\n\n\n\n\n\n\n\nAgain, the new functionality supports powerful analyses with clean, easy-to-read code."
  },
  {
    "objectID": "published/window-functions.html#footnotes",
    "href": "published/window-functions.html#footnotes",
    "title": "Responsive open-source software: Two examples from dbplyr",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nRun install.packages(c(\"farr\", \"dplyr\", \"DBI\", \"duckdb\")) within R to install all the packages you need to run the code in this note.↩︎\nThe “go-ahead” seems to be implied by Hadley’s reopening of the issue on 24 June 2021.↩︎\nSee Chapter 17 of Empirical Research in Accounting: Tools and Methods for more on the michels_2017 data. Of course, a more careful approach would probably use trading days before and after events; see Chapter 12 of Empirical Research in Accounting: Tools and Methods for discussion of how to do this.↩︎\nI have written about this book here.↩︎\nIt is not necessary to specify a table name if we are just using dbplyr to analyse the data.↩︎\nThis makes sense as the dplyr/dbplyr code is translated into SQL behind the scenes.↩︎\nIn Chapter 4 of SQL for Data Analysis, year_ends is created using SQL in the relevant dialect, which is PostgreSQL in the book.↩︎\nThis data type is similar to factors in R, a topic covered in Chapter 2 of Empirical Research in Accounting: Tools and Methods.↩︎\nApart from formatting changes, the main modification I made to the query was the use of common-table expressions (CTEs) in place of subqueries. I discuss the merits of CTEs (and of using dbplyr to write SQL) here.↩︎"
  }
]