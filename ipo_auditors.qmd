---
title: "Spreadsheets for data collection"
author: Ian D. Gow
date: 2025-01-01
date-format: "D MMMM YYYY"
format:
  html:
    colorlinks: true
  pdf: 
    include-in-header:
      text: |
        \usepackage[group-digits = integer, group-separator={,}, group-minimum-digits = 4]{siunitx}
        \usepackage{scrextend}
        \deffootnote{1.6em}{1.6em}{\thefootnotemark.\enskip}
        \addtokomafont{disposition}{\rmfamily}
    colorlinks: true
    geometry:
      - left=2cm
      - right=2cm
    papersize: a4
    mainfont: TeX Gyre Pagella
    mathfont: TeX Gyre Pagella Math
    CJKmainfont: AppleMyungjo
bibliography: papers.bib
csl: jfe.csl
---

As discussed in a [recent post on LinkedIn](https://www.linkedin.com/posts/iangow_working-with-json-data-the-case-of-sec-submissions-activity-7199520005409906690-XlFZ), one goal of [*Empirical Research in Accounting: Tools and Methods*](https://iangow.github.io/far_book/) (co-authored with [Tony Ding](https://fbe.unimelb.edu.au/our-people/staff/accounting/tongqing-ding)) is to provide a pathway to mastery of the contents of [*R for Data Science*](https://r4ds.hadley.nz).^[*Empirical Research in Accounting: Tools and Methods* was published in [print form](https://www.routledge.com/Empirical-Research-in-Accounting-Tools-and-Methods/Ding-Gow/p/book/9781032586502) by CRC Press in 2024 and remains free online.]
In that post, I identified three gaps in our coverage relative to [*R for Data Science*](https://r4ds.hadley.nz).
Previous notes covered [hierarchical data](https://raw.githubusercontent.com/iangow/notes/main/sec_submissions.pdf) and [dates and times](https://raw.githubusercontent.com/iangow/notes/main/datetimes.pdf) and this note covers spreadsheets.

My aim with these notes is not to replace existing resources such as [*R for Data Science*](https://r4ds.hadley.nz), but to build on such resources with applied examples using richer data sets.^[Two excellent resources are [Chapter 20](https://r4ds.hadley.nz/spreadsheets) of @Wickham:2023aa and @Broman:2018aa and I discuss these in more detail below.]
The application I study here uses spreadsheets as part of a process of linking data on IPOs in the United States with auditor IDs, as found in Form AP data.^[Guidance on downloading the Form AP data is provided in [another earlier note](https://github.com/iangow/notes/blob/main/form_ap_names.pdf).]

In writing this note, I use the packages listed below.^[Execute `install.packages(c("tidyverse", "DBI", "duckdb", "farr", "googlesheets4", "arrow", "dbplyr")` within R to install all the packages you need to run the code in this note.]
This note was written using [Quarto](https://quarto.org) and compiled with [RStudio](https://posit.co/products/open-source/rstudio/), an integrated development environment (IDE) for working with R.
The source code for this note is available [here](https://raw.githubusercontent.com/iangow/notes/main/ipo_auditors.qmd) and the latest version of this PDF is [here](https://raw.githubusercontent.com/iangow/notes/main/ipo_auditors.pdf).

```{r}
#| message: false
#| include: true
library(tidyverse)
library(DBI)
library(farr)
library(googlesheets4)
library(arrow)
library(dbplyr)
```

```{r}
#| include: false
library(tinytable)
```

# Spreadsheets and data science

My sense is that the use of spreadsheets in data science falls into one of two scenarios.
In the first scenario, the data are provided in a spreadsheet format that is outside the control of the reseacher.
In the other scenario, spreadsheets are used by the researcher as part of the data collection process.

## Scenario 1: Importing spreadsheet data

[Chapter 20](https://r4ds.hadley.nz/spreadsheets) of @Wickham:2023aa covers spreadsheets with an emphasis on importing data provided in spreadsheet form---either in a Microsoft Excel file or in a Google Sheets document.
While the chapter in @Wickham:2023aa does cover writing data to Excel or Google Sheets, it does not discuss *why* you would want to do this.^[In general, there are much better formats for exporting data than either of these approaches and I would see no good reason to use Excel files as part of a data science workflow.]

The coverage provided in @Wickham:2023aa is excellent and I refer readers to that book for details on importing spreadsheet data.
However, a weakness of [Chapter 20](https://r4ds.hadley.nz/spreadsheets) of @Wickham:2023aa is its limited guidance on what I would regard as essential best practices for importing spreadsheet data.^[To be fair, earlier chapters of @Wickham:2023aa do discuss issues regarding importing data and scripts, but I think the importance of reproducibility is high enough---and the tendency for many to neglect it in practice---that it really bears repetition and more detailed guidance in each context.]
So I provide some guidance on these here.

```{r}
#| label: fig-whole-game 
#| echo: false
#| out.width: NULL
#| fig-cap: |
#|   An expanded representation of the data science workflow
#| fig-alt: |
#|   A diagram displaying the data science workfow.

knitr::include_graphics("images/data-science-collect.png", dpi = 300)
```

First, the import process should be done as much as possible in a script that allows the whole process to be repeated.
Importing spreadsheets falls clearly into the *Curate* portion of the extended "whole game" of a data analysis workflow depicted in @fig-whole-game.^[@fig-whole-game is a slightly expected version of the "whole game" I discussed in [an earlier note](https://github.com/iangow/notes/blob/main/import_sirca.pdf) on data curation---itself an extension of the whole game depicted in @Wickham:2023aa [p. xii].]
The whole *Curate* process should be documented and scripted as much as possible.
For example, if the spreadsheet data come from a URL for a Microsoft Excel file, then the download step should be [if possible] part of the script.

Second, the source data files should not be modified in *any* way.
While this point is closely related to the previous one, I believe that it warrants specific emphasis.
Clearly one should not be opening Excel files and modifying data by deleting columns or reformatting values, as these steps are undocumented and not repeatable in an automated fashion.
My advice here mirrors that of @Broman:2018aa [p. 7] in a slightly different context: "Your primary data file should be a pristine store of data. Write-protect it, back it up, and do not touch it."

Even simply opening the source Excel files in Excel and clicking save can modify the underlying files in ways that affect later parts of the data science workflow.
I had one project where the source data appeared to be `.xls` Excel files, but were in fact simply structured text in HTML format.
Scripts written on this basis performed better than ones that read them as Excel data.^["Performed better" here means not only that the scripts worked faster, but that the imported data were easier to work with.]
However, when updated spreadsheets were received from the vendor they first passed through the data team at the institution I then worked at and my scripts no longer worked.
The data team had "helpfully" opended the files in Excel and saved them before sending them along to me.
I had to insist that the data team pass along the data files *exactly* as they had received them without "helpful" steps in between.^[Additional examples of what can go wrong if one opens a structured text data file in Excel and saves it are provided in [my earlier note](https://github.com/iangow/notes/blob/main/import_sirca.pdf).]

The deleterious effects of "simply" opening a file in Excel and saving it can be seen by comparing @tbl-orig with @tbl-excel.
@tbl-orig shows the original data, while @tbl-excel shows the file created by Excel.
As can be seen, Excel changed the first column so that the data are no longer in a consistent (`d/m/yyyy`) format, mangled the first `cusip` value, and messed up the encoding (and I wasn't even using Windows!) so that the Korean name in the third row is no longer legible.


test_date | cusip      | result | name  |
----------|------------|--------|-------|
1/7/2006  | "945324E7" | 1.7    | Bob   |
8/3/2013  | "12345678" | 2.6    | Sarah |
28/4/2016 | "23456789" | NA     | 철수  |

: Original CSV data {#tbl-orig}

test_date | cusip      | result | name  |
----------|------------|--------|-------|
1/7/06    | 9.45E+12   | 1.7    | Bob   |
8/3/13    | "12345678" | 2.6    | Sarah |
28/4/2016 | "23456789" | NA     | _Êö÷ |

: Excel-exported CSV data {#tbl-excel}

## Scenario 2: Using spreadsheets for data collection

While Chapter 20 of @Wickham:2023aa focuses on importing data from spreadsheets, @Broman:2018aa focuses on the use of spreadsheets for data collection.
Interestingly, @Wickham:2023aa does not discuss primary data collection in any detail and @Broman:2018aa does not *explicitly* cover the process of importing data into statistical analysis software.
However, it does seem implicit in @Broman:2018aa that importing data into such software will be part of the process.
For example, @Broman:2018aa [p. 9] suggest that "data rearrangement is best accomplished via code (such as with an R, Python, or Ruby script) so you never lose the record of what you did to the data."
In terms of @fig-whole-game, @Broman:2018aa is largely about "Collect" and @Wickham:2023aa picks up with "Import".^[The original "whole game" in @Wickham:2023aa does not even discuss "Collect", but I think @Broman:2018aa shows how it is an essential part of the data science process.]

I view this note as building on @Broman:2018aa by adding some additional recommendations, linking data collection with data importation *à la* @Wickham:2023aa, and providing a more comprehensive use case to highlight some issues and ideas not addressed by either @Broman:2018aa or @Wickham:2023aa.

@Broman:2018aa emphasizes cases where the user of the data is entering the data directly.
However, a very common use case I have encountered is on where I (along with collaborators) will be using the data, but the data are being entered by someone else (perhaps several others).
This note focuses on this scenario.

This note aims to expand on and refine the advice provided by @Broman:2018aa for situations where others---such as data entry specialists or research assistants---are entering data into spreadsheets.
@Broman:2018aa provide a list of twelve recommendations.
In this note, I will provide a set of recommendations specific to the approach I recommend here and then follow those with comments on the recommendations in @Broman:2018aa in light of the different context.

As mentioned earlier, @Broman:2018aa is predicated on the user of the data also doing the data entry.
References to things such as `Serum_batch1_2015-01-30.csv`, `Mouse153` and `glucose_6_weeks` suggest a setting of collection of observations and measurements from scientific experiments.

In contrast, a more common setting in my experience is that of collecting relatively large amounts of somewhat messy data from documents found on the internet.
Typically, researchers need people to collect these data because either they are too messy to be collected in an automated fashion with sufficiently low cost and high quality or their collection requires some element of judgement.

That context helps to respond to the question that many users might have at this point: Why use spreadsheets at all?
Given that spreadsheets have all these problems, it might seem better to use some other appraoch.
For example, why not use a web form that is used to populate a database?
I think the answer lies in a couple of points.

First, many researchers are collecting data but do not have access to the skills needed to stand up a web page that allows data to be collected and put into a database.
For all their problems, using spreadsheets removes the need for such skills.

Second, many data collection tasks are unclear.
Precisely what data will be collected?
What values are valid in certain columns?
What strange cases need to be addressed and how?
In such situations, often the best approach is to just start collecting data and adapt over time.
Spreadsheets are often the best tool for such situations.
Columns can be added later on.
Data validation rules can be tightened and loosened as circumstances dictate.
This can be much easier than bringing the web-and-database developer back into to add new fields to the forms and new columns to the database tables.

Additionally, data collection often needs to be quite free-form.
For example, maybe it make sense to drop a link to a document to explain a classification decision made in another column.
Spreadsheets allow this easily.

## Other use cases for spreadsheets

### Spreadsheets for modelling

Spreadsheet tools such as Microsoft Excel and Google Sheets are the mainstay of certain kinds of analysis, including financial modelling and valuation.
Spreadsheets also allow people who use them for other reasons to easily run Monte Carlo simulations and the like.
I view these uses as legitimate, but place them slightly outside the scope of data science in the sense implied by @fig-whole-game, hence beyond the scope of this note.

The one word of caution I would offer is that one should not fall into the trap of thinking "I use Excel for modelling, so maybe I should also use Excel for statistical analysis."
The shortcomings of Excel for statistical analysis are well-documented and need not be repeated here [see @Broman:2018aa for some leads].


I will also note a couple of additional points.
First, even if (say) Microsoft Excel is your preferred tool for producingfinancial or valuation models, there can be value in replicating analysis in something like Python or R.
In 2005 or 2006, I was working with Morgan Stanley to produce a valuation model that could be used by institutional investor clients with data generated by Morgan Stanely research analysts.
I would produce a version of the model in Excel and programmers would translate it into C++ code.
In one call, one of the programmers mentioned that they had detected the change in formula after a certain number of years and implemented that in the code.
Aargh!
This change in formula was simply a spreadsheet error on my part that because all the more obvious when the logic was transcribed from Excel formulas to C++ code.
The error was quickly rectified.

Second, there is a world of best practices to be followed with the use of spreadsheets in financial and valuation modelling that are deserving of (at least) a note of their own.
When I worked at corporate finance consulting firm Stern Stewart in the late 1990s, a big part of the training of new analysts and associates was inculculating some of these best practices, such as separating inputs from calculations and documentating data sources and rationales for approaches.
A lot of ideas used there are analogues of ideas found in discussions of reproducibility in data science.

### Spreadsheets for data analysis

While @Broman:2018aa does mandate "No Calculations in the Raw Data Files", this is coupled with a more lenient piece of advice [@Broman:2018aa, p. 7]: "If you want to do some analyses in Excel, make a copy of the file and do your calculations and graphs in the copy.
I would argue that this is actually not a good idea at all.

If the idea is that the analysis will be the data analytical form of brainstorming that will not form the basis of anything beyond that, I think that is fine.
But if the calculations and graphs in the copy end up going in a document, then the data in the copy become the de facto canonical version of the data.
If the original data are updated or correct, there will often be no easy way to incorporate the associated changes into the analysis.
Also it will often be difficult to be sure that the raw data have been unchanged.
Having the original data as a "pristine store of data" [@Broman:2018aa, p. 7] does not help much if the data actually used have been corrupted in some way (e.g., by accidentally typing a value into a cell).^[I beleive it is possible to lock seelcted cells in a spreadsheet, but I suspect that few users avail themselves of this functionality.]

### Spreadsheets for organizing output

The final use case for spreadsheets that I will discuss is the use of spreadsheets for producing output, such as tables and graphs.
Many researchers who use statistical packages such as SAS or Stata reach the limits of their skills with those packages when it comes to producing plots or tables of statistical analyses for inclusion in papers.
Such users will often turn to packages such as Microsoft Excel to produce this kind of output.

I would recommend that readers of this note consider working on their data science skills to obviate the use of Excel for this "last mile" of the data analysis journey for a number of reasons.

First, using spreadsheets for this part of the process inevitably breaks the reproducibility of any analysis.
The Excel workflow for making a plot is roughly:

  1. Save data for analysis
  2. Open data in Excel spreadsheet.
  3. Using your mouse, highlight the data to be used in graph.
  4. Click one of the graph icons.
  5. Make a number of seclections in various dropdown menus and checkboxes to produce the desired plot.
  6. Copy plot to Microsoft Word document.
  
Now, suppose the data change.
The whole process needs to be repeated again and it is very unlikely that any step (save perhaps the first one) is associated with any documentation whatsoeever.

One consequence of using a manual workflow is that analyses can quickly become inconsistent with each other, as it is often seems not worthwhile to update earlier analyses when data change.

Second, using spreadsheets for this part of the process can lead to errors.
Manual steps are easy to mess up and I've seen cases where column labels are switched or the signs of coefficients are wrong because they were updated manually.

Finally, the plotting functionality of Excel will generally pale in comparison to what is available in Python or R (or even Stata).

A lot of researchers seem unaware that it's even possible for modern statistical packages to produce production-quality output without manual intervention at every step.


# Google Sheets

My primary recommendation is to use Google Sheets rather than Excel spreadsheets.
While it seems somewhat less than ideal to recommend an approach that relies on a product from one company, I am not aware of an equivalent to Google Sheets in terms of functionality at this time.^[If there is an alternative---or if such a product becomes available at a later date---hopefully much of what I say here will apply to that product.]

Google Sheets offers a number of benefits over Excel even for a single user.
For example, Google Sheets has some functions (e.g., support for regular expressions) that can greatly facilitate data entry.
Also I haven't had encoding issues with Google Sheets of the kind we saw above with the Korean name `철수` being mangled into `_Êö÷`.
We saw above that Excel can mangle CSV files in ways that Google Sheets will not.
I would go so far as to provide the blanket advice: "Don't use Excel to handle CSV files."

But the benefits of Google Sheets become especially clear when there are several people working with the data, such as multiple researchers and possibly many research assistants.

One issue I have encountered with Excel is version control and these issues are particularly acute when there are multiple people involved in data collection.
In my experience, the standard approach to version control for users of products like Microsoft Word or Microsoft Excel involve two elements.
First, there is some implicit shared mental note about who has "the document" at any given time.
When Anne has "the document", Bob and Clara know that they should not be editing the document.

This is imperfect for many reasons.
For one, the implicit shared mental note system is not perfect.
So problems arise when Bob starts editing the literature review section of document while Anne is busy reworking the introduction: "I didn't realize that Anne was working on that. I guess we will have to figure out how to combine my changes with hers."
Second, Microsoft fans often share versions via email and problems can arise when Clara starts editing `ABC_12-24-24_anne_final_v_21.docx` rather than the copy Anne sent via email with a "please use this version" subject line (`ABC_12-24-25_anne_updated_v_23.docx`) the next day.

All these issues are even worse with Microsoft Excel because there are no "track changes" or "compare documents" features to assist with clean-up of messes from Bob-or-Anne-edited-the-wrong-version situations.
These issues largely disappear with Google Sheets.^[That is, of course, you don't have dimwitted people coming from a Microsoft Office background replicating their approach to version control by creating copies of Google Sheets documents!]

Another benefit of Google Sheets is that access to the data is continuous.
In a Google Docs setting, all co-authors can have access to the document and Bob can edit the literature review while Anne punches up the introduction.^[That said, while Google Docs is much better than Microsoft Word for collaborative editing, I would not use it for this purpose myself because there are better ways if you have co-authors not trapped in the late 20th century.]

Relatedly data access is ensured. 
There is no risk that the Excel spreadsheet containing the data ends up on the computer of a former research assistant who is no longer available.
Knowing that the final data file is `C:/Desktop/Best Project Ever/data collection_final_v3_3-4-1997.xls` does not help if that computer was wiped when the RA left for a better job.^[This is not a purely hypothetical scenario!]

Having continuous data access can be particularly helpful if research assistants are doing much of the data collection.
One does not need to email the research assistant saying "please send me the updated spreadsheet before you go home today, as I'd like to run some numbers."
The spreadsheet is right there.

Also, one can monitor data entry in real-time.
Importing data using `googlesheets4` and generating plots using `ggplot2` can happen without bothering the RAs in any way.
I have worked on projects where there were dozens of research assistant (RAs).
While there were detailed instructions, one cannot be sure that RAs will read them.
Nor can one anticipate all the ways that things can go wrong.
With Google Sheets, once can try importing data as it is being collected and discover issues along the way.
Then one can email the relevant RA (e.g., "Please don't put values like `2013-03-23 (need to check)` in the `date` column"), and perhaps update instructions according.

Sometimes data collection tasks involve an initial learning curve for each RA, but the work can be so tedious that productivity declines over time.
With Google Sheets, it is relatively straightforward to monitor productivity over time.
In one case, we hired RAs on a week-by-week basis and found that RA productivity would decline after two weeks and we would not invite RAs back when that happened.^[Presumably these RAs found work, if they wanted it, on a new task for a different faculty member.]

Both Excel and Google Sheets offer data validation functionality.
But sometimes it is not possible to anticipate the range of valid values.
With Google Sheets, adding data validation on the fly is easy.
In the example above, we could add a validation rule requiring that `date` values be valid dates and added an addition column for notes to store the "need to check" comment.

With Google Sheets, multiple people can enter data at once. 
An alternative might be to have each person have their own Excel file, but this introduces complexity.
One approach would work with a single sheet shared by multiple users and have RAs fill in an `RA` column with their names for any rows for which they collected data.

With Google Sheets, data access can be added and removed with ease.
Almost everyone has a Google ID and this can be used to grant access.
Removing access when it is no longer needed is very simple too.

In short, Google Sheets is simply a superior product to Microsoft Excel for data collection.
Still Excel has its fans.
In the case of the stranded Excel file I mentioned above, the RA in question chose to download the provided Google Sheets file as an Excel file because he was "more comfortable" with the Excel interface.
The RA in question worked for a co-author, not for me, so I wasn't as insistent as I should've been.
Today I would configure the Google Sheets document so that downloading would not even be possible.

# Case study: Linking IPOs with auditor IDs

The best way to illustrate some key points about using spreadsheets to collect data is to explore a specific example in greater depth.
The example I study involves collecting data from various sources to facilitate the matching of IPOs with the firms that provided audits for the firms going public, where an audit firm is identified using the identification number provided by the Public Company Accounting Oversight Board (PCAOB) in the United States.

The data on IPOs come from the Nasdaq website and the data on audit firms come from the Form AP in [an earlier note](https://raw.githubusercontent.com/iangow/notes/main/form_ap_names.pdf).

## IPO data

Nasdaq provides information about IPOs on its website.
I have organized these data in two parquet files: `ipos.parquet` and `ipo_experts.parquet`.
A copy of the `ipos.parquet` file can be found at <https://go.unimelb.edu.au/2rq8> and a copy of the `ipos_experts.parquet` file can be found at <https://go.unimelb.edu.au/hrq8>.
If you want to follow along with the code below, you could download these data files into the `nasdaq` schema of a data repository organized along the lines of that I discuss in [Appendix E](https://iangow.github.io/far_book/parquet-wrds.html) of [*Empirical Research in Accounting: Tools and Methods*](https://www.routledge.com/Empirical-Research-in-Accounting-Tools-and-Methods/Ding-Gow/p/book/9781032586502) and then load them into a DuckDB database using code like this:

```{r}
db <- dbConnect(duckdb::duckdb())

ipo_experts <- load_parquet(db, "ipo_experts", "nasdaq")
ipos <- load_parquet(db, "ipos", "nasdaq")
```

Alternatively, you could put these two files in, say `~/Downloads/ipo_auditors`, and then load them as follows:

```{r}
#| eval: false
db <- dbConnect(duckdb::duckdb())

ipo_experts <- load_parquet(db, "ipo_experts", data_dir = "~/Downloads/ipo_auditors")
ipos <- load_parquet(db, "ipos", "nasdaq", data_dir = "~/Downloads/ipo_auditors")
```

For example, details about the IPO of Galmed Pharmaceuticals Ltd. (GLMD) on 13 March 2014 can be found at <https://www.nasdaq.com/market-activity/ipos/overview?dealId=926632-74581> and these data are found in the `ipo.parquet` file found at <https://go.unimelb.edu.au/2rq8>.
The Nasdaq page also includes information about the "experts" associated with each IPO.
The [information for GLMD](https://www.nasdaq.com/market-activity/ipos/experts?dealId=926632-74581) that can be found in the `ipo_experts` data frame is shown in @tbl-glmd-experts.
The `ipo_experts.parquet` file can be found at <https://go.unimelb.edu.au/hrq8>.

```{r}
#| include: false
num_ipos <-
  ipos |>
  count() |>
  pull()
```

There are `r prettyNum(num_ipos, big.mark = ",")` rows in `ipos.parquet` and each row represents an IPO  identified by the primary key column `dealID`.
I collect some statistics on these deals in `expert_counts` and show the results in @tbl-expert-stats.
As can be seen from the `n_deals` column, every IPO has an auditor, a company counsel, and a transfer agent, with some deals having more than one of these.
Not every deal has a lead underwriter, but from @tbl-no-uws, it can be seen that all deals without a lead underwriter have at least one underwriter.
It can be inferred from @tbl-expert-stats that the typical IPO has multiple underwriters.

```{r}
expert_counts <-
  ipo_experts |> 
  group_by(role) |> 
  summarize(n_deals = n_distinct(dealID), 
            n_rows = n(),
            .groups = "drop") |> 
  arrange(desc(n_deals))
```


```{r}
#| echo: false
#| label: tbl-expert-stats
#| tbl-cap: Statistics for different kinds of experts
expert_counts |>
  collect() |>
  tt() |>
  style_tt(align = "ldd") |>
  format_tt(escape = TRUE, digits = 0) 
```

```{r}
#| echo: false
#| label: tbl-no-uws
#| tbl-cap: Statistics for IPOs without lead underwriters
deals_wo_lead_uw <-
  ipo_experts |>
  filter(role == "LeadUnderwriter") |>
  distinct(dealID)

ipo_experts |>
  anti_join(deals_wo_lead_uw, by = "dealID") |>
  group_by(role) |> 
  summarize(n_deals = n_distinct(dealID), 
            n_rows = n(),
            .groups = "drop") |> 
  arrange(desc(n_deals)) |>
  collect() |>
  tt() |>
  style_tt(align = "ldd") |>
  format_tt(escape = TRUE, digits = 0) 
```


```{r}
#| echo: false
#| label: tbl-glmd-experts
#| tbl-cap: Experts associated with GLMD IPO
ipo_experts |>
  filter(dealID == "926632-74581") |>
  select(-dealID) |>
  collect() |>
  tt() |>
  format_tt(escape = TRUE) 
```

Digging a little deeper into one case---shown in  @tbl-glmd-experts---we see a number of things.
First, we appear to have "two auditors", albeit with names that appear to differ only in the point at which the names are truncated with "`...`".
Second, there are no identifiers for the experts other than their names.
Names are problematic for many reasons.
Firms can change names.
Names can be abbreviated or misspelt or change in subtle ways that are not obvious to a human reader, but that make them unusable as identifiers in code.
Third, at least for some expert types, there can be more than one expert for a given IPO.

Before moving onto the data on auditors provided on Form APs, I compile data on auditors and IPOs from the IPO data and store it in `ipo_auditors`.

```{r}
ipo_auditors <-
  ipos |> 
  select(dealID, companyName, CIK, pricedDate, CompanyAddress) |> 
  inner_join(ipo_experts, by = "dealID") |>
  filter(role == "Auditor") |>
  rename(cik = CIK) |>
  select(-role) |>
  compute()
```

## Form AP data

I get data on auditors from the database constructed by the PCAOB from Form AP filings.
I have written about this database in a [previous note](https://github.com/iangow/notes/blob/main/form_ap_names.pdf).

```{r}
form_aps <- load_parquet(db, "form_aps", "pcaob")
```

We can collect auditor IDs associated with each CIK from `form_aps`.
Of course, firms change auditors over time, but if we are interested in the auditors at the time of IPO, it might make sense to pick the first auditor associated with each CIK.^[Some firms have SEC filings long before their IPOs, so this is not a perfect approach, but it is good enough for our purposes here.]
The following code creates `first_auditors` with information on the first auditor for each issuer.

```{r}
first_auditors <-
  form_aps |>
  group_by(issuer_cik) |>
  filter(audit_report_date == min(audit_report_date, na.rm = TRUE)) |>
  ungroup() |>
  rename(cik = issuer_cik) |>
  mutate(cik = if_else(cik == "9999999999", NA, cik)) |>
  select(cik, firm_id, firm_name, firm_country, audit_report_date) |>
  compute()
```

## Name-based links

If a match between `expertName` (from IPOs) and `firm_id` and `firm_name` (from Form AP data) shows up many times, there's a good chance that it's valid match.
In constructing `top_matches`, I require that a match be present at least five times.
This threshold is arbitrary, but it's just a starting point in two ways.
First, I will apply some judgement to the matches before flagging them as valid.
Second, I will probably come back to additional candidate matches in subsequent analyses.

```{r}
top_matches <-
  ipo_auditors |>
  left_join(first_auditors, by = "cik") |> 
  filter(!is.na(firm_name)) |> 
  count(firm_id, expertName, firm_name, sort =  TRUE) |>
  collect() |>
  filter(n >= 5)
```

I will use a Google Sheets document to collect the data.
In the past, I have often taken data such as that in `top_matches` and exported it to a CSV file that I open in Google Sheets.
Here I chose to create a new Google Sheets file using the `gs4_create()` from `googlesheets4`: 

```{r}
#| eval: false
options(gargle_oauth_email = TRUE)
gs_ipo_auditors <- gs4_create(name = "ipo-auditors")
```

```{r}
#| include: false
gs_ipo_auditors <- as_sheets_id("1m2F4nnhJyg81gj17-h3JWeeSbd4eyRP_SUYlX1OzR-8")
```

```{r}
#| cache: true
#| eval: false
gs_ipo_auditors
```

```
── <googlesheets4_spreadsheet> ─────────────────────────────────────────────────────────────────────────
Spreadsheet name: ipo-auditors                                
              ID: 1m2F4nnhJyg81gj17-h3JWeeSbd4eyRP_SUYlX1OzR-8
          Locale: en_US                                       
       Time zone: Etc/GMT                                     
     # of sheets: 1                                           

── <sheets> ────────────────────────────────────────────────────────────────────────────────────────────
(Sheet name): (Nominal extent in rows x columns)
      Sheet1: 1000 x 26
```

After I've created the Google Sheets file, I will access it using the key seen in the output above.

```{r}
#| include: true
#| eval: false
gs_ipo_auditors <- as_sheets_id("1m2F4nnhJyg81gj17-h3JWeeSbd4eyRP_SUYlX1OzR-8")
```

I can put the data from `top_matches` in the Google Sheets spreadsheet using `write_sheet()`.

```{r}
#| eval: false
write_sheet(top_matches, ss = gs_ipo_auditors, sheet = "top_matches")
```

```
✔ Writing to ipo-auditors.
✔ Writing to sheet top_matches.
```

The next step is to open the spreadsheet in a browser:

```{r}
#| eval: false
gs4_browse(gs_ipo_auditors)
```

I then inserted a new column `valid` to the right of the existing columns.
I also renamed the sheet from `top_matches` to `name_matches`.^[For one, the new name is more descriptive of how I will use the data it contains.
Additionally, if I rerun the `write_sheet()` code above, it will not overwrite the sheet that I already have.
Note that Google Sheets has excellent version history that makes it possible to restore an earlier version if you accidentally overwrite hand-collected data.]
I looked at the matches and classified ones that looked good as valid by putting the value `TRUE` in the row under the `valid` column.
Most of the first 30 or so rows looked good.
But a match with `expertName` of "Ernst & Young LLP" and `firm_name` of "KPMG LLP" is not a valid match and I put `FALSE` in the `valid` column for this case.^[Note that I later deleted the entire rows for such clearly bad matches.]

### Problematic name-based links

The next step I took was to look at cases where `expertName` was associated with more than one distinct `firm_id` value.

```{r}
#| include: true
#| eval: true
#| message: false
top_matches_processed <- read_sheet(ss = gs_ipo_auditors, sheet = "top_matches")

top_match_problems <-
  top_matches_processed |>
  filter(valid) |>
  group_by(expertName) |>
  filter(n_distinct(firm_id) > 1) |>
  ungroup() |>
  select(-valid) |>
  arrange(expertName, desc(n))
```

Looking at @tbl-top-match-problems, we see that there are multiple `firm_id` values for some values of `expertName` even when we have deemed a match to be valid.^[Note that further research was required to determine that the match of "Marcum Asia CPAs LLP" with "Friedman LLP" is valid; there was a name change that occurred in 2022.]


```{r}
#| echo: false
#| label: tbl-top-match-problems
#| tbl-cap: Contents of `top_match_problems`
top_match_problems |>
  collect() |>
  tt() |>
  format_tt(escape = TRUE) |>
  style_tt(fontsize = 0.8,
           align = "rllr") |>
  theme_tt("spacing")
```

Some additional digging (using data in `form_aps`) confirms that the main explanation is that the different `firm_id` values relate to different national partnerships of Big Four audit firms.
For example, as seen in @tbl-top-match-countries, `firm_id` of 34 is the US arm of Deloitte, while 1147 and 1208 refer to the UK and Canadian operations, respectively.

```{r}
#| echo: false
#| label: tbl-top-match-countries
#| tbl-cap: Countries of firms in `top_match_problems`
form_aps |>
  distinct(firm_id, firm_country) |>
  inner_join(top_match_problems |> 
               select(firm_id, firm_name), by = "firm_id", copy = TRUE) |>
  select(firm_id, firm_name, firm_country) |>
  arrange(firm_name) |>
  collect() |> 
  tt() |>
  format_tt(escape = TRUE) |>
  style_tt(fontsize = 0.8) |>
  theme_tt("spacing")
```

### Manual links by `dealID`

One approach to handling these cases would be to get information on countries from the IPO data and use that to distinguish the `firm_id` values to be used for cases with multiple apparent matches.
However, given the relatively small number of non-US matches to be handled, I instead chose to grab the `dealID` values for those cases and proceed to validate matches on a deal-by-deal basis.

To start with, I identify the most common `firm_id` value for each `expertName`.
In effect, I will set these as the default for the name-based matches.

```{r}
top_match_problem_top_ids <-
  top_match_problems |>
  group_by(expertName) |>
  filter(n == max(n, na.rm = TRUE)) |>
  ungroup() |> 
  select(firm_id) |>
  pull()

top_match_problem_top_ids
```

I then collect data on all other cases where `expertName` is in `top_match_problems` and these is some match to Form AP data (i.e., `firm_name` is present).

```{r}
top_match_problems_countries <-
  ipo_auditors |>
  left_join(first_auditors, by = "cik") |>  
  filter(expertName %in% top_match_problems$expertName,
         !firm_id %in% top_match_problem_top_ids,
         !is.na(firm_name)) 
```

I then export data related to these matches to an additional tab `deal_matches_add` that I can use to verify candidate matches for each `dealID`.

```{r}
#| eval: false
top_match_problems_countries |>
  select(dealID, cik, CompanyAddress, firm_id, firm_name, 
         firm_country, audit_report_date) |>
  collect() |>
  write_sheet(ss = gs_ipo_auditors, sheet = "deal_matches_add")
```

In this case, I renamed the tab to `deal_matches`, added a `valid` column much as before, and also added additional columns such as `sec_filings` and `notes`.
I use `sec_filings` to create a formula-based hyperlink to the SEC filings around the time of the IPO that I can use to check cases where the information in the spreadsheet does not suffice to give me confidence in validating a match.
Looking at the formula for cells with "SEC Filings" in the `sec_filings` column of the `deal_matches` sheet, you will see that the formula keys off values in `cik` and `pricedDate` to make it easy to get to the relevant filings.

While this might appear to violate the "No Calculations in the Raw Data Files" dictum of @Broman:2018aa, it reflects the different way that I am using the spreadsheet format.
In effect, the Google Sheets file is a kind of notebook with supporting information for the data that will be used in analysis.

Variants on the steps above are repeated until the tabs `name_matches` and `deal_matches` contain a fairly complete set of data on both name-based and deal-level matches of each IPO with the `firm_id` value for the auditor at the time of the IPO.

## Combining data to produce the link table

The next step is to use the data in these sheets to create a link table between IPOs and `firm_id` values.
We start by loading data from two sheets in our Google Sheets file.

```{r}
#| message: false
name_matches <- read_sheet(ss = gs_ipo_auditors, sheet = "name_matches")
deal_matches <- read_sheet(ss = gs_ipo_auditors, sheet = "deal_matches")
```



```{r}
deal_matches_processed <-
  deal_matches |>
  filter(valid) |>
  unnest(firm_id, keep_empty = TRUE) |>
  select(dealID, firm_id) |>
  mutate(match_type = "manual")
```

Note that we give priority to deal-level matches: if an IPO is matched in `deal_matches`, we do not consider a potential match using names in `name_matches`.
This is accomplished by putting all valid matches from `deal_matches` in `deal_matches_processed` and then using an anti-join when constructing `name_matches_processed`.

```{r}
name_matches_processed <-
  name_matches |>
  filter(valid) |>
  select(firm_id, expertName) |>
  inner_join(ipo_auditors, by = "expertName", copy = TRUE) |>
  mutate(match_type = "name-based") |>
  anti_join(deal_matches_processed, by = "dealID") |>
  select(dealID, firm_id, match_type)
```

Having constructed `deal_matches_processed` and `name_matches_processed`, we can combine them using `union_all()`.

```{r}
all_matches <-
  deal_matches_processed |>
  union_all(name_matches_processed) |>
  copy_to(dest = db, df = _, name = "all_matches",
          overwrite = TRUE)
```

### Name-based links without Form AP matches

To facilitate data collection, I compile firm names and countries for audit firms on `form_aps`.

```{r}
auditor_names <-
  form_aps |>
  group_by(firm_id) |>
  window_order(desc(audit_report_date)) |>
  mutate(firm_name = first(firm_name),
         firm_country = first(firm_country)) |>
  distinct(firm_id, firm_name, firm_country) |>
  compute()
```

I then compile data on all auditors associated with each issuer CIK, including the names and countries.
Unfortunately,  `dbplyr` does not provide the interface we need to directly invoke the `array_agg()` aggregate with an `ORDER BY` clause.
Fortunately, we can use `sql()` to inject precisely the SQL we need.^[The reason for specifying `ORDER BY firm_id` is to ensure that the values in `firm_ids` correspend with the respective values in `firm_names`, etc.]

```{r}
all_auditors <-
  form_aps |>
  distinct(issuer_cik, firm_id) |>
  inner_join(auditor_names, by = "firm_id") |>
  group_by(issuer_cik) |>
  summarize(
    firm_ids = sql("array_agg(firm_id ORDER BY firm_id)"),
    firm_names = sql("array_agg(firm_name ORDER BY firm_id)"),
    firm_countries = sql("array_agg(firm_country ORDER BY firm_id)")
    ) |>
  compute()
```

I collect data on all name-based matches with 

```{r}
no_form_ap_matches <-
  all_matches |>
  filter(match_type == "name-based") |>
  inner_join(ipo_auditors, by = "dealID") |>
  select(dealID, cik, CompanyAddress, firm_id, expertName, pricedDate) |>
  rename(issuer_cik = cik) |>
  inner_join(all_auditors, by = "issuer_cik") |>
  filter(!firm_id %in% firm_ids) |>
  distinct() 
```

Note that once IPOs have been added to the `deal_matches` tab, it probably makes sense to exclude them from `no_form_ap_matches` by adding a line such as `anti_join(deal_matches, copy = TRUE, by = "dealID")` after the `inner_join()` above.

We can dump the data from `no_form_ap_matches` into a Google Sheets tab so that we can dig into them more deeply.

```{r}
#| message: false  
no_form_ap_matches |>  
  mutate(across(c(firm_ids, firm_names, firm_countries), as.character)) |>
  arrange(dealID) |>
  collect() |>
  write_sheet(ss = gs_ipo_auditors, sheet = "all_auditors")
```

Finally, we look for any deals in `ipo_auditors` that are not matched in `all_matches` and add these to our Google Sheets file.
One reason for these cases would be as new IPO deals get added to the data with `expertName` values not handled already.

```{r}
#| message: false
ipo_auditors |> 
  anti_join(all_matches, by = "dealID", copy = TRUE) |>
  mutate(firm_id = NA, firm_name = NA, firm_country = NA) |>
  select(dealID, cik, CompanyAddress, firm_id, expertName,
         firm_name, firm_country, pricedDate) |>
  collect() |>
  write_sheet(ss = gs_ipo_auditors, sheet = "extra_deals")
```

The cases in `extra_deals` sheet could be handled by added additional name-based matches to `name_matches` or additional deal-level matches to `deal_matches` as seems most appropriate.
For the most part I handled cases added to `extra_deals` by moving them to `deal_matches` as the first step and processing as I did for other cases above.
One this has been done, we can run the "`extra_deals`" code again and, if all is well, there will be no additional deals in `extra_deals` after doing so.

At that point, we could take the data in `all_matches` and "persist" it to disk.
In my case, I add `ipo_auditors.parquet` to the `nasdaq` "schema" of my parquet-based data repository.
More details on this data repository can be found in [an appendix](https://iangow.github.io/far_book/parquet-wrds.html) of [*Empirical Research in Accounting: Tools and Methods*](https://www.routledge.com/Empirical-Research-in-Accounting-Tools-and-Methods/Ding-Gow/p/book/9781032586502).

```{r}
all_matches |>
  mutate(firm_id = as.integer(firm_id)) |>
  collect() |>
  write_parquet(sink = file.path(Sys.getenv("DATA_DIR"), 
                                 "nasdaq", "ipo_auditors.parquet"))
```

# Tweaking the recommendations of @Broman:2018aa

@Broman:2018aa contains twelve numbered recommendations regarding best practices to follow when using spreadsheets to collect data.
Fundamentally, I don't disagree with any of these recommendations.
However, I feel that some of them need to be tweaked slightly to reflect the slightly different context I focus on here, or to update their recommendations to reflect changes in technology since 2018 (e.g., the increasing use of the cloud storage).
I discuss these tweaks in this section.

## "No Empty Cells"

I the case study above, there are inevitably empty cells before data are collected.
In some cases, there are empty cells even after data are collected.
For example, the `sec_filings` column is only populated on an as-needed basis.

What @Broman:2018aa says about empty cells is valid in general, so I think the recommendation should be refined a little.
For example, "NA" should be used to flag cases where there is the decision is to code a cell as missing, but this is very different from making a cell blank prior to the RA evaluating the case.
It is probably better to view "NA" as reflecting some conscious output of the data collection process and to recognize that sometimes not every value will be relevant (so there is no value in having an RA code a column as "NA" if it wasn't even examined).

Other recommendations about blank cells in @Broman:2018aa are completely valid.
In general, blank cells should not be used to represent zeros and blank cells should not be implicitly "filled" in a way that is "obvious" if looking at the spreadsheet, but not at all obvious to the data analyst.

## "No Calculations in the Raw Data Files"

The prescription here [@Broman:2018aa, p. 7] is that "your primary data files should contain *just the data* and nothing else: no calculations, no graphs."
I think I would relax this prescription a little in the context of sometimes-messy data collection like the ones I have used spreadsheets for.

Sometimes the data are the results of calculations and I think it would be fine to use the spreadsheet functionality to support data collection efforts.
Suppose I were hand-collecting data on executive compensation and the value in the `salary` column is almost always found as a single value that can be typed in by the RA.
But then one firm reports salary as 12 monthly payments of $132,236.
In such a case, I think it would be perfectly appropriate for the RA to type `= 12 * 132236` in the cell under `salary` rather than calculating by hand and typing the result in.

Other situations might go the other way.
For example, suppose that `director_names` contained a list of names of directors, such as `['Anne', 'Bob', 'Clara']`.
It might still make sense to have the RA enter a value under `num_directors` (3 in this case) even though this could be easily calculated

In other cases, calculations can facilitate the collection of data in the first place.
For example, the URL behind the hyperlink for `sec_filings` used in the case study above is calculated.
In other cases, calculations might provide guidelines that help the RA collect data in other fields.
For example, `total_compensation` might be collected by the RA and it might be helpful to calculate this value from components for comparison to allow discrepancies to be flagged and resolved as part of the data collection process.

## "Do Not Use Font Color or Highlighting as Data"

I do not disagree with this one, but I would say that it can be fine to use font colour or highlighting to communicate with humans: "Please focus on the cells I have coloured green before moving on to the other cases."
The critical thing is that the colours are not being used as *data* per se.

## "Make Backups"

"Make regular backups of your data. In multiple locations."
This recommendation [@Broman:2018aa, p. 8] has perhaps not aged well.
For example, what counts as "multiple locations" in a world in which everything lives in some cloud service like Google Drive or Dropbox?
Also, precisely what should be backed up if one is collecting data in Google Sheets?
I would need to export data to a different format (e.g., Microsoft Excel) and back *that* up if my concern is that Google might disappear and take my data with it.

One idea might be to export the underlying data to CSV and back *that* up, but this would mean losing a lot of the cell notes and such like that were an integral part of the data collection process.

Also @Broman:2018aa [p. 8] suggests "keep all versions of the data files" without indicating what counts as a version of a file.
If an RA is between entering data in one column and entering data in another column, we probably don't want to keep that version.
Nor is it clear that a back-up between each row is necessary.
If you're a researcher working on papers, the sensible view might be that some back-up be retained for each version of a paper that is posted or shared with others.
In such a case, one could restore the spreadsheet data applicable to a particular version of your paper.
But this can be unhelpful in practice if you are using data from other sources and isn't doing some kind of version control for those other sources.

Additionally, my experience is that "regular backups ... in multiple locations" can be a recipe for complexity and confusion.
If I need to go back to an earlier version, how do I decide whether to use the copy of `glucose_6_weeks` stored on a thumb drive in my drawer or the copy of `glucose_6_weeks` in Dropbox?
Perhaps I would need to add the date on which I made the back-up (e.g.,  `glucose_6_weeks_2024-12-31`) to allow me to distinguish them.
But maybe I don't always want the latest version of the data (if I did, why keep anything but the last copy?).
If the reason for restoring the data is some issue that came up in the last week of 2024 then I'd probably prefer the `glucose_6_weeks_2024-12-21` version.
Of course, I may not even be sure when the issue arose, so versions named just for dates may not be particularly helpful.

My sense is that if you are collecting data in spreadsheets, you are probably not the lynchpin of some multi-billion dollar Stage III drug trial aimed at getting FDA approval.
So accepting the "Google ceases to exist" risk does not seem unreasonable.
Even "Google Sheets ceases to exist" seems tolerable (or at least not very different from "I can't open Microsoft Excel files from 2024" risk) for almost any use case where storing data in a spreadsheet makes sense.

My recommendation is to rely on version control features embedded in Google Sheets for organization of data somewhat less critical than the codes for the US nuclear arsenal.
For example, "name current version" allows you to label a version as something like "qje_submission" and this is likely to suffice for the relatively low stakes of most data collection efforts conducted with spreadsheets.

## "Save the Data in Plain Text Files"

This one is a little difficult to evaluate, as its rationale is not entirely clear.
One reason is perhaps given by the statement that "this sort of nonproprietary file format [CSV] does not and never will require any sort of special software" [@Broman:2018aa, p. 8].
But this claim arguably applies to the `ipo_auditors.parquet` file that I created above.^[Of course if you were transported back to 1995 with nothing but a single data file, you might prefer CSV to parquet, but it seems exceedingly unlikely that you will be in a situation where you cannot work with a parquet file in 2025.]

The "CSV files are easier to handle in code" is arguably false because parquet files include information about data types and the like that are missing from CSV files.

In short, I am not sure that I fundamentally disagree with this recommendation if it simply means that there should be a "persist" step along the lines of that depicted in @fig-whole-game and the persistent format is something "nonproprietary" like CSV or parquet.
An implicit assumption in @fig-whole-game is that the data are being persisted as part of the *Curate* process rather than the *Curate* and *Understand* processes being tangled messes where it is unclear where one begins and the other ends.
I would argue that the spreadsheet plus code should be viewed together as a unit that fosters reproducibility.

## "Create a Data Dictionary"

At some level it is difficult to quibble much with this one.
However, it has an air of "eat less; do more exercise" (as advice for losing weight) to it.
I would guess that most spreadsheets used for data collection do not come with the kind of data dictionary envisaged by @Broman:2018aa [p. 8].
For example, there is no data dictionary in the Google Sheets document I created for linking IPOs with auditor IDs.
That said, it is probably a good idea to make one.
In my case, the critical variables are `deal_ID` and `firm_id`, which are both defined by other entities (Nasdaq and PCAOB, respectively).
Nonetheless, some discussion of the attributes of those variables likely makes sense, as does some discussion of the purpose of linking the tables so that a user of the data can understand the rationale for any decisions made when the links are not entirely clear.^[For example, the goal might be to identify the auditor at the time of the IPO, even in cases where a change in auditor is in process at that time.]

More importantly, I think that the data dictionary is less important than other documentation that can be associated with a spreadsheet, such as discussion of the goals of data collection and scripts used to test features of the data and to create final data sets that are passed along to the *Understand* process as depicted in @fig-whole-game.

It may be that the best documentation is something like the document you are reading right now, with discussion of data sources, issues to be addressed, and illustrative code.

# Additional case studies

Here are some cases where I have used spreadsheets to collect data.

1. Data on director biographies. 
These data were used to support the @Gow:2018aa.
2. Data on director ethnicities.
3. Data on activist directors.
4. Data on CIK.
5. Matching conference calls on StreetEvents to PERMNOs (CRSP security identifier).
6. Corrections to data on audit fees from SIRCA.

# References {-}
